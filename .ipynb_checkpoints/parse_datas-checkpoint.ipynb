{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import byte\n",
    "import chardet\n",
    "import csv\n",
    "import json\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex as re\n",
    "import string\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from chardet import UniversalDetector as udet\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from string import printable\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "from cimaq_utils import get_cimaq_dir_paths\n",
    "from sniffbytes import flatten\n",
    "from sniffbytes import loadfiles\n",
    "from sniffbytes import loadimages\n",
    "from cimaq_utils import repair_enc_task\n",
    "from cimaq_utils import fetch_cimaq\n",
    "\n",
    "from json_read import json_read\n",
    "\n",
    "import sniffbytes as snif\n",
    "\n",
    "from scanzip import scanzip\n",
    "from scanzip import getnametuple\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "from multiple_replace import multiple_replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'ub_id\\n108391\\n120839\\n122922\\n127228\\n139593\\n147863\\n150649\\n164965\\n175295\\n178101\\n189005\\n197192\\n199801\\n219637\\n229301\\n247659\\n254402\\n255499\\n258618\\n258912\\n267168\\n270218\\n271596\\n314409\\n326073\\n336665\\n337021\\n350555\\n370092\\n385370\\n386333\\n396250\\n403131\\n408506\\n413474\\n427357\\n437101\\n439776\\n441008\\n458807\\n459801\\n462345\\n484204\\n490035\\n502616\\n517070\\n520377\\n543589\\n549994\\n555537\\n567214\\n597569\\n619278\\n628299\\n630120\\n652850\\n658178\\n659068\\n668786\\n677561\\n711830\\n729722\\n739694\\n748676\\n763590\\n778749\\n783781\\n785217\\n785245\\n804743\\n845675\\n866812\\n878354\\n884343\\n886007\\n893978\\n901551\\n906145\\n914042\\n915022\\n920577\\n932933\\n936730\\n938001\\n955548\\n956049\\n956130\\n968913\\n974246\\n979001\\n983291\\n988602\\n996599\\n998166'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_dir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/CIMAQ_fmri_memory/data'\n",
    "zeprimes = pjoin(cimaq_dir, 'task_files/zipped_eprime')\n",
    "qc_path = xpu(pjoin(cimaq_dir, 'participants/sub_list_TaskQC.tsv'))\n",
    "# snif.get_bytes(qc_path)\n",
    "# type(qc_path) == str and os.path.isfile(qc_path)\n",
    "qc_sheet = snif.clean_bytes(qc_path)\n",
    "qc_sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'', b'']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# def get_bytes__str_path(inpt: Union[str, os.PathLike]):\n",
    "#     if type(inpt) == bytes or bytearray:\n",
    "#         outpt = [inpt.lower() if bool(len(inpt.splitlines()) > \\\n",
    "#                   0 and inpt != None) else b\"1\"][0]\n",
    "#     if type(inpt) == str and os.path.isfile(inpt):\n",
    "#         with open(inpt, \"rb\", buffering = 0) as myfile:\n",
    "#             outpt = myfile.read().lower()\n",
    "#             outpt = [outpt if bool(len(outpt.splitlines()) > 1)\n",
    "#                      else b\"1\"]\n",
    "#             myfile.close()\n",
    "#     else:\n",
    "#         outpt = inpt.decode()\n",
    "#     return outpt\n",
    "abspace = ' '.encode()\n",
    "abspace.split(b' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b' '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tbytes = snif.get_bytes(pjoin(xpu(cimaq_dir),'participants/sub_list_TaskQC.tsv'))\n",
    "# snif.clean_bytes(snif.get_bytes(tbytes))\n",
    "snif.get_delimiter(snif.get_bytes(snif.get_bytes(tbytes)))\n",
    "# snif.get_bytes(tbytes).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function pack in module _struct:\n",
      "\n",
      "pack(...)\n",
      "    pack(format, v1, v2, ...) -> bytes\n",
      "    \n",
      "    Return a bytes object containing the values v1, v2, ... packed according\n",
      "    to the format string.  See help(struct) for more on format strings.\n",
      "\n",
      "###############################################################################\n"
     ]
    }
   ],
   "source": [
    "dir(True)\n",
    "bool(2+2==5).to_bytes(True.bit_length(), sys.byteorder)\n",
    "str(np.nan).encode()\n",
    "[str(np.nan).encode()]*4\n",
    "pd.Series([str(np.nan).encode()]*3)\n",
    "\n",
    "bytes(str(np.nan), 'utf8')\n",
    "b'True'.decode()\n",
    "# UnidecodeError\n",
    "# bytearray(struct.pack(\"f\", str(np.nan).encode()))  \n",
    "help(struct.pack)\n",
    "print('#'*79\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/103 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2496.61it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6080.46it/s]\n",
      "  2%|▏         | 2/103 [00:00<00:05, 19.38it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5257.34it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5061.92it/s]\n",
      "  4%|▍         | 4/103 [00:00<00:05, 16.92it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4985.31it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5103.80it/s]\n",
      "  6%|▌         | 6/103 [00:00<00:05, 17.11it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5427.41it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5468.45it/s]\n",
      "  8%|▊         | 8/103 [00:00<00:06, 15.29it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5409.21it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5533.38it/s]\n",
      " 10%|▉         | 10/103 [00:00<00:06, 14.27it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5310.59it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3981.68it/s]\n",
      " 12%|█▏        | 12/103 [00:00<00:06, 13.65it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6330.07it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5565.69it/s]\n",
      " 14%|█▎        | 14/103 [00:01<00:06, 13.16it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5309.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6262.02it/s]\n",
      " 16%|█▌        | 16/103 [00:01<00:07, 12.04it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5851.43it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6083.99it/s]\n",
      " 17%|█▋        | 18/103 [00:01<00:07, 11.74it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2870.45it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5750.35it/s]\n",
      " 19%|█▉        | 20/103 [00:01<00:07, 11.64it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6155.42it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6075.18it/s]\n",
      " 21%|██▏       | 22/103 [00:01<00:07, 10.88it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4949.62it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5428.82it/s]\n",
      " 23%|██▎       | 24/103 [00:01<00:07, 11.21it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4484.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5747.20it/s]\n",
      " 25%|██▌       | 26/103 [00:02<00:06, 11.56it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4691.62it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4790.21it/s]\n",
      " 27%|██▋       | 28/103 [00:02<00:06, 11.76it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3084.50it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3148.40it/s]\n",
      " 29%|██▉       | 30/103 [00:02<00:06, 11.10it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6258.29it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6303.43it/s]\n",
      " 31%|███       | 32/103 [00:02<00:05, 12.27it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4109.65it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5105.04it/s]\n",
      " 33%|███▎      | 34/103 [00:02<00:05, 12.26it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6440.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5343.06it/s]\n",
      " 35%|███▍      | 36/103 [00:02<00:05, 12.40it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4994.41it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4623.35it/s]\n",
      " 37%|███▋      | 38/103 [00:03<00:05, 12.44it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5400.86it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6460.73it/s]\n",
      " 39%|███▉      | 40/103 [00:03<00:05, 11.63it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5459.91it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4513.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5497.12it/s]\n",
      " 42%|████▏     | 43/103 [00:03<00:04, 12.72it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5046.08it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5115.00it/s]\n",
      " 44%|████▎     | 45/103 [00:03<00:04, 13.14it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5225.90it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5967.99it/s]\n",
      " 46%|████▌     | 47/103 [00:03<00:04, 12.89it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3526.40it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6160.85it/s]\n",
      " 48%|████▊     | 49/103 [00:03<00:04, 12.61it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5277.18it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5861.24it/s]\n",
      " 50%|████▉     | 51/103 [00:04<00:04, 12.45it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5419.00it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5438.67it/s]\n",
      " 51%|█████▏    | 53/103 [00:04<00:04, 11.52it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4981.36it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4297.44it/s]\n",
      " 53%|█████▎    | 55/103 [00:04<00:04, 11.68it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5341.70it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5336.26it/s]\n",
      " 55%|█████▌    | 57/103 [00:04<00:03, 11.73it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5981.61it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4938.35it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4987.28it/s]\n",
      " 58%|█████▊    | 60/103 [00:04<00:03, 12.97it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5536.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5351.24it/s]\n",
      " 60%|██████    | 62/103 [00:04<00:03, 12.91it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5055.81it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5274.53it/s]\n",
      " 62%|██████▏   | 64/103 [00:05<00:03, 11.83it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5326.10it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5348.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5380.07it/s]\n",
      " 65%|██████▌   | 67/103 [00:05<00:02, 12.74it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5256.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4882.78it/s]\n",
      " 67%|██████▋   | 69/103 [00:05<00:02, 12.77it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5257.34it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4664.48it/s]\n",
      " 69%|██████▉   | 71/103 [00:05<00:02, 12.68it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6107.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4861.27it/s]\n",
      " 71%|███████   | 73/103 [00:05<00:02, 12.58it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4791.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6124.86it/s]\n",
      " 73%|███████▎  | 75/103 [00:06<00:02, 12.53it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3578.15it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6022.84it/s]\n",
      " 75%|███████▍  | 77/103 [00:06<00:02, 12.40it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 4940.29it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4942.23it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5329.48it/s]\n",
      " 78%|███████▊  | 80/103 [00:06<00:01, 14.62it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5482.75it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5294.17it/s]\n",
      " 80%|███████▉  | 82/103 [00:06<00:01, 14.93it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5178.15it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5394.60it/s]\n",
      " 82%|████████▏ | 84/103 [00:06<00:01, 14.12it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5465.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5629.94it/s]\n",
      " 83%|████████▎ | 86/103 [00:06<00:01, 13.58it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5236.33it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5595.39it/s]\n",
      " 85%|████████▌ | 88/103 [00:06<00:01, 12.44it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5209.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5087.70it/s]\n",
      " 87%|████████▋ | 90/103 [00:07<00:01, 12.46it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5164.13it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5488.49it/s]\n",
      " 89%|████████▉ | 92/103 [00:07<00:00, 12.55it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6245.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4715.88it/s]\n",
      " 91%|█████████▏| 94/103 [00:07<00:00, 11.81it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5019.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5058.25it/s]\n",
      " 93%|█████████▎| 96/103 [00:07<00:00, 12.14it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4750.06it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5462.52it/s]\n",
      " 95%|█████████▌| 98/103 [00:07<00:00, 12.13it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5111.26it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5179.43it/s]\n",
      " 97%|█████████▋| 100/103 [00:07<00:00, 12.39it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5267.90it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2603.86it/s]\n",
      " 99%|█████████▉| 102/103 [00:08<00:00, 11.97it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2981.45it/s]\n",
      "100%|██████████| 103/103 [00:08<00:00, 12.62it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "cimaq_infos = pd.concat(val.T for val in\n",
    "                        df(tuple(scanzip(apath,\n",
    "                                         exclude = ['Practice', 'Pratique', 'PRATIQUE', 'PRACTICE'],\n",
    "                                         to_xtrct = ['.pdf', '.edat2'],\n",
    "                                         dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                                 for apath in tqdm(snif.loadimages(xpu(zeprimes)))),\n",
    "                          dtype = object)[0].values.flatten()).dropna().reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_infos.delimiter.unique()[-1].decode('utf16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsheets = [(row[1].to_dict(),\n",
    "              df((line.split('\\t') for line in unidecode(\n",
    "                  snif.clean_bytes(row[1].bsheets,\n",
    "                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "                                   [1:4]).decode()).split('\\n')),\n",
    "                 dtype = object).T.set_index(0, drop = True).T)\n",
    "             if row[1].has_header else\n",
    "             (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "                 snif.clean_bytes(row[1].bsheets,\n",
    "                                  [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "                                  [1:4]).decode()).split('\\n')),\n",
    "                                   dtype = object))\n",
    "             for row in tqdm(pd.concat(\n",
    "                 val.T for val in\n",
    "                 df(tuple(scanzip(apath,\n",
    "                                  exclude = ['Practice', 'Pratique',\n",
    "                                             'PRATIQUE', 'PRACTICE'],\n",
    "                                  to_xtrct = ['.pdf', '.edat2'],\n",
    "                                  dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                          for apath in tqdm(snif.loadimages(xpu(zeprimes)))),\n",
    "                    dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "                             desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/103 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5157.46it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5971.39it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5740.90it/s]\n",
      "  3%|▎         | 3/103 [00:00<00:06, 14.81it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5151.44it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4852.65it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3849.40it/s]\n",
      "  6%|▌         | 6/103 [00:00<00:06, 14.95it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5377.31it/s]\n",
      "  7%|▋         | 7/103 [00:00<00:07, 12.61it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5359.45it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5454.23it/s]\n",
      "  9%|▊         | 9/103 [00:00<00:07, 12.31it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2608.40it/s]\n",
      " 10%|▉         | 10/103 [00:00<00:08, 11.18it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2496.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2753.25it/s]\n",
      " 12%|█▏        | 12/103 [00:00<00:08, 11.29it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6135.61it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5507.23it/s]\n",
      " 14%|█▎        | 14/103 [00:01<00:07, 11.35it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5171.77it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5957.82it/s]\n",
      " 16%|█▌        | 16/103 [00:01<00:07, 11.44it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6252.69it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2808.18it/s]\n",
      " 17%|█▋        | 18/103 [00:01<00:07, 11.30it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3010.55it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3293.78it/s]\n",
      " 19%|█▉        | 20/103 [00:01<00:07, 11.20it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3498.75it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5521.73it/s]\n",
      " 21%|██▏       | 22/103 [00:01<00:07, 10.22it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4614.20it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4935.64it/s]\n",
      " 23%|██▎       | 24/103 [00:02<00:07, 10.25it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5919.14it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5835.15it/s]\n",
      " 25%|██▌       | 26/103 [00:02<00:07, 10.54it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5017.11it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5389.75it/s]\n",
      " 27%|██▋       | 28/103 [00:02<00:07, 10.15it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6101.69it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4920.58it/s]\n",
      " 29%|██▉       | 30/103 [00:02<00:07, 10.42it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6026.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6157.23it/s]\n",
      " 31%|███       | 32/103 [00:02<00:06, 10.76it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5205.14it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5410.61it/s]\n",
      " 33%|███▎      | 34/103 [00:03<00:06, 10.99it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6421.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3228.87it/s]\n",
      " 35%|███▍      | 36/103 [00:03<00:06, 11.12it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5293.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5291.83it/s]\n",
      " 37%|███▋      | 38/103 [00:03<00:06, 10.56it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5360.82it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6953.42it/s]\n",
      " 39%|███▉      | 40/103 [00:03<00:05, 10.82it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5447.15it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4826.59it/s]\n",
      " 41%|████      | 42/103 [00:03<00:05, 11.99it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5485.62it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5157.78it/s]\n",
      " 43%|████▎     | 44/103 [00:03<00:04, 11.94it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 4895.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5066.81it/s]\n",
      " 45%|████▍     | 46/103 [00:04<00:04, 12.40it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4306.27it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5143.86it/s]\n",
      " 47%|████▋     | 48/103 [00:04<00:04, 12.10it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5961.21it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5225.90it/s]\n",
      " 49%|████▊     | 50/103 [00:04<00:04, 11.90it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6208.27it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5190.97it/s]\n",
      " 50%|█████     | 52/103 [00:04<00:04, 11.71it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5138.82it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5082.77it/s]\n",
      " 52%|█████▏    | 54/103 [00:04<00:04, 11.60it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5082.77it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5043.66it/s]\n",
      " 54%|█████▍    | 56/103 [00:04<00:04, 11.60it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5403.87it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6091.06it/s]\n",
      " 56%|█████▋    | 58/103 [00:05<00:04, 10.79it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4233.82it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4140.48it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5305.22it/s]\n",
      " 59%|█████▉    | 61/103 [00:05<00:03, 12.01it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5210.32it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5253.39it/s]\n",
      " 61%|██████    | 63/103 [00:05<00:03, 11.95it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5301.19it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5284.16it/s]\n",
      " 63%|██████▎   | 65/103 [00:05<00:03, 12.62it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5529.01it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5156.51it/s]\n",
      " 65%|██████▌   | 67/103 [00:05<00:02, 12.47it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5330.84it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5341.70it/s]\n",
      " 67%|██████▋   | 69/103 [00:05<00:02, 12.33it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5232.42it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5590.91it/s]\n",
      " 69%|██████▉   | 71/103 [00:06<00:02, 12.39it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3327.76it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5271.88it/s]\n",
      " 71%|███████   | 73/103 [00:06<00:02, 12.11it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5005.14it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6195.43it/s]\n",
      " 73%|███████▎  | 75/103 [00:06<00:02, 11.31it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5601.37it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6126.65it/s]\n",
      " 75%|███████▍  | 77/103 [00:06<00:02, 11.40it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5050.34it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4858.27it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4963.67it/s]\n",
      " 78%|███████▊  | 80/103 [00:06<00:01, 12.83it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3572.66it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2799.00it/s]\n",
      " 80%|███████▉  | 82/103 [00:06<00:01, 13.01it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3376.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 3346.07it/s]\n",
      " 82%|████████▏ | 84/103 [00:07<00:01, 13.36it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2610.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5506.74it/s]\n",
      " 83%|████████▎ | 86/103 [00:07<00:01, 12.53it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5269.23it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5815.73it/s]\n",
      " 85%|████████▌ | 88/103 [00:07<00:01, 12.32it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5116.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5382.83it/s]\n",
      " 87%|████████▋ | 90/103 [00:07<00:01, 12.15it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5424.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4980.18it/s]\n",
      " 89%|████████▉ | 92/103 [00:07<00:00, 11.27it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5687.96it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5389.75it/s]\n",
      " 91%|█████████▏| 94/103 [00:07<00:00, 11.39it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5297.18it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5334.91it/s]\n",
      " 93%|█████████▎| 96/103 [00:08<00:00, 11.57it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5595.39it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5105.67it/s]\n",
      " 95%|█████████▌| 98/103 [00:08<00:00, 10.85it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5240.26it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4968.38it/s]\n",
      " 97%|█████████▋| 100/103 [00:08<00:00, 11.11it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5014.71it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5502.89it/s]\n",
      " 99%|█████████▉| 102/103 [00:08<00:00, 11.25it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5355.34it/s]\n",
      "100%|██████████| 103/103 [00:08<00:00, 11.68it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n",
      "creating Pandas DataFrames: 365it [00:16, 21.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# #             (\"encoding\", \"delimiter\", \"has_header\", \"dup_index\",\n",
    "# #              \"lineterminator\", \"nfields\", \"width\", \"nrows\").\n",
    "######### MODEL #############################################33\n",
    "# newsheets = [(row[1].to_dict(),\n",
    "#               df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T)\n",
    "#              if row[1].has_header else\n",
    "#              (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object))\n",
    "#              for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.loadimages(xpu(zeprimes)))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdir = pjoin(os.getcwd(), 'cimaq_clean_eprime_datas3')\n",
    "os.makedirs(newdir, exist_ok = True)\n",
    "[itm[1].to_csv(pjoin(newdir, itm[0]['filename']), sep = '\\t',\n",
    "               header = [0 if itm[0]['has_header'] else None][0],\n",
    "               index = None)\n",
    " for itm in newsheets]\n",
    "\n",
    "# json.dumps(newsheets)\n",
    "# newsheets[0][0]\n",
    "# def megamerge_dicts(dict_list: list) -> dict:\n",
    "#     return reduce(\n",
    "#         lambda x, y: {**x, **y}, dict_list\n",
    "#     )\n",
    "# allinfos = megamerge_dicts([itm[0] for itm in newsheets])\n",
    "# allinfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fname</th>\n",
       "      <th>ext</th>\n",
       "      <th>fpaths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3025432_658178_tache_irm_onset-event-encoding_...</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3025432_658178_tache_irm_output-responses-enco...</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3025432_658178_tache_irm_output_retrieval_cima...</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3025432_658178_tache_irm_retrieval-3025432-1</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3291977_748676_tache_irm_encoding-scan-3291977-1</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>9929164_197192_tache_irm_encoding-scan-9929164-1</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>9929164_197192_tache_irm_onset-event-encoding_...</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>362</th>\n",
       "      <td>9929164_197192_tache_irm_output-responses-enco...</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>363</th>\n",
       "      <td>9929164_197192_tache_irm_output_retrieval_cima...</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>364</th>\n",
       "      <td>9929164_197192_tache_irm_retrieval-9929164-1</td>\n",
       "      <td>.txt</td>\n",
       "      <td>/home/francois/cimaq_memory/cimaq_clean_eprime...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>365 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 fname   ext  \\\n",
       "0    3025432_658178_tache_irm_onset-event-encoding_...  .txt   \n",
       "1    3025432_658178_tache_irm_output-responses-enco...  .txt   \n",
       "2    3025432_658178_tache_irm_output_retrieval_cima...  .txt   \n",
       "3         3025432_658178_tache_irm_retrieval-3025432-1  .txt   \n",
       "4     3291977_748676_tache_irm_encoding-scan-3291977-1  .txt   \n",
       "..                                                 ...   ...   \n",
       "360   9929164_197192_tache_irm_encoding-scan-9929164-1  .txt   \n",
       "361  9929164_197192_tache_irm_onset-event-encoding_...  .txt   \n",
       "362  9929164_197192_tache_irm_output-responses-enco...  .txt   \n",
       "363  9929164_197192_tache_irm_output_retrieval_cima...  .txt   \n",
       "364       9929164_197192_tache_irm_retrieval-9929164-1  .txt   \n",
       "\n",
       "                                                fpaths  \n",
       "0    /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "1    /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "2    /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "3    /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "4    /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "..                                                 ...  \n",
       "360  /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "361  /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "362  /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "363  /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "364  /home/francois/cimaq_memory/cimaq_clean_eprime...  \n",
       "\n",
       "[365 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loadfiles(sorted(loadimages(newdir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index'])\n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.clean_bytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     has_header = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.clean_bytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                has_header = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['as_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[row[1].as_df.shape for row in cimaq_infos.iterrows()]\n",
    "cimaq_infos.as_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([itm[1] for itm in\n",
    "           list(cimaq_infos['as_df'].iloc[0].iteritems())[[0, 1, 2, 3][-1]:]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_infos = df(tuple(scanzip(apath) for apath in\n",
    "#                        tqdm(loadimages(xpu(zeprimes)),\n",
    "#                             desc = 'sniffing')))\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.mkfrombytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     hdr = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n",
    "\n",
    "# cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                           dtype = object).T.set_index(0, drop = True).T\n",
    "#                          if row[1].has_header else\n",
    "#                          df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                            dtype = object)\n",
    "#                          for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                          desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['as_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import collections\n",
    "help(collections.abc)\n",
    "# help(typing.Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index\n",
    "\n",
    "tsheets = cimaq_infos.iloc[:3, :]\n",
    "# bsheets\n",
    "def clean_bytes(inpt, encoding, hdr, delimiter, lineterminator, dup_index, nfields):\n",
    "    newsheet = b'\\n'.join([b'\\t'.join(itm.strip(b'\\\\s') for itm in re.sub(b'\\\\s{2,}',\n",
    "                                         b'\\\\s'+delimiter+b'\\\\s',\n",
    "                                         line).split(delimiter))\n",
    "                       for line in fix_na_reps(inpt.lower(), encoding,\n",
    "                                               delimiter,\n",
    "                                               lineterminator).decode(\n",
    "                           \"utf8\", \"replace\").replace(\"�\", \"\").strip().encode(\n",
    "                           \"utf8\").splitlines()])\n",
    "    return [fix_dup_index(newsheet, encoding, hdr, delimiter, nfields)\n",
    "            if dup_index else newsheet][0]\n",
    "\n",
    "newsheets = [clean_bytes(row[1].bsheets, row[1].encoding, row[1].has_header,\n",
    "                                row[1].delimiter, row[1].lineterminator,\n",
    "                                row[1].dup_index, row[1].nfields)\n",
    "                    for row in tsheets.iterrows()]\n",
    "\n",
    "newsheets[0].splitlines()\n",
    "\n",
    "\n",
    "# udec = pd.read_csv(StringIO(unidecode(newsheet.decode())), sep = '\\t')\n",
    "# no_udec = pd.read_csv(StringIO(newsheet.decode()), sep = '\\t')\n",
    "# all(no_udec == udec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0].values == itm[1][1].values)]\n",
    "    return b'\\n'.join(b'\\t'.join(row[1].values.tolist()) for row in\n",
    "                      pd.concat((evdf[booltest],\n",
    "                       pd.Series([str(np.nan).encode()]*evdf.shape[1]),\n",
    "#                       pd.Series((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1)\n",
    "#                                 for row in evdf.iterrows()),\n",
    "                       oddf[booltest[-1]:]), axis = 1).iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # help(os.PathLike)\n",
    "\n",
    "# def fix_dup_index(\n",
    "#     inpt: Union[bytes, str, os.PathLike],\n",
    "#     encoding: str = None,\n",
    "#     hdr: bool = False,\n",
    "#     delimiter: bytes = None,\n",
    "#     nfields: int = None\n",
    "# ) -> bytes:\n",
    "#     inpt = get_bytes(inpt)\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "#     evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "#                   in evenodd(inpt.splitlines()))\n",
    "#     booltest = [itm[0] for itm in enumerate(\n",
    "#                    tuple(zip([itm[1] for itm in\n",
    "#                               evdf.iteritems()],\n",
    "#                               [itm[1] for itm in\n",
    "#                                oddf.iteritems()])))\n",
    "#                 if all(itm[1][0].values == itm[1][1].values)]\n",
    "#     datas = pd.concat((evdf[booltest],\n",
    "#                       pd.Series(bytes((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1), 'utf8')\n",
    "#                                 for row in evdf.iterrows()),\n",
    "#                       oddf[booltest[-1]:]), axis = 1)\n",
    "#     return b'\\n'.join(b'\\t'.join(itm for\n",
    "#                                  itm in row[1].values.tolist())\n",
    "#                       for row in datas.iterrows())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bytes(inpt: Union[bytes, str, os.PathLike, object]) -> bytes:\n",
    "#     \"\"\" Returns bytes from file either from memory or from reading \"\"\"\n",
    "#     if type(input) == object:\n",
    "#         return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                      itm in row[1].values.tolist())\n",
    "#                           for row in inpt.iterrows())    \n",
    "#     elif type(inpt) == bytes:\n",
    "#         return [inpt if bool(len(inpt.splitlines()) > \\\n",
    "#                   0 and inpt != None) else b\"1\"][0]\n",
    "#     elif type(inpt) == str:\n",
    "# #         try:\n",
    "#         with open(inpt, \"rb\", buffering=0) as myfile:\n",
    "#             outpt = myfile.read()\n",
    "#             if bool(len(myfile.read().splitlines()) > \\\n",
    "#                       0 and outpt != None):\n",
    "#                 return (outpt, myfile.close())[0]\n",
    "#             else:\n",
    "#                 return (b\"1\"[0], myfile.close())[0]\n",
    "# inpt = loadfiles(loadimages(xpu(zeprimes))).iloc[:, :-1]\n",
    "# tsheet = b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                itm in row[1].values.tolist())\n",
    "#                       for row in inpt.iterrows())\n",
    "# inpt.shape, tsheet.splitlines().__len__(), tsheet.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     return '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#             row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#             ['non'.encode(encoding)] + \\\n",
    "#             row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#             oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                         for row in evdf.fillna(\n",
    "#                                             'non'.encode(encoding)).iterrows())).replace(\n",
    "#             'non'.encode(encoding), str(np.nan).encode(encoding))\n",
    "\n",
    "# str(np.nan).encode(encoding)\n",
    "\n",
    "\n",
    "#     booltest = [itm[0] for itm in evdf.dropna().iteritems()\n",
    "#                 if all(itm[1].values == oddf.dropna()[itm[0]].values)]\n",
    "#     booltest = [itm[0][0] for itm in\n",
    "#                 tuple(zip(list(itm for itm in evdf.dropna().iteritems()),\n",
    "#                           list(itm for itm in oddf.dropna().iteritems())))\n",
    "#                if bool(all(itm[0][1].values) == all(itm[1][1].values))]\n",
    "#     tmp = [pd.Series(itm[0] + [str(np.nan).encode(encoding)] \\\n",
    "#                      + itm[1]).unique()\n",
    "#            if len(pd.Series(itm[0] + itm[1]).unique().tolist()) < nfields\n",
    "#            else pd.Series(itm[0] + itm[1]).unique()\n",
    "#            for itm in\n",
    "#            tuple(zip([line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[0]],\n",
    "#                     [line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[1]]))]\n",
    "#     return tmp\n",
    "#     evdf = df([line.decode().split()\n",
    "#                for line in tmp[0]])\n",
    "#     oddf = df([line.decode().split() for line in tmp[1]])\n",
    "\n",
    "#     return evdf.replace('None', np.nan)\n",
    "#     return pd.concat([evdf, df(oddf[[col[0] for col in\n",
    "#                                      enumerate(oddf.columns)\n",
    "#                                      if col[0] not in\n",
    "#                                      booltest]].iteritems())[1:]],\n",
    "#                      axis = 1)\n",
    "#pd.merge(evdf, oddf[list(oddf.columns)], on = booltest)\n",
    "# .drop(columns = booltest)\n",
    "#     booltest = [col for col in evdf.columns if\n",
    "#                 evdf[col].values.all() == oddf[col].values.all()]\n",
    "#     dup_test = [col for col in oddf.columns if evdf[col].values.all() == oddf[col].values.all()]\n",
    "\n",
    "#     return \"\\\\n\".join(\n",
    "#         [\n",
    "#             \"\\t\".join([itm for itm in line.split(=)])\n",
    "#             for line in newsheet.rename(\n",
    "#                 dict(enumerate(newsheet.columns))\n",
    "#             ).values.tolist()\n",
    "#         ]\n",
    "#     ).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(pd.Series)\n",
    "\n",
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes = cimaq_infos.loc[[row[0] for row in cimaq_infos.iterrows()\n",
    "                                   if 'onset' in row[1].filename]].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsheets = [fix_dup_index(fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter),\n",
    "                               row[1].encoding, row[1].has_header, row[1].delimiter, row[1].nfields)\n",
    "                             for row in cimaq_infos.iloc[:3, :].iterrows()]\n",
    "testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsheets = cimaq_infos.iloc[:3, :]\n",
    "# def fix_na_reps(inpt: bytes, encoding: str = None, delimiter: bytes = None) - > bytes:\n",
    "#     return '\\\\n'.encode(encoding).join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "#                       delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "#                       line) for line in inpt.splitlines())\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "\n",
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = snif.get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else snif.get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in snif.evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0] == itm[1][1])]\n",
    "    datas = pd.concat((evdf[booltest], pd.Series((int(len(row[1].values)) == nfields[0] -1)\n",
    "                                         for row in evdf.iterrows()),\n",
    "                     oddf[booltest[-1]:]), axis = 1)\n",
    "    return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "                               itm in row[1].values.tolist())\n",
    "                      for row in datas.iterrows())\n",
    "\n",
    "def fix_na_reps(inpt: bytes,\n",
    "                encoding: str = None,\n",
    "                delimiter: bytes = None,\n",
    "                lterm: bytes = None) -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    delimiter = [delimiter if delimiter else get_delimiter(inpt, encoding)][0]\n",
    "    lterm = [lterm if lterm else get_lineterminator(inpt)][0]\n",
    "    return lterm.join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "                      delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "                      line) for line in inpt.split(lterm))\n",
    "# newsheets = [fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter)\n",
    "#                              for row in testsheets.iterrows()]\n",
    "# testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "# testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = fix_dup_index(all_dup_indexes.bsheets[55],\n",
    "              all_dup_indexes.encoding[55],\n",
    "              all_dup_indexes.has_header[55],\n",
    "              all_dup_indexes.delimiter[55])\n",
    "\n",
    "fid\n",
    "#     '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#         row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#         [str(np.nan).encode(encoding)] + \\\n",
    "#         row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#         oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                  for row in evdf.iterrows()))\n",
    "#  for line in evdf.iterrows()fid[0][0].values.tolist()]\n",
    "# all(fid[0][0][3].values == fid[0][1][3].values)\n",
    "df((snif.evenodd((line.split() for line in\n",
    "                  all_dup_indexes.iloc[55].bsheets.splitlines()))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fdi[0][3] == fdi[1][3]\n",
    "# def get_nfields(inpt: bytes, hdr: bool = None) -> bytes:\n",
    "#     inpt = [get_bytes(inpt).splitlines()[1:] if hdr else\n",
    "#             get_bytes(inpt).splitlines()][0]\n",
    "#     return pd.Series(len(line.split())\n",
    "#                       for line in inpt).max()\n",
    "#             if not hdr\n",
    "#             else pd.Series(len(line.split()) for line in\n",
    "#                            inpt.splitlines()[1:]).max()][0]\n",
    "# fix_dup_index(all_dup_indexes.bsheets[55])\n",
    "\n",
    "# [chr(itm) for itm in list(all_dup_indexes.delimiter[0])].__len__()\n",
    "# fxd = fix_dup_index()\n",
    "# fxd\n",
    "# all_dup_indexes['na_reps'] = [chr(list(row[1].bsheets)[-1])\n",
    "#                               for row in all_dup_indexes.iterrows()]\n",
    "\n",
    "\n",
    "# all_dup_indexes\n",
    "# all_dup_indexes['na_reps']\n",
    "# all_dup_indexes.bsheets['na_rep'] = [row[1].bsheets]\n",
    "#              encoding: str = None,\n",
    "#              hdr: bool = False,\n",
    "#              delimiter: bytes = None)\n",
    "# chardet.detect('�'.encode())\n",
    "# '�'.encode('utf32')\n",
    "# [line.split().__len__() for line in\n",
    "#  all_dup_indexes.bsheets[55].splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets'] = ['\\\\n'.encode(row[1].encoding).join(['\\t'.encode(row[1].encoding).join(\n",
    "                                   re.sub(b'\\\\s' + b'{2,}',\n",
    "                                          b'\\\\s',\n",
    "                                          re.sub(row[1].delimiter + b'{2,}',\n",
    "                                          row[1].delimiter + \\\n",
    "                                          str(np.nan).encode(row[1].encoding) + \\\n",
    "                                          row[1].delimiter,\n",
    "                                          line)) \\\n",
    "                                for line in row[1].bsheets.splitlines())])\n",
    "    for row in cimaq_infos.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes['incr_delim'] = [Counter([snif.get_delimiter(line, 'ascii')\n",
    "                                          for line in row[1]['bsheets'].splitlines()]).most_common(1)[0][0]\n",
    "                                 for row in all_dup_indexes.iterrows()]\n",
    "all_dup_indexes['incr_delim'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['force_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "                             for row in cimaq_infos.iterrows()]\n",
    "# with open(pjoin(os.getcwd(), 'test.txt'), 'wb') as newfile:\n",
    "#     newfile.write(cimaq_infos.iloc[124]['bsheets'])\n",
    "#     newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded'] = [row[1].force_utf8.decode()\n",
    "                          for row in cimaq_infos.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index']).decode(\n",
    "#                                     'utf8', 'replace').replace(\n",
    "#                                         '�', '').encode().decode().strip()\n",
    "                            \n",
    "                            \n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n'\n",
    "                                            ))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.iloc[667]['as_df']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2 = pd.concat(val.T for val in cimaq_infos[0].values.flatten())\n",
    "cimaq_infos2 = cimaq_infos2.dropna()\n",
    "cimaq_infos2['newsheets'] = [snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode()\n",
    "                             for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                             desc = 'repairing')]\n",
    "\n",
    "cimaq_infos2['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n'))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['as_df'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[99].as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['to_csv'] = [pd.read_csv(StringIO(row[1].unidecoded),\n",
    "                                      sep = '\\t',\n",
    "                                      header = [0 if row[1].has_header\n",
    "                                                else None][0],\n",
    "                                      engine = 'c')\n",
    "#                                       quoting = 1)\n",
    "                              for row in cimaq_infos2.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[667].to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['cleaned'] = ['\\n'.join(['\\t'.join(line.split()) for line in\n",
    "                                      snif.is_printable(row[1].newsheets.decode(\n",
    "                                          row[1].encoding).encode(\n",
    "                                          'utf8', 'replace').decode()).splitlines()]).encode().decode()\n",
    "                           for row in cimaq_infos2.iterrows()]\n",
    "cimaq_infos2['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pd.read_csv(StringIO(row[1].newsheets.decode(row[1].encoding)), sep = '\\t')\n",
    " for row in cimaq_infos2.iterrows()][666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(scanzip(loadimages(xpu(zeprimes))[0]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chardet.detect(b'0xf8')#0xf\n",
    "# chr(int.from_bytes(b'0xf', sys.byteorder))\n",
    "# bytes(int.from_bytes(b'0xf8', sys.byteorder))\n",
    "# import codecs\n",
    "# help(codecs)\n",
    "[chr(item) for item in list(b'0xf')]\n",
    "[itm for itm in list]\n",
    "int.from_bytes(b'0xf', sys.byteorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniffzip(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    scan_bytes(myzip.open(row[1].))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "        return pd.concat([df(snif.evenodd(repr(itm)[8:-1].strip().replace('=', ' ').split())).T[1]\n",
    "                          for itm in\n",
    "                (ZipFile(archv_path).__dict__['filelist'][1:],\n",
    "                ZipFile(archv_path).close())[0]\n",
    "                if '__MACOSX' not in repr(itm)], axis = 1).T\n",
    "\n",
    "# atest = list(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'][0].values())\n",
    "# btest = get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['filelist'][0]\n",
    "# atest# json.dumps(repr(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'].values.tolist()).split())\n",
    "# #.replace('=', '\":').replace('\\\\', '')\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])\n",
    "df(repr(itm[1])[8:-1].replace('=', ' ').split() for itm in\n",
    "      ZipFile(loadimages(xpu(zeprimes))[0]).__dict__['NameToInfo'].items()\n",
    "      if '__MACOSX' not in itm)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GOOD ONE ######################333\n",
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    return ((df(zip(tuple(itm.strip(\"'\") for itm in repr(\n",
    "               ZipFile(archv_path).getinfo(nm))[8:-1].strip().replace(\n",
    "                   b'='.decode(), ' ').split()#.replace(chr(34), '').split())\n",
    "                   for nm in snif.filter_lst_exc(\n",
    "                       exclude, [ntpl if ntpl\n",
    "                       else getnametuple(ZipFile(archv_path))][0])))),\n",
    "            ZipFile(archv_path).close())[0])\n",
    "\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])[0]\n",
    "#.replace('=', '\":').replace('\\\\', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_archv(apath: Union[str, os.PathLike], ntpl: Union[str, list, tuple] = []) -> object:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_contents(\n",
    "    archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = [],\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    withbytes: bool = False,\n",
    "    to_sniff: bool = False,\n",
    "    to_close: bool = True,\n",
    ") -> object:\n",
    "    myzip = ZipFile(archv_path)\n",
    "    ntpl = snif.filter_list_exc(exclude,\n",
    "                                [ntpl if ntpl else snif.getnametuple(myzip)][0])\n",
    "\n",
    "    vals = (\n",
    "        df(\n",
    "            tuple(\n",
    "                dict(zip(evenodd(itm)[0], evenodd(itm)[1]))\n",
    "                for itm in tuple(\n",
    "                    tuple(\n",
    "                        force_ascii(repr(itm.lower()))\n",
    "                        .strip()\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"=\", \" \")[:-2]\n",
    "                        .split()\n",
    "                    )[1:]\n",
    "                    for itm in set(\n",
    "                        repr(myzip.getinfo(itm))\n",
    "                        .strip(\" \")\n",
    "                        .replace(itm, itm.replace(\" \", \"_\"))\n",
    "                        if \" \" in itm\n",
    "                        else repr(myzip.getinfo(itm)).strip(\" \")\n",
    "                        for itm in ntpl\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            dtype=\"object\",\n",
    "        )\n",
    "        .sort_values(\"filename\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    vals[[\"src_name\", \"ext\"]] = [(nm, os.path.splitext(nm)[1]) for nm in ntpl]\n",
    "    vals[\"filename\"] = [\n",
    "        \"_\".join(\n",
    "            pd.Series(\n",
    "                row[1].filename.lower().replace(\"/\",\n",
    "                                                \"_\").replace(\"-\",\n",
    "                                                             \"_\").split(\"_\")\n",
    "            ).unique()\n",
    "            .__iter__()\n",
    "        )\n",
    "        for row in vals.iterrows()\n",
    "    ]\n",
    "    if exclude:\n",
    "        vals = vals.drop(\n",
    "            [\n",
    "                row[0]\n",
    "                for row in vals.iterrows()\n",
    "                if row[1].filename\n",
    "                not in filter_lst_exc(exclude, [itm.lower() for itm in vals.filename])\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    if withbytes:\n",
    "        vals[\"bsheets\"] = [\n",
    "#             snif.strip_null() \n",
    "            myzip.open(row[1].src_name).read().lower() for row in vals.iterrows()\n",
    "        ]\n",
    "        \n",
    "    if to_sniff:\n",
    "        vals[[\"encoding\", \"delimiter\", \"has_header\", \"width\", \"dup_index\", \"nrows\"]] = \\\n",
    "            [tuple(snif.scan_bytes(row[1].bsheets).values()) for row in vals.iterrows()]\n",
    "    if to_close:\n",
    "        myzip.close()\n",
    "        return vals\n",
    "    else:\n",
    "        return (myzip, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azpath = loadfiles(loadimages(xpu(zeprimes))).fpaths[0]\n",
    "# with zipfile.Zipfile(azpath) as myzip:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  dict((bname(apath.replace('-', '_').strip()),\n",
    "                    get_zip_contents(apath, withbytes = True,\n",
    "                         to_sniff = True, to_close = True))\n",
    "                    for apath in tqdm(loadimages(zeprimes),\n",
    "                          desc = 'scan sniff'))\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sniffbytes import no_ascii\n",
    "# # chardet.detect(no_ascii('bon matin tin�tin!').encode())\n",
    "# # Union[str, bytes]\n",
    "# ''.join([chr(int.from_bytes(int.from_bytes(itm.encode(), sys.byteorder)))#.encode(encoding)\n",
    "#  for itm in ])\n",
    "# # ==  dict(json.dumps(set(string.printable)))\n",
    "set([chr(int.from_bytes(itm, sys.byteorder)).encode('utf32')\n",
    " for itm in [itm.encode() for itm in list(string.printable)]])\n",
    "[int.from_bytes(itm.encode(), sys.byteorder) for itm in list(string.printable)[-2:]]\n",
    "# '\\x00'.encode('ISO-8859-1')\n",
    "encodings = ['Windows-1252', 'utf32', 'utf8', 'ISO-8859-1']\n",
    "[int.from_bytes(itm.encode(encoding), sys.byteorder) for itm in list(string.printable)]\n",
    "int.from_bytes(b'', sys.byteorder)\n",
    "chardet.detect(b'\\x00'), chardet.detect(b'None')\n",
    "is_printable = [itm.encode('utf32') for itm in list(string.printable)], '\\x00'.encode('ascii')\n",
    "\n",
    "def force_encoding(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "    \"\"\"\n",
    "    notnull_printable = set(itm.encode(encoding) for itm in list(string.printable)\n",
    "                            if int.from_bytes(itm, sys.byteorder) != 0)\n",
    "#                             if chr(int.from_bytes(itm, sys.byteorder)) != \"\\x00\")\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    return \"\".encode(encoding).join(filter(lambda x: x in notnull_printable, inpt))\n",
    "\n",
    "\n",
    "# is_printable = , '\\x00'.encode('ascii')\n",
    "# '0xff'.encode('utf32') in is_printable\n",
    "utf32p = ''.encode('utf32').join(set(itm.encode('utf32') for itm in list(string.printable)))\n",
    "windows1252p = ''.encode('Windows-1252').join(set(itm.encode('Windows-1252') for itm in list(string.printable)))\n",
    "utf8p = list(itm.encode('utf8') for itm in list(string.printable))\n",
    "utf32p = list(itm.encode('utf32') for itm in list(string.printable))\n",
    "test = set(itm.encode('utf32') for itm in list(string.printable)\n",
    "           if int.from_bytes(itm.encode('utf32'),\n",
    "                                 'little') != '\\x00'.encode('utf32'))\n",
    "# test == utf32set\n",
    "# force_encoding(utf8p, 'Windows-1252')\n",
    "# chr(0)\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in utf32set]\n",
    "\n",
    "astring = \"    Salut\\tbébé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\"\n",
    "chardet.detect((\"Salut bébé, mon nom c'est François!\").encode())\n",
    "# weird = list(chr(itm) for itm in tuple(''.join(utf32set).encode()))[-5]\n",
    "# astring.encode('ascii')\n",
    "\n",
    "# clean_astring = ''.join(itm for itm in list(astring) if\n",
    "#                         int.from_bytes(itm.encode('utf32'), sys.byteorder) != 0)\n",
    "\n",
    "def clean_bytes(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    return ''.join(itm for itm in list(inpt) if\n",
    "                        chr(int.from_bytes(itm)) != chr(0).encode(encoding) and itm in\n",
    "                       set(itm.encode(encoding) for itm in list(string.printable)))\n",
    "# chr(int.from_bytes(weird.encode('ascii'), sys.byteorder))\n",
    "# list(astring)\n",
    "# int.from_bytes('\\\\x00'.encode(), 'big')\n",
    "# int.from_bytes('\\\\x00'.encode(), sys.byteorder) == int.from_bytes('\\\\x00'.encode(), 'little')\n",
    "maybe_null == notnull_printable\n",
    "(astring, clean_astring)\n",
    "# '\\x00'.encode('utf32')\n",
    "nareps = [chr(0).encode('ascii'), chr(0).encode('utf32'), chr(0).encode('ISO-8859-1')]\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in nareps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astring = \"    \\\\\\tSalut\\tbé bé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\\S\"\n",
    "bstring = astring.encode('utf16')\n",
    "narep = chr(0).encode(snif.get_bencod(bstring))\n",
    "chr(0).encode('utf16') == chr(0).encode(snif.get_bencod(bstring))\n",
    "[chr(itm) for itm in list(bstring)]# chardet.detect('ç'.encode())\n",
    "# [chr(itm) for itm in list(bstring) if chr(itm) != ]\n",
    "narep, bstring, bstring.replace(narep, ''.encode(snif.get_bencod(bstring)))\n",
    "tuple(zip(list(astring), list(bstring)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(int.from_bytes(''.encode(), sys.byteorder)).encode('utf32'), chr(int.from_bytes('0'.encode(), sys.byteorder)).encode('utf32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import bytes\n",
    "# # import format as fmt\n",
    "\n",
    "# def strip_null(inpt: bytes, nullrep: bytes = None, encoding: str = None):\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     nullrep = [nullrep if nullrep else snif.get_nullrep(inpt, encoding)][0]\n",
    "#     return {\n",
    "#         format(print(\"[%q]\",\n",
    "#                    bytes.Trim(byte(\" !!! Achtung! Achtung! !!! \"), \"! \")))}\n",
    "\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "#         ''.join(get_nullrep(inpt, encoding)), '').encode()\n",
    "    try:\n",
    "        return inpt.replace(chr(int.from_bytes(\n",
    "                   b\"\\x00\", sys.byteorder)).encode(encoding), ''.encode(encoding))\n",
    "    except UnidecodeError:\n",
    "        return inpt.replace(get_nullrep(inpt, encoding), ''.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbytes = b'bonjour\\xff\\xfe\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "chardet.detect(snif.strip_null(testbytes))#.encode('ISO-8859-1')\n",
    "chardet.detect(testbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nullrep(inpt, encoding: str = None) -> bytes:\n",
    "#     return [itm for itm in list(inpt) if\n",
    "#             chr(int.from_bytes(itm, sys.byteorder)).encode(encoding) == \"\\x00\".encode(encoding)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "help(codecs.lookup('utf8').streamreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest = [dict((subject[0], \n",
    "                df.from_dict(dict((itm for itm in\n",
    "                                   list(snif.scan_bytes(snif.force_utf8(\n",
    "                  row[1].bsheets, row[1].encoding)).items()) + \\\n",
    "                      [('filename', row[1].filename),\n",
    "                       (('pscid', 'dccid'), row[1].filename.split('_')[:2])])),\n",
    "                             orient = 'index'))\n",
    "               for row in tqdm(subject[1].iterrows(), desc = 'new sniff'))\n",
    "           for subject in list(test.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtest\n",
    "# # both = {**test, **newtest}\n",
    "# both = tuple({**itm[0], **itm[1]} for itm\n",
    "#              in )\n",
    "# both\n",
    "# both = tuple(zip([itm.values() for itm in test], [itm.values() for itm in newtest]))\n",
    "# both\n",
    "tuple(zip(test.values(), newtest.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_xtrct(archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = None,\n",
    "    to_xtrct: Union[str, list, tuple] = 'all',\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    to_close: bool = True,\n",
    "    withbytes: bool = False,\n",
    "    dst_path: Union[os.PathLike, str] = None) -> object:\n",
    "    dst_path = [dst_path if dst_path\n",
    "                else pjoin(os.get_cwd(),\n",
    "                           os.path.splitext(bname(archv_path))[0])][0]\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "    myzip = zipfile.ZipFile(archv_path)\n",
    "    ntpl = filter_lst_exc([ntpl if ntpl else getnametuple(myzip)][0])\n",
    "    contents = get_zip_contents(archv_path, ntpl, excllude, to_xtrct,\n",
    "                                to_close = True, withbytes = True, to_sniff = True)\n",
    "    xtrct_lst = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename\n",
    "            in filter_lst_inc(to_xtrct, list(vals.filename), sort=True)\n",
    "        ]\n",
    "    ]\n",
    "    [\n",
    "        shutil.move(\n",
    "            myzip.extract(member=row[1].src_name, path=dst_path),\n",
    "            pjoin(\n",
    "                dst_path,\n",
    "                \"_\".join(\n",
    "                    pd.Series(\n",
    "                        row[1].filename.lower().replace(\"-\", \"_\").split(\"_\")\n",
    "                    ).unique()\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        for row in tqdm(xtrct_lst.iterrows(), desc=\"extracting\")\n",
    "    ]\n",
    "    vals = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename not in xtrct_lst.values\n",
    "        ]\n",
    "    ]\n",
    "    removeEmptyFolders(dst_path, False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_printables = [chr(int.from_bytes(itm.encode(), sys.byteorder)).encode() for itm in string.printable]\n",
    "str(b_printables[-1]) == repr(b_printables[-1])\n",
    "\n",
    "def force_to(inpt, src_enc: str = None, dst_enc: str = 'utf8'):\n",
    "    inpt = get_bytes(inpt)\n",
    "    as_ints = [[line.split()]]\n",
    "    \n",
    "    \n",
    "chr(list(asheet)[0])\n",
    "list(asheet).__len__()\n",
    "# chr(int.from_bytes(b_printables[-1], sys.byteorder))\n",
    "\n",
    "# [chr(itm) for itm in list(asheet)]\n",
    "# help(chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_topdir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/'\n",
    "# big = loadfiles([apath for apath in loadimages(xpu(cimaq_topdir))\n",
    "#                  if os.path.isfile(apath)])\n",
    "# big[['dccid', 'pscid']] = [([re.compile('\\d{6}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{6}').search(row[1].fname) != None\n",
    "#                              else None][0],\n",
    "#                             [re.compile('\\d{7}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{7}').search(row[1].fname) != None\n",
    "#                              else None][0])\n",
    "#                            for row in big.iterrows()]\n",
    "'\\\\x0'.encode('utf16')\n",
    "chr(int.from_bytes(b\"\\0xff\", sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(asheet)\n",
    "\n",
    "# test2 = pd.concat([itm[1] for itm in tqdm(sorted(list(test)), desc = 'concatenate')])\n",
    "\n",
    "# test2['forced_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "#                         for row in test2.iterrows()]\n",
    "\n",
    "#.filename,\n",
    "#                    snif.scan_bytes(row[1].forced_utf8, encoding='utf'))\n",
    "#                         for row in itm[1].iterrows())\n",
    "#              for itm in tqdm(list(test.items()),  desc = 'scan & sniff utf8')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert newtest.loc['encoding'].all() == 'ascii' or 'UTF-8'\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder))\n",
    "\n",
    "b'n\\a' == 'n\\a'.encode()\n",
    "\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder)).encode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_item(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Returns null byte representation as bytes in native file encoding'''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "#     rep = chr(list(inpt)[-1]).encode(encoding)\n",
    "    last1 = inpt.splitlines()[-1].split()[-1]\n",
    "    last2 = [chr(itm).encode(encoding) for itm in\n",
    "             list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]\n",
    "    return (last1, last2)\n",
    "#     return [chr(itm) for itm in list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(chr(int.from_bytes(b'\\x00', sys.byteorder)))\n",
    "# snif.get_bencod(chr(0))\n",
    "int.from_bytes(b'\\x00', sys.byteorder)\n",
    "aguy = random.sample(list(test.values()), 1)\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aguy = list(test.values())[0]\n",
    "aguy['size_check'] = [int(row[1].file_size) == len(list(row[1].bsheets))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy['na_reps'] = [snif.get_nullrep(row[1].bsheets) for row in aguy.iterrows()]\n",
    "aguy['last_item'] = [last_item(row[1].bsheets, row[1].encoding) for row in aguy.iterrows()]\n",
    "\n",
    "aguy[['n_zbytes', 'n_scanbytes', 'chkup']] = [(int(len(list(row[1].bsheets))), int(row[1].file_size),\n",
    "                                      (int(len(list(row[1].bsheets))) == int(row[1].file_size)))\n",
    "                                     for row in aguy.iterrows()]\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aguy['utf8len'] = [len(row[1].forced_utf8.splitlines()) for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_tophalf'] = [row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))] == \\\n",
    "                                 row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]\n",
    "                              for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_bothalf'] = [[line.strip() for line in\n",
    "                               row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))]] == \\\n",
    "                                 [line.strip() for line in\n",
    "                                  row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]]\n",
    "                              for row in aguy.iterrows()]\n",
    "\n",
    "aguy['nrows_test'] = [(len(row[1].bsheets.splitlines()) == len(row[1].forced_utf8.splitlines()))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy[['missing_line_index', 'missing_line_values']] = [(row[1].nrows_test)*2 if row[1].nrows_test else\n",
    "                        sorted([(line[0], line[1]) for line in enumerate(row[1].bsheets.splitlines()) if\n",
    "                                snif.force_utf8(line[1], 'utf8') not in row[1].forced_utf8.splitlines()])\n",
    "                       for row in aguy.iterrows()]\n",
    "# aguy['eq_lines'] = [len([line[0] for line in enumerate(tuple(zip(row[1].bsheets.splitlines(),\n",
    "#                       row[1].forced_utf8.splitlines()))) if line[1][0] == line[1][1]])\n",
    "#                     == len\n",
    "#                     for row in aguy.iterrows()]\n",
    "\n",
    "aguy.last_item.iloc[6][1][0].decode('utf16').encode('utf8').decode('utf8')\n",
    "aguy.forced_utf8, aguy.bsheets\n",
    "aguy['missing_line'].iloc[5]\n",
    "# aguy.iloc[5].bsheets.splitlines(), aguy.iloc[5].forced_utf8.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asheet = aguy.iloc[4].bsheets[0]\n",
    "utfsheet = aguy.iloc[4].forced_utf8[0]\n",
    "asheet == utfsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asheet = aguy.sample(1).bsheets.values[0]\n",
    "# last_item(asheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = snif.get_bencod(abyte)\n",
    "nonull = strip_null(abyte, enc)\n",
    "nonull.splitllines()[:4], abyte.splitlines()[:4]\n",
    "# aguy['no_null'] = [strip_null(row[1].bsheets,\n",
    "#                               row[1].encoding)#.encode(row[1].encoding)\n",
    "#                    for row in aguy.iterrows()]\n",
    "# aguy['bprints'] = [snif.bytes_printable(row[1].bsheets).encode(snif.get_bencod(row[1].bsheets))\n",
    "#                  for row in aguy.iterrows()]\n",
    "# checkfx = []\n",
    "\n",
    "\n",
    "# list(abyte.splitlines()[-1])\n",
    "# [chr(itm).encode() for itm in\n",
    "#  [list(row[1].bsheets.splitlines()[-1])[-1]\n",
    "# for row in aguy.iterrows()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = list(test.values())[0].iloc[0].bsheets\n",
    "encoding = list(test.values())[0].iloc[0].encoding\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    return '\\n'.join(inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "        ''.join(snif.get_nullrep(inpt, encoding)), '').splitlines())\n",
    "\n",
    "pd.read_csv(StringIO(snif.mkfrombytes(inpt.strip()).decode()), sep = '\\t')\n",
    "#     narep =''.encode(encoding).join([itm.encode(encoding) for itm in\n",
    "#                                            snif.get_nullrep(inpt, encoding)])\n",
    "#     return inpt.replace(narep, '|'.encode(encoding))\n",
    "#         inpt = inpt.replace(narep.encode(encoding),\n",
    "#                             '|'.encode(encoding)).replace('|'.encode(encoding),\n",
    "#                                                           ''.encode(encoding))\n",
    "#         return inpt\n",
    "#     return snif.bytes_printable(inpt.replace(,\n",
    "#                                              repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "# snif.mkfrombytes(strip_null(inpt, 'utf8').encode())\n",
    "# pd.read_csv(StringIO(snif.mkfrombytes(inpt).decode(encoding)),\n",
    "#             sep='\\t', header = [0 if snif.get_has_header(strip_null(inpt).encode(encoding)) else None][0])\n",
    "# inpt.decode(snif.get_bencod(strip_null(inpt)), 'ignore').replace('�', '')\n",
    "# pd.read_csv(StringIO(), sep='\\t')\n",
    "# [[list(itm for itm in line.split() if itm not in nareps)]\n",
    "#  for line in snif.bytes_printable(inpt)]\n",
    "# def get_na_reps(inpt: bytes, encoding: str = None) -> Union[list, int, bytes]:\n",
    "#     inpt = snif.bytes_printable(inpt)\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)]\n",
    "#     return [chr(itm).encode(encoding) for itm\n",
    "#             in [chr(nulb) for nulb in\n",
    "#                 list(snif.get_nullrep(snif.bytes_printable(inpt)))][0]]\n",
    "\n",
    "# get_na_reps(inpt)\n",
    "# # [list(''.encode(encoding).join((line.split())))\n",
    "# #  for line in inpt.splitlines()]\n",
    "# chr(int.from_bytes('0xf8', sys.byteorder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 3+5 ==9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_replace import multiple_replace\n",
    "\n",
    "def clear_nullbytes(inpt: bytes, encoding: str = None):\n",
    "    inpt = snif.bytes_printable(inpt)\n",
    "    encoding = snif.get_bencod(inpt)\n",
    "    toclear = dict((itm.encode(encoding), '|'.encode(encoding))\n",
    "                   for itm in\n",
    "                   )\n",
    "\n",
    "\n",
    "    for narep in nareps:\n",
    "        inpt = inpt.replace(narep, '|'.encode(encoding))\n",
    "    return inpt\n",
    "#     return inpt.replace('|'.encode(encoding), 'nan'.encode(encoding))\n",
    "# multiple_replace(toclear, snif.bytes_printable(abyte), encoding)\n",
    "# help(str.replace)\n",
    "# b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "# b_printable\n",
    "\n",
    "inpt = list(test.values())[0].iloc[8].bsheets\n",
    "encoding = list(test.values())[0].iloc[8].encoding\n",
    "# new = snif.bytes_printable(abyte).replace(tostrip, ''.encode(encoding))\n",
    "clear_nullbytes(inpt, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def bytes_prntble(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Same as is_printable, but for bytes in native file encoding '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "\n",
    "#     b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "#     return ''.encode(encoding).join([str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding) for ch in \n",
    "#                                     list(inpt) if str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding)\n",
    "#                                     in b_printable])\n",
    "    \n",
    "# def get_nullrep(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Returns null byte representation as bytes in native file encoding'''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "\n",
    "# def strip_null(inpt: bytes, encoding: str = None, replace_val: str = None) -> bytes:\n",
    "#     ''' Remove null bytes from byte stream with proper representation\n",
    "#         Adapted from:\n",
    "#         https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "#         All files end by a null byte, so the last byte in a file shows\n",
    "#         how null bytes are represented within this file '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     repval = ['' if not replace_val else replace_val][0].encode(encoding)\n",
    "#     return bytes_prntble(inpt.replace(get_nullrep(inpt, encoding), repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "\n",
    "# snif.get_nullrep(abyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abyte = list(test.values())[0].iloc[1].bsheets\n",
    "enc = snif.get_bencod(abyte)\n",
    "# (abyte.decode(enc, 'replace').splitlines()[0], repr(abyte.decode(enc, 'replace').splitlines()[0]),\n",
    "#  repr(snif.force_utf8(abyte).splitlines()[0].decode())\n",
    "# )\n",
    "repr(abyte.decode(enc, 'replace'))\n",
    "def to_utf8(astring):\n",
    "    return unidecode(repr(astring.decode(enc, 'replace').replace('�', '').encode(\n",
    "              'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', ''))).encode()\n",
    "# len(to_utf8(abyte).splitlines()\n",
    "# )\n",
    "len(abyte.splitlines())\n",
    "new = b'\\n'.join([b'\\t'.join([to_utf8(itm) for itm in to_utf8(line).split()])\n",
    "            for line in abyte.splitlines()]).decode()\n",
    "\n",
    "df([' '.join(line.split(\"\\\\x00\")).split() for line in new.splitlines()])\n",
    "# clean_utf8 = '\\n'.join([['\\t'.join([to_utf8(itm) for itm in line.split()])]\n",
    "#               for line in abyte.splitlines()])\n",
    "# clean_utf8\n",
    "# newsheet = repr(abyte.replace(bytes(str(chr(0)), enc),\n",
    "#               str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x0'\", enc), str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x'\", enc), ''.encode(enc)).replace(\n",
    "#                       bytes(str(np.nan), enc), str(\"\").encode(enc)).decode(\n",
    "#                           'ascii', 'replace').replace('�', '').replace('ÿ', ''))\n",
    "\n",
    "# cleaned = '\\n'.join(['\\t'.join([itm.replace('\\\\x0', ' ').strip().replace('\\\\x', ' ').strip() for itm in line.split()])\n",
    "#            for line in repr(newsheet).replace('\\\\x00', ' ').splitlines()]).strip()[1:].encode().decode()\n",
    "# pd.read_csv(StringIO(unidecode(cleaned)), sep='\\t')\n",
    "# [line.replace('\\\\x00', str(np.nan)).replace('\\\\x0', str(np.nan)) for line in newsheet.splitlines() if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abyte = list(test.values())[0].iloc[2].bsheets\n",
    "# encod = list(test.values())[0].iloc[2].encoding\n",
    "\n",
    "\n",
    "    \n",
    "enc = snif.get_bencod(snif.strip_null(abyte))\n",
    "# snif.strip_null(abyte)\n",
    "# [strip_null(line) for line in strip_null(bytes_prntble(abyte)).splitlines(keepends = True)]\n",
    "\n",
    "# get_nullrep(abyte)+b'1'\n",
    "# # int.from_bytes(b'\\\\x0', 'little'), int.from_bytes(b'\\\\x0', sys.byteorder), int.from_bytes(b'\\\\x0', 'big')\n",
    "\n",
    "# # newsheet = '\\n'.join(['\\t'.join([str(itm).replace(\"'\", \"\").replace(\"'\", '\"') for itm in\n",
    "# #                                  repr(line).replace('ÿ', '\\s').replace('\\\\x0', '\\s').replace(\n",
    "# #                                      '\\\\x', '\\s').replace('\\s', '').split()])\n",
    "# #                       for line in list(test.values())[0].iloc[5].bsheets.decode(\n",
    "# #                encod, 'replace').splitlines()]).splitlines()\n",
    "# # newsheet\n",
    "# # strtest = abyte.splitlines()[0]\n",
    "\n",
    "# repr(abyte.replace(repr(chr(0)).encode(encod), ''.encode(encod)).decode(encod, 'replace').replace('�', '')).encode(\n",
    "#     'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', '').encode()\n",
    "# abyte.decode('ISO-8859-1').encode('ascii', 'ignore').decode('utf8', 'replace').replace('�', '')\n",
    "\n",
    "# unidecode(repr(abyte.decode(encod, 'replace').replace('�', '')))\n",
    "# is_printable(abyte.decode(encod, 'replace').replace('�', '').encode().replace(chr(0).encode(), ''.encode()).decode())\n",
    "\n",
    "# # new = [[unidecode(itm).encode('ascii', 'ignore').decode('ascii', 'replace').replace('�', '')\n",
    "# #         for itm in line.replace(repr(chr(0)),\n",
    "# #                                '').split()\n",
    "# #       if unidecode(is_printable(itm)) != '']\n",
    "# #      for line in abyte.decode(encod, 'replace').replace('�', '').splitlines()\n",
    "# #      if line != []]\n",
    "# # new[0], ''.join(new[0]).replace(repr(chr(0)).encode(encod), ' '.encode(encod))\n",
    "# new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in string.printable #list(string.printable)\n",
    "\n",
    "chr(int.from_bytes(b'z', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(int.from_bytes(b'\\\\x00', 'big'))\n",
    "[chr(itm).encode() for itm in list(string.printable)]\n",
    "# chr(int.from_bytes('\\\\x0'.encode('Windows-1252'), sys.byteorder))\n",
    "# chr(int.from_bytes(b'\\\\x00', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# abyte.decode('utf8', 'replace').replace('�', '').encode().decode()\n",
    "# unidecode(abyte.decode(snif.get_bencod(abyte), 'replace').replace('�', '')).encode()\n",
    "snif.force_utf8(abyte, enc).splitlines()\n",
    "clean1 = re.sub('\\\\x00', '', abyte.decode('ascii', 'replace').replace('�', '').encode('utf16').decode('utf8', 'replace'))\n",
    "# unidecode(clean1.encode().decode()).encode('utf16', 'replace')\n",
    "clean1\n",
    "[re.sub(bytes(str(np.nan), enc), bytes('', enc),\n",
    "        re.sub(bytes(\"\\x00\", enc),\n",
    "               bytes(str(np.nan), enc),\n",
    "               line)).decode('utf8', 'replace').replace('�', '').replace('\\\\', '')#unidecode(line.decode(enc, 'replace'))).encode('ascii', 'replace').decode('utf8', 'replace').replace('�', '')\n",
    "  for line in abyte.splitlines()][0].split()[0]\n",
    "# snif.force_utf8(abyte.decode(enc, 'replace'))\n",
    "#.decode(enc, 'replace')]\n",
    "# re.sub('\\x0', '', clean1)\n",
    "# unidecode(\n",
    "#           .replace('\\x00', '').replace(\n",
    "#     '\\\\x0', '').replace('\\\\x', ''))\n",
    "#.replace(b\"\\x\".decode(), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(0)\n",
    "# str(\"\\xff\")\n",
    "chr(1).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes('n\\a', 'Windows-1252')\n",
    "unidecode(b'\\x0b\\x0c'.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_tsvs(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths'])`.set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                        [loadfiles(loadimages(row[1].fpaths)).dropna(axis = 0).T\n",
    "#                         for row in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[2: 4].iterrows()]).T\n",
    "#     test = dict((grp, dict((sgrp, cimaq.groupby('subids').get_group(grp).groupby(\n",
    "#                'fname').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "#                    'subids').get_group(grp).groupby('fname').groups))\n",
    "#                 for grp in allscans.groupby('subids').groups)\n",
    "# #     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "# #                       [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "# #                 columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "# #                                   for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "# #                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "# #     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'stereonl', 4: 'behavioral',\n",
    "# #                                     5: 'confounds', 6: 'events', 7:'func'})\n",
    "#     return test\n",
    "# cimaq = load_tsvs(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq = fetch_cimaq(xpu(cimaq_dir))\n",
    "#cimaq.sort_values('dccid').set_index('dccid', drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_cimaq(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths']).set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + [loadfiles(loadimages(cimaqpath)).dropna(axis = 0)['fpaths']\n",
    "#                                   for cimaqpath in get_cimaq_dir_paths(cimaq_dir)[0].loc['fpaths'][2: 5]],\n",
    "#                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "#     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'behavioral', 4: 'confounds',\n",
    "#                                     5: 'events'})\n",
    "#     return cimaq.set_index('dccid').sort_index()#.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scans(cimaq_dir):\n",
    "    cimaq = fetch_cimaq(cimaq_dir)\n",
    "\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                   [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "#             columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "#                               for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "#                   axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "    \n",
    "    allscans = pd.concat([loadfiles(loadimages(pjoin(dname(xpu(cimaq_dir)),\n",
    "                                                     sub))).dropna(axis = 0).T\n",
    "                          for sub in [itm for itm in ls(dname(xpu(cimaq_dir)))\n",
    "                        if itm.startswith('sub-')]], axis = 1).T\n",
    "    allscans[['dccid', 'modality', 'general']] = [(bname(row[1].fpaths).split('_')[0].split('-')[1],\n",
    "                                                   bname(row[1].fpaths).split('_')[-1],\n",
    "                                                   bname(dname(row[1].fpaths)))\n",
    "                         for row in allscans.iterrows()]\n",
    "\n",
    "    test = dict((grp, dict((sgrp, allscans.groupby('dccid').get_group(grp).groupby(\n",
    "               'general').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "                   'dccid').get_group(grp).groupby('general').groups))\n",
    "                for grp in allscans.groupby('dccid').groups)\n",
    "    allscans = df.from_dict(test, orient = 'index').sort_index()\n",
    "    indexes = set.intersection(set(cimaq.index), set(allscans.index)) \n",
    "    return pd.concat([allscans.loc[indexes], cimaq.loc[indexes]], axis = 0)\n",
    "test = load_scans(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.iloc[0].fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [pd.concat([row[1][['fname']], df.from_dict(json_read(row[1].fpaths, 'r'), orient = 'index')])\n",
    "#           for row in test[0].iterrows()\n",
    "#           if row[1].ext == '.json']\n",
    "# # allparams = pd.concat(((itm.groupby('fname').get_group(grp)\n",
    "# #                  for grp in itm.groupby('fname').groups)\n",
    "# #              for itm in allparams), axis = 1)\n",
    "# # pd.concat(params, axis =1).T.sort_values('fname')\n",
    "# pd.concat(params, axis = 1).T.set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "    \"\"\"\n",
    "    Context manager to set the timestamp of the path to plus or\n",
    "    minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "    if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "    \"\"\"\n",
    "    stats = os.stat(path)\n",
    "    if set_new:\n",
    "        new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "    else:\n",
    "        new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
