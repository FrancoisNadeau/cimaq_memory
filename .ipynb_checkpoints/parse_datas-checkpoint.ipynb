{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import byte\n",
    "import chardet\n",
    "import csv\n",
    "import json\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex as re\n",
    "import string\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from chardet import UniversalDetector as udet\n",
    "from io import StringIO\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from string import printable\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "from cimaq_utils import get_cimaq_dir_paths\n",
    "from sniffbytes import flatten\n",
    "from sniffbytes import loadfiles\n",
    "from sniffbytes import loadimages\n",
    "from cimaq_utils import repair_enc_task\n",
    "from cimaq_utils import fetch_cimaq\n",
    "\n",
    "from json_read import json_read\n",
    "\n",
    "import sniffbytes as snif\n",
    "\n",
    "\n",
    "from get_zip_contents import scanzip\n",
    "from get_zip_contents import getnametuple\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "from multiple_replace import multiple_replace\n",
    "\n",
    "cimaq_dir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/CIMAQ_fmri_memory/data'\n",
    "zeprimes = pjoin(cimaq_dir, 'task_files/zipped_eprime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# import csv\n",
    "# import json\n",
    "# import nilearn\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import regex as re\n",
    "# import string\n",
    "# import struct\n",
    "# import sys\n",
    "\n",
    "# from chardet import UniversalDetector as udet\n",
    "# from io import StringIO\n",
    "# from os.path import basename as bname\n",
    "# from os.path import dirname as dname\n",
    "# from os.path import expanduser as xpu\n",
    "# from os import listdir as ls\n",
    "# from os.path import join as pjoin\n",
    "# from pandas import DataFrame as df\n",
    "# from string import printable\n",
    "# from typing import Union\n",
    "# from zipfile import ZipFile\n",
    "# from tqdm import tqdm\n",
    "# from unidecode import unidecode\n",
    "\n",
    "# import sniffbytes as snif\n",
    "\n",
    "# def scanzip(archv_path: Union[os.PathLike, str],\n",
    "#                ntpl: Union[str, list, tuple] = [],\n",
    "#                exclude: Union[str, list, tuple] = []) -> object:\n",
    "\n",
    "\n",
    "#     myzip = ZipFile(archv_path)\n",
    "#     ntpl = [ntpl if ntpl else getnametuple(myzip)][0]\n",
    "\n",
    "#     vals = df(\n",
    "#             tuple(\n",
    "#                 dict(zip(snif.evenodd(itm)[0], snif.evenodd(itm)[1]))\n",
    "#                 for itm in tuple(\n",
    "#                     tuple(\n",
    "#                         snif.is_printable(repr(itm.lower()))\n",
    "#                         .strip()\n",
    "#                         .replace(\"'\", \"\")\n",
    "#                         .replace(\"'\", \"\")\n",
    "#                         .replace(\"=\", \" \")[:-2]\n",
    "#                         .split()\n",
    "#                     )[1:]\n",
    "#                     for itm in tqdm(set(\n",
    "#                         repr(myzip.getinfo(itm))\n",
    "#                         .strip(\" \")\n",
    "#                         .replace(itm, itm.replace(\" \", \"_\"))\n",
    "#                         if \" \" in itm\n",
    "#                         else repr(myzip.getinfo(itm)).strip(\" \")\n",
    "#                         for itm in ntpl\n",
    "#                     ), desc = 'scanning archive')\n",
    "#                 )\n",
    "#             ),\n",
    "#             dtype=\"object\",\n",
    "#         ).sort_values(\"filename\").reset_index(drop=True)\n",
    "\n",
    "#     vals['src_names'] = sorted(ntpl)\n",
    "#     vals['bsheets'] = [myzip.open(row[1].src_names).read()\n",
    "#                        for row in vals.iterrows()]\n",
    "#     myzip.close()\n",
    "#     sniffed = df(snif.scan_bytes(row[1].bsheets)\n",
    "#                for row in vals.iterrows()).to_dict()\n",
    "#     return df.from_dict({**vals.to_dict(), **sniffed}, orient = 'index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sniffing:   0%|          | 0/103 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5121.95it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5875.29it/s]\n",
      "sniffing:   2%|▏         | 2/103 [00:00<00:07, 12.67it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6175.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5059.47it/s]\n",
      "sniffing:   4%|▍         | 4/103 [00:00<00:08, 12.06it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4917.12it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5674.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5788.44it/s]\n",
      "sniffing:   7%|▋         | 7/103 [00:00<00:07, 12.76it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5570.95it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5406.58it/s]\n",
      "sniffing:   9%|▊         | 9/103 [00:00<00:07, 12.76it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5734.28it/s]\n",
      "sniffing:  10%|▉         | 10/103 [00:00<00:07, 11.64it/s]\n",
      "scanning archive: 100%|██████████| 13/13 [00:00<00:00, 5516.03it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 4164.73it/s]\n",
      "sniffing:  12%|█▏        | 12/103 [00:01<00:08, 11.13it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 4432.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6501.79it/s]\n",
      "sniffing:  14%|█▎        | 14/103 [00:01<00:07, 11.18it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5237.06it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 13/13 [00:00<00:00, 3396.62it/s]\n",
      "sniffing:  16%|█▌        | 16/103 [00:01<00:07, 10.90it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6623.75it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5198.12it/s]\n",
      "sniffing:  17%|█▋        | 18/103 [00:01<00:07, 11.25it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5416.66it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5877.67it/s]\n",
      "sniffing:  19%|█▉        | 20/103 [00:01<00:07, 11.21it/s]\n",
      "scanning archive: 100%|██████████| 13/13 [00:00<00:00, 6675.56it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6228.55it/s]\n",
      "sniffing:  21%|██▏       | 22/103 [00:01<00:07, 10.45it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5722.11it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5676.50it/s]\n",
      "sniffing:  23%|██▎       | 24/103 [00:02<00:07, 10.59it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6559.75it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6418.22it/s]\n",
      "sniffing:  25%|██▌       | 26/103 [00:02<00:07, 10.77it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5603.20it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5710.85it/s]\n",
      "sniffing:  27%|██▋       | 28/103 [00:02<00:06, 11.01it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6632.36it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5616.37it/s]\n",
      "sniffing:  29%|██▉       | 30/103 [00:02<00:06, 10.65it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6283.08it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 6739.31it/s]\n",
      "sniffing:  31%|███       | 32/103 [00:02<00:06, 10.77it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5351.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5698.78it/s]\n",
      "sniffing:  33%|███▎      | 34/103 [00:03<00:06, 10.18it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6607.52it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 3971.88it/s]\n",
      "sniffing:  35%|███▍      | 36/103 [00:03<00:06, 10.67it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5560.28it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 4795.68it/s]\n",
      "sniffing:  37%|███▋      | 38/103 [00:03<00:06, 10.71it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5759.03it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 7441.11it/s]\n",
      "sniffing:  39%|███▉      | 40/103 [00:03<00:05, 10.67it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 4554.08it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4604.07it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5604.86it/s]\n",
      "sniffing:  42%|████▏     | 43/103 [00:03<00:05, 11.84it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 2559.84it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 4097.33it/s]\n",
      "sniffing:  44%|████▎     | 45/103 [00:03<00:04, 11.65it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5758.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6611.45it/s]\n",
      "sniffing:  46%|████▌     | 47/103 [00:04<00:04, 11.22it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5776.39it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6078.70it/s]\n",
      "sniffing:  48%|████▊     | 49/103 [00:04<00:04, 11.33it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5678.21it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5983.32it/s]\n",
      "sniffing:  50%|████▉     | 51/103 [00:04<00:04, 11.13it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 4813.05it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5192.40it/s]\n",
      "sniffing:  51%|█████▏    | 53/103 [00:04<00:04, 11.01it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5676.50it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5517.37it/s]\n",
      "sniffing:  53%|█████▎    | 55/103 [00:04<00:04, 11.25it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5782.85it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5698.78it/s]\n",
      "sniffing:  55%|█████▌    | 57/103 [00:05<00:04, 10.91it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6428.05it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4253.86it/s]\n",
      "sniffing:  57%|█████▋    | 59/103 [00:05<00:03, 12.33it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5680.77it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5779.93it/s]\n",
      "sniffing:  59%|█████▉    | 61/103 [00:05<00:03, 12.35it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5602.12it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5506.74it/s]\n",
      "sniffing:  61%|██████    | 63/103 [00:05<00:03, 11.68it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5509.40it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5480.06it/s]\n",
      "sniffing:  63%|██████▎   | 65/103 [00:05<00:03, 11.59it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5625.41it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5495.68it/s]\n",
      "sniffing:  65%|██████▌   | 67/103 [00:05<00:03, 11.11it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5562.74it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5571.60it/s]\n",
      "sniffing:  67%|██████▋   | 69/103 [00:06<00:03, 11.05it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5565.20it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 4972.83it/s]\n",
      "sniffing:  69%|██████▉   | 71/103 [00:06<00:02, 11.32it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6193.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5811.97it/s]\n",
      "sniffing:  71%|███████   | 73/103 [00:06<00:02, 10.92it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 3913.41it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6112.17it/s]\n",
      "sniffing:  73%|███████▎  | 75/103 [00:06<00:02, 11.07it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6544.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6770.47it/s]\n",
      "sniffing:  75%|███████▍  | 77/103 [00:06<00:02, 11.14it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5839.84it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4629.47it/s]\n",
      "sniffing:  77%|███████▋  | 79/103 [00:06<00:01, 12.30it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5426.01it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5667.98it/s]\n",
      "sniffing:  79%|███████▊  | 81/103 [00:07<00:01, 12.11it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5802.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 3885.61it/s]\n",
      "sniffing:  81%|████████  | 83/103 [00:07<00:01, 13.02it/s]\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 3057.28it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5564.21it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sniffing:  83%|████████▎ | 85/103 [00:07<00:01, 10.88it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5744.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5228.36it/s]\n",
      "sniffing:  84%|████████▍ | 87/103 [00:07<00:01, 11.08it/s]\n",
      "scanning archive: 100%|██████████| 13/13 [00:00<00:00, 6605.20it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5719.51it/s]\n",
      "sniffing:  86%|████████▋ | 89/103 [00:07<00:01, 10.98it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5491.52it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 13/13 [00:00<00:00, 5558.77it/s]\n",
      "sniffing:  88%|████████▊ | 91/103 [00:08<00:01, 11.06it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5629.94it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 6547.46it/s]\n",
      "sniffing:  90%|█████████ | 93/103 [00:08<00:00, 11.09it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5375.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5672.24it/s]\n",
      "sniffing:  92%|█████████▏| 95/103 [00:08<00:00, 11.43it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5602.12it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5423.67it/s]\n",
      "sniffing:  94%|█████████▍| 97/103 [00:08<00:00, 11.45it/s]\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 5566.76it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5559.05it/s]\n",
      "sniffing:  96%|█████████▌| 99/103 [00:08<00:00, 11.14it/s]\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5418.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5762.20it/s]\n",
      "sniffing:  98%|█████████▊| 101/103 [00:08<00:00, 11.03it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6470.47it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 10/10 [00:00<00:00, 5770.13it/s]\n",
      "sniffing: 100%|██████████| 103/103 [00:09<00:00, 11.30it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n",
      "repairing: 683it [00:11, 59.80it/s]\n",
      "creating Pandas DataFrames: 683it [00:06, 98.16it/s] \n"
     ]
    }
   ],
   "source": [
    "# cimaq_infos = df(tuple(scanzip(apath) for apath in\n",
    "#                        tqdm(loadimages(xpu(zeprimes)),\n",
    "#                             desc = 'sniffing')))\n",
    "cimaq_infos = pd.concat(val.T for val in\n",
    "                        df(tuple(scanzip(apath) for apath in\n",
    "                                 tqdm(loadimages(xpu(zeprimes)),\n",
    "                                      desc = 'sniffing')),\n",
    "                          dtype = object)[0].values.flatten()).dropna()\n",
    "\n",
    "cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode('utf').strip()\n",
    "                            \n",
    "                            \n",
    "                             for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                             desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(row[1].newsheets).split('\\\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(row[1].newsheets).split('\\\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc14</td>\n",
       "      <td>Old22</td>\n",
       "      <td>Encoding</td>\n",
       "      <td>113.022</td>\n",
       "      <td>3</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>116.022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc28</td>\n",
       "      <td>Old24</td>\n",
       "      <td>Encoding</td>\n",
       "      <td>235.011</td>\n",
       "      <td>3</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>238.011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>58</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc39</td>\n",
       "      <td>Old58</td>\n",
       "      <td>Encoding</td>\n",
       "      <td>350.501</td>\n",
       "      <td>3</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>353.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc40</td>\n",
       "      <td>Old60</td>\n",
       "      <td>Encoding</td>\n",
       "      <td>356.5</td>\n",
       "      <td>3</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>359.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    0    1      2      3         4        5  6         7        8\n",
       "0  21  Enc  Enc14  Old22  Encoding  113.022  3  Fixation  116.022\n",
       "1  39  Enc  Enc28  Old24  Encoding  235.011  3  Fixation  238.011\n",
       "2  58  Enc  Enc39  Old58  Encoding  350.501  3  Fixation    353.5\n",
       "3  59  Enc  Enc40  Old60  Encoding    356.5  3  Fixation    359.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_infos.iloc[150]['as_df']\n",
    "# with open(pjoin(os.getcwd(), 'test.txt'), 'wb') as newfile:\n",
    "#     newfile.write(cimaq_infos.iloc[124]['bsheets'])\n",
    "#     newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating Pandas DataFrames: 683it [01:06, 10.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index']).decode(\n",
    "#                                     'utf8', 'replace').replace(\n",
    "#                                         '�', '').encode().decode().strip()\n",
    "                            \n",
    "                            \n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n'\n",
    "                                            ))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TrialNumber</th>\n",
       "      <th>Category</th>\n",
       "      <th>TrialCode</th>\n",
       "      <th>OldNumber</th>\n",
       "      <th>CorrectSource</th>\n",
       "      <th>Stim_RESP</th>\n",
       "      <th>Stim_RT</th>\n",
       "      <th>Stim_ACC\\n1</th>\n",
       "      <th>Enc</th>\n",
       "      <th>Enc00</th>\n",
       "      <th>...</th>\n",
       "      <th>3</th>\n",
       "      <th>1949</th>\n",
       "      <th>0\\n120</th>\n",
       "      <th>Enc</th>\n",
       "      <th>Enc78</th>\n",
       "      <th>Old20</th>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <th>1634</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 848 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [TrialNumber, Category, TrialCode, OldNumber, CorrectSource, Stim_RESP, Stim_RT, Stim_ACC\\n1, Enc, Enc00, NaN, 8, 3, 788, 0\\n2, Enc, Enc000, NaN, 9, 3, 746, 0\\n3, CTL, CTL0, NaN, 8, 3, 743, 0\\n4, Enc, Enc01, Old56, 6, 3, 1588, 0\\n5, Enc, Enc02, Old11, 5, 3, 847, 0\\n6, Enc, Enc03, Old26, 6, 3, 1093, 0\\n7, Enc, Enc04, Old70, 9, 3, 1502, 0\\n8, Enc, Enc05, Old65, 8, 3, 966, 0\\n9, Enc, Enc06, Old52, 9, 3, 1091, 0\\n10, Enc, Enc07, Old34, 8, 3, 845, 0\\n11, CTL, CTL01, NaN, 8, 3, 719, 0\\n12, CTL, CTL02, NaN, 5, 3, 976, 0\\n13, Enc, Enc08, Old51, 6, 3, 1220, 0\\n14, CTL, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 848 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_infos.iloc[667]['as_df']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3079\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3080\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-da3cb346dcb1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcimaq_infos2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcimaq_infos\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcimaq_infos2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcimaq_infos2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m cimaq_infos2['newsheets'] = [snif.mkfrombytes(\n\u001b[1;32m      4\u001b[0m                                 \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbsheets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                 \u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3022\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3080\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3081\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3082\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3084\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtolerance\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "cimaq_infos2 = pd.concat(val.T for val in cimaq_infos[0].values.flatten())\n",
    "cimaq_infos2 = cimaq_infos2.dropna()\n",
    "cimaq_infos2['newsheets'] = [snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode()\n",
    "                             for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                             desc = 'repairing')]\n",
    "\n",
    "cimaq_infos2['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n'))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['as_df'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[99].as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['to_csv'] = [pd.read_csv(StringIO(row[1].unidecoded),\n",
    "                                      sep = '\\t',\n",
    "                                      header = [0 if row[1].has_header\n",
    "                                                else None][0],\n",
    "                                      engine = 'c')\n",
    "#                                       quoting = 1)\n",
    "                              for row in cimaq_infos2.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[667].to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['cleaned'] = ['\\n'.join(['\\t'.join(line.split()) for line in\n",
    "                                      snif.is_printable(row[1].newsheets.decode(\n",
    "                                          row[1].encoding).encode(\n",
    "                                          'utf8', 'replace').decode()).splitlines()]).encode().decode()\n",
    "                           for row in cimaq_infos2.iterrows()]\n",
    "cimaq_infos2['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pd.read_csv(StringIO(row[1].newsheets.decode(row[1].encoding)), sep = '\\t')\n",
    " for row in cimaq_infos2.iterrows()][666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(scanzip(loadimages(xpu(zeprimes))[0]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chardet.detect(b'0xf8')#0xf\n",
    "# chr(int.from_bytes(b'0xf', sys.byteorder))\n",
    "# bytes(int.from_bytes(b'0xf8', sys.byteorder))\n",
    "# import codecs\n",
    "# help(codecs)\n",
    "[chr(item) for item in list(b'0xf')]\n",
    "[itm for itm in list]\n",
    "int.from_bytes(b'0xf', sys.byteorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniffzip(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    scan_bytes(myzip.open(row[1].))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "        return pd.concat([df(snif.evenodd(repr(itm)[8:-1].strip().replace('=', ' ').split())).T[1]\n",
    "                          for itm in\n",
    "                (ZipFile(archv_path).__dict__['filelist'][1:],\n",
    "                ZipFile(archv_path).close())[0]\n",
    "                if '__MACOSX' not in repr(itm)], axis = 1).T\n",
    "\n",
    "# atest = list(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'][0].values())\n",
    "# btest = get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['filelist'][0]\n",
    "# atest# json.dumps(repr(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'].values.tolist()).split())\n",
    "# #.replace('=', '\":').replace('\\\\', '')\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])\n",
    "df(repr(itm[1])[8:-1].replace('=', ' ').split() for itm in\n",
    "      ZipFile(loadimages(xpu(zeprimes))[0]).__dict__['NameToInfo'].items()\n",
    "      if '__MACOSX' not in itm)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GOOD ONE ######################333\n",
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    return ((df(zip(tuple(itm.strip(\"'\") for itm in repr(\n",
    "               ZipFile(archv_path).getinfo(nm))[8:-1].strip().replace(\n",
    "                   b'='.decode(), ' ').split()#.replace(chr(34), '').split())\n",
    "                   for nm in snif.filter_lst_exc(\n",
    "                       exclude, [ntpl if ntpl\n",
    "                       else getnametuple(ZipFile(archv_path))][0])))),\n",
    "            ZipFile(archv_path).close())[0])\n",
    "\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])[0]\n",
    "#.replace('=', '\":').replace('\\\\', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_archv(apath: Union[str, os.PathLike], ntpl: Union[str, list, tuple] = []) -> object:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_contents(\n",
    "    archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = [],\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    withbytes: bool = False,\n",
    "    to_sniff: bool = False,\n",
    "    to_close: bool = True,\n",
    ") -> object:\n",
    "    myzip = ZipFile(archv_path)\n",
    "    ntpl = snif.filter_list_exc(exclude,\n",
    "                                [ntpl if ntpl else snif.getnametuple(myzip)][0])\n",
    "\n",
    "    vals = (\n",
    "        df(\n",
    "            tuple(\n",
    "                dict(zip(evenodd(itm)[0], evenodd(itm)[1]))\n",
    "                for itm in tuple(\n",
    "                    tuple(\n",
    "                        force_ascii(repr(itm.lower()))\n",
    "                        .strip()\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"=\", \" \")[:-2]\n",
    "                        .split()\n",
    "                    )[1:]\n",
    "                    for itm in set(\n",
    "                        repr(myzip.getinfo(itm))\n",
    "                        .strip(\" \")\n",
    "                        .replace(itm, itm.replace(\" \", \"_\"))\n",
    "                        if \" \" in itm\n",
    "                        else repr(myzip.getinfo(itm)).strip(\" \")\n",
    "                        for itm in ntpl\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            dtype=\"object\",\n",
    "        )\n",
    "        .sort_values(\"filename\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    vals[[\"src_name\", \"ext\"]] = [(nm, os.path.splitext(nm)[1]) for nm in ntpl]\n",
    "    vals[\"filename\"] = [\n",
    "        \"_\".join(\n",
    "            pd.Series(\n",
    "                row[1].filename.lower().replace(\"/\",\n",
    "                                                \"_\").replace(\"-\",\n",
    "                                                             \"_\").split(\"_\")\n",
    "            ).unique()\n",
    "            .__iter__()\n",
    "        )\n",
    "        for row in vals.iterrows()\n",
    "    ]\n",
    "    if exclude:\n",
    "        vals = vals.drop(\n",
    "            [\n",
    "                row[0]\n",
    "                for row in vals.iterrows()\n",
    "                if row[1].filename\n",
    "                not in filter_lst_exc(exclude, [itm.lower() for itm in vals.filename])\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    if withbytes:\n",
    "        vals[\"bsheets\"] = [\n",
    "#             snif.strip_null() \n",
    "            myzip.open(row[1].src_name).read().lower() for row in vals.iterrows()\n",
    "        ]\n",
    "        \n",
    "    if to_sniff:\n",
    "        vals[[\"encoding\", \"delimiter\", \"has_header\", \"width\", \"dup_index\", \"nrows\"]] = \\\n",
    "            [tuple(snif.scan_bytes(row[1].bsheets).values()) for row in vals.iterrows()]\n",
    "    if to_close:\n",
    "        myzip.close()\n",
    "        return vals\n",
    "    else:\n",
    "        return (myzip, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azpath = loadfiles(loadimages(xpu(zeprimes))).fpaths[0]\n",
    "# with zipfile.Zipfile(azpath) as myzip:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  dict((bname(apath.replace('-', '_').strip()),\n",
    "                    get_zip_contents(apath, withbytes = True,\n",
    "                         to_sniff = True, to_close = True))\n",
    "                    for apath in tqdm(loadimages(zeprimes),\n",
    "                          desc = 'scan sniff'))\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sniffbytes import no_ascii\n",
    "# # chardet.detect(no_ascii('bon matin tin�tin!').encode())\n",
    "# # Union[str, bytes]\n",
    "# ''.join([chr(int.from_bytes(int.from_bytes(itm.encode(), sys.byteorder)))#.encode(encoding)\n",
    "#  for itm in ])\n",
    "# # ==  dict(json.dumps(set(string.printable)))\n",
    "set([chr(int.from_bytes(itm, sys.byteorder)).encode('utf32')\n",
    " for itm in [itm.encode() for itm in list(string.printable)]])\n",
    "[int.from_bytes(itm.encode(), sys.byteorder) for itm in list(string.printable)[-2:]]\n",
    "# '\\x00'.encode('ISO-8859-1')\n",
    "encodings = ['Windows-1252', 'utf32', 'utf8', 'ISO-8859-1']\n",
    "[int.from_bytes(itm.encode(encoding), sys.byteorder) for itm in list(string.printable)]\n",
    "int.from_bytes(b'', sys.byteorder)\n",
    "chardet.detect(b'\\x00'), chardet.detect(b'None')\n",
    "is_printable = [itm.encode('utf32') for itm in list(string.printable)], '\\x00'.encode('ascii')\n",
    "\n",
    "def force_encoding(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "    \"\"\"\n",
    "    notnull_printable = set(itm.encode(encoding) for itm in list(string.printable)\n",
    "                            if int.from_bytes(itm, sys.byteorder) != 0)\n",
    "#                             if chr(int.from_bytes(itm, sys.byteorder)) != \"\\x00\")\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    return \"\".encode(encoding).join(filter(lambda x: x in notnull_printable, inpt))\n",
    "\n",
    "\n",
    "# is_printable = , '\\x00'.encode('ascii')\n",
    "# '0xff'.encode('utf32') in is_printable\n",
    "utf32p = ''.encode('utf32').join(set(itm.encode('utf32') for itm in list(string.printable)))\n",
    "windows1252p = ''.encode('Windows-1252').join(set(itm.encode('Windows-1252') for itm in list(string.printable)))\n",
    "utf8p = list(itm.encode('utf8') for itm in list(string.printable))\n",
    "utf32p = list(itm.encode('utf32') for itm in list(string.printable))\n",
    "test = set(itm.encode('utf32') for itm in list(string.printable)\n",
    "           if int.from_bytes(itm.encode('utf32'),\n",
    "                                 'little') != '\\x00'.encode('utf32'))\n",
    "# test == utf32set\n",
    "# force_encoding(utf8p, 'Windows-1252')\n",
    "# chr(0)\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in utf32set]\n",
    "\n",
    "astring = \"    Salut\\tbébé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\"\n",
    "chardet.detect((\"Salut bébé, mon nom c'est François!\").encode())\n",
    "# weird = list(chr(itm) for itm in tuple(''.join(utf32set).encode()))[-5]\n",
    "# astring.encode('ascii')\n",
    "\n",
    "# clean_astring = ''.join(itm for itm in list(astring) if\n",
    "#                         int.from_bytes(itm.encode('utf32'), sys.byteorder) != 0)\n",
    "\n",
    "def clean_bytes(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    return ''.join(itm for itm in list(inpt) if\n",
    "                        chr(int.from_bytes(itm)) != chr(0).encode(encoding) and itm in\n",
    "                       set(itm.encode(encoding) for itm in list(string.printable)))\n",
    "# chr(int.from_bytes(weird.encode('ascii'), sys.byteorder))\n",
    "# list(astring)\n",
    "# int.from_bytes('\\\\x00'.encode(), 'big')\n",
    "# int.from_bytes('\\\\x00'.encode(), sys.byteorder) == int.from_bytes('\\\\x00'.encode(), 'little')\n",
    "maybe_null == notnull_printable\n",
    "(astring, clean_astring)\n",
    "# '\\x00'.encode('utf32')\n",
    "nareps = [chr(0).encode('ascii'), chr(0).encode('utf32'), chr(0).encode('ISO-8859-1')]\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in nareps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astring = \"    \\\\\\tSalut\\tbé bé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\\S\"\n",
    "bstring = astring.encode('utf16')\n",
    "narep = chr(0).encode(snif.get_bencod(bstring))\n",
    "chr(0).encode('utf16') == chr(0).encode(snif.get_bencod(bstring))\n",
    "[chr(itm) for itm in list(bstring)]# chardet.detect('ç'.encode())\n",
    "# [chr(itm) for itm in list(bstring) if chr(itm) != ]\n",
    "narep, bstring, bstring.replace(narep, ''.encode(snif.get_bencod(bstring)))\n",
    "tuple(zip(list(astring), list(bstring)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(int.from_bytes(''.encode(), sys.byteorder)).encode('utf32'), chr(int.from_bytes('0'.encode(), sys.byteorder)).encode('utf32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import bytes\n",
    "# # import format as fmt\n",
    "\n",
    "# def strip_null(inpt: bytes, nullrep: bytes = None, encoding: str = None):\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     nullrep = [nullrep if nullrep else snif.get_nullrep(inpt, encoding)][0]\n",
    "#     return {\n",
    "#         format(print(\"[%q]\",\n",
    "#                    bytes.Trim(byte(\" !!! Achtung! Achtung! !!! \"), \"! \")))}\n",
    "\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "#         ''.join(get_nullrep(inpt, encoding)), '').encode()\n",
    "    try:\n",
    "        return inpt.replace(chr(int.from_bytes(\n",
    "                   b\"\\x00\", sys.byteorder)).encode(encoding), ''.encode(encoding))\n",
    "    except UnidecodeError:\n",
    "        return inpt.replace(get_nullrep(inpt, encoding), ''.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbytes = b'bonjour\\xff\\xfe\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "chardet.detect(snif.strip_null(testbytes))#.encode('ISO-8859-1')\n",
    "chardet.detect(testbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nullrep(inpt, encoding: str = None) -> bytes:\n",
    "#     return [itm for itm in list(inpt) if\n",
    "#             chr(int.from_bytes(itm, sys.byteorder)).encode(encoding) == \"\\x00\".encode(encoding)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "help(codecs.lookup('utf8').streamreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest = [dict((subject[0], \n",
    "                df.from_dict(dict((itm for itm in\n",
    "                                   list(snif.scan_bytes(snif.force_utf8(\n",
    "                  row[1].bsheets, row[1].encoding)).items()) + \\\n",
    "                      [('filename', row[1].filename),\n",
    "                       (('pscid', 'dccid'), row[1].filename.split('_')[:2])])),\n",
    "                             orient = 'index'))\n",
    "               for row in tqdm(subject[1].iterrows(), desc = 'new sniff'))\n",
    "           for subject in list(test.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtest\n",
    "# # both = {**test, **newtest}\n",
    "# both = tuple({**itm[0], **itm[1]} for itm\n",
    "#              in )\n",
    "# both\n",
    "# both = tuple(zip([itm.values() for itm in test], [itm.values() for itm in newtest]))\n",
    "# both\n",
    "tuple(zip(test.values(), newtest.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_xtrct(archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = None,\n",
    "    to_xtrct: Union[str, list, tuple] = 'all',\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    to_close: bool = True,\n",
    "    withbytes: bool = False,\n",
    "    dst_path: Union[os.PathLike, str] = None) -> object:\n",
    "    dst_path = [dst_path if dst_path\n",
    "                else pjoin(os.get_cwd(),\n",
    "                           os.path.splitext(bname(archv_path))[0])][0]\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "    myzip = zipfile.ZipFile(archv_path)\n",
    "    ntpl = filter_lst_exc([ntpl if ntpl else getnametuple(myzip)][0])\n",
    "    contents = get_zip_contents(archv_path, ntpl, excllude, to_xtrct,\n",
    "                                to_close = True, withbytes = True, to_sniff = True)\n",
    "    xtrct_lst = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename\n",
    "            in filter_lst_inc(to_xtrct, list(vals.filename), sort=True)\n",
    "        ]\n",
    "    ]\n",
    "    [\n",
    "        shutil.move(\n",
    "            myzip.extract(member=row[1].src_name, path=dst_path),\n",
    "            pjoin(\n",
    "                dst_path,\n",
    "                \"_\".join(\n",
    "                    pd.Series(\n",
    "                        row[1].filename.lower().replace(\"-\", \"_\").split(\"_\")\n",
    "                    ).unique()\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        for row in tqdm(xtrct_lst.iterrows(), desc=\"extracting\")\n",
    "    ]\n",
    "    vals = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename not in xtrct_lst.values\n",
    "        ]\n",
    "    ]\n",
    "    removeEmptyFolders(dst_path, False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_printables = [chr(int.from_bytes(itm.encode(), sys.byteorder)).encode() for itm in string.printable]\n",
    "str(b_printables[-1]) == repr(b_printables[-1])\n",
    "\n",
    "def force_to(inpt, src_enc: str = None, dst_enc: str = 'utf8'):\n",
    "    inpt = get_bytes(inpt)\n",
    "    as_ints = [[line.split()]]\n",
    "    \n",
    "    \n",
    "chr(list(asheet)[0])\n",
    "list(asheet).__len__()\n",
    "# chr(int.from_bytes(b_printables[-1], sys.byteorder))\n",
    "\n",
    "# [chr(itm) for itm in list(asheet)]\n",
    "# help(chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_topdir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/'\n",
    "# big = loadfiles([apath for apath in loadimages(xpu(cimaq_topdir))\n",
    "#                  if os.path.isfile(apath)])\n",
    "# big[['dccid', 'pscid']] = [([re.compile('\\d{6}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{6}').search(row[1].fname) != None\n",
    "#                              else None][0],\n",
    "#                             [re.compile('\\d{7}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{7}').search(row[1].fname) != None\n",
    "#                              else None][0])\n",
    "#                            for row in big.iterrows()]\n",
    "'\\\\x0'.encode('utf16')\n",
    "chr(int.from_bytes(b\"\\0xff\", sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(asheet)\n",
    "\n",
    "# test2 = pd.concat([itm[1] for itm in tqdm(sorted(list(test)), desc = 'concatenate')])\n",
    "\n",
    "# test2['forced_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "#                         for row in test2.iterrows()]\n",
    "\n",
    "#.filename,\n",
    "#                    snif.scan_bytes(row[1].forced_utf8, encoding='utf'))\n",
    "#                         for row in itm[1].iterrows())\n",
    "#              for itm in tqdm(list(test.items()),  desc = 'scan & sniff utf8')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert newtest.loc['encoding'].all() == 'ascii' or 'UTF-8'\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder))\n",
    "\n",
    "b'n\\a' == 'n\\a'.encode()\n",
    "\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder)).encode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_item(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Returns null byte representation as bytes in native file encoding'''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "#     rep = chr(list(inpt)[-1]).encode(encoding)\n",
    "    last1 = inpt.splitlines()[-1].split()[-1]\n",
    "    last2 = [chr(itm).encode(encoding) for itm in\n",
    "             list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]\n",
    "    return (last1, last2)\n",
    "#     return [chr(itm) for itm in list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(chr(int.from_bytes(b'\\x00', sys.byteorder)))\n",
    "# snif.get_bencod(chr(0))\n",
    "int.from_bytes(b'\\x00', sys.byteorder)\n",
    "aguy = random.sample(list(test.values()), 1)\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aguy = list(test.values())[0]\n",
    "aguy['size_check'] = [int(row[1].file_size) == len(list(row[1].bsheets))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy['na_reps'] = [snif.get_nullrep(row[1].bsheets) for row in aguy.iterrows()]\n",
    "aguy['last_item'] = [last_item(row[1].bsheets, row[1].encoding) for row in aguy.iterrows()]\n",
    "\n",
    "aguy[['n_zbytes', 'n_scanbytes', 'chkup']] = [(int(len(list(row[1].bsheets))), int(row[1].file_size),\n",
    "                                      (int(len(list(row[1].bsheets))) == int(row[1].file_size)))\n",
    "                                     for row in aguy.iterrows()]\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aguy['utf8len'] = [len(row[1].forced_utf8.splitlines()) for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_tophalf'] = [row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))] == \\\n",
    "                                 row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]\n",
    "                              for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_bothalf'] = [[line.strip() for line in\n",
    "                               row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))]] == \\\n",
    "                                 [line.strip() for line in\n",
    "                                  row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]]\n",
    "                              for row in aguy.iterrows()]\n",
    "\n",
    "aguy['nrows_test'] = [(len(row[1].bsheets.splitlines()) == len(row[1].forced_utf8.splitlines()))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy[['missing_line_index', 'missing_line_values']] = [(row[1].nrows_test)*2 if row[1].nrows_test else\n",
    "                        sorted([(line[0], line[1]) for line in enumerate(row[1].bsheets.splitlines()) if\n",
    "                                snif.force_utf8(line[1], 'utf8') not in row[1].forced_utf8.splitlines()])\n",
    "                       for row in aguy.iterrows()]\n",
    "# aguy['eq_lines'] = [len([line[0] for line in enumerate(tuple(zip(row[1].bsheets.splitlines(),\n",
    "#                       row[1].forced_utf8.splitlines()))) if line[1][0] == line[1][1]])\n",
    "#                     == len\n",
    "#                     for row in aguy.iterrows()]\n",
    "\n",
    "aguy.last_item.iloc[6][1][0].decode('utf16').encode('utf8').decode('utf8')\n",
    "aguy.forced_utf8, aguy.bsheets\n",
    "aguy['missing_line'].iloc[5]\n",
    "# aguy.iloc[5].bsheets.splitlines(), aguy.iloc[5].forced_utf8.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asheet = aguy.iloc[4].bsheets[0]\n",
    "utfsheet = aguy.iloc[4].forced_utf8[0]\n",
    "asheet == utfsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asheet = aguy.sample(1).bsheets.values[0]\n",
    "# last_item(asheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = snif.get_bencod(abyte)\n",
    "nonull = strip_null(abyte, enc)\n",
    "nonull.splitllines()[:4], abyte.splitlines()[:4]\n",
    "# aguy['no_null'] = [strip_null(row[1].bsheets,\n",
    "#                               row[1].encoding)#.encode(row[1].encoding)\n",
    "#                    for row in aguy.iterrows()]\n",
    "# aguy['bprints'] = [snif.bytes_printable(row[1].bsheets).encode(snif.get_bencod(row[1].bsheets))\n",
    "#                  for row in aguy.iterrows()]\n",
    "# checkfx = []\n",
    "\n",
    "\n",
    "# list(abyte.splitlines()[-1])\n",
    "# [chr(itm).encode() for itm in\n",
    "#  [list(row[1].bsheets.splitlines()[-1])[-1]\n",
    "# for row in aguy.iterrows()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = list(test.values())[0].iloc[0].bsheets\n",
    "encoding = list(test.values())[0].iloc[0].encoding\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    return '\\n'.join(inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "        ''.join(snif.get_nullrep(inpt, encoding)), '').splitlines())\n",
    "\n",
    "pd.read_csv(StringIO(snif.mkfrombytes(inpt.strip()).decode()), sep = '\\t')\n",
    "#     narep =''.encode(encoding).join([itm.encode(encoding) for itm in\n",
    "#                                            snif.get_nullrep(inpt, encoding)])\n",
    "#     return inpt.replace(narep, '|'.encode(encoding))\n",
    "#         inpt = inpt.replace(narep.encode(encoding),\n",
    "#                             '|'.encode(encoding)).replace('|'.encode(encoding),\n",
    "#                                                           ''.encode(encoding))\n",
    "#         return inpt\n",
    "#     return snif.bytes_printable(inpt.replace(,\n",
    "#                                              repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "# snif.mkfrombytes(strip_null(inpt, 'utf8').encode())\n",
    "# pd.read_csv(StringIO(snif.mkfrombytes(inpt).decode(encoding)),\n",
    "#             sep='\\t', header = [0 if snif.get_has_header(strip_null(inpt).encode(encoding)) else None][0])\n",
    "# inpt.decode(snif.get_bencod(strip_null(inpt)), 'ignore').replace('�', '')\n",
    "# pd.read_csv(StringIO(), sep='\\t')\n",
    "# [[list(itm for itm in line.split() if itm not in nareps)]\n",
    "#  for line in snif.bytes_printable(inpt)]\n",
    "# def get_na_reps(inpt: bytes, encoding: str = None) -> Union[list, int, bytes]:\n",
    "#     inpt = snif.bytes_printable(inpt)\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)]\n",
    "#     return [chr(itm).encode(encoding) for itm\n",
    "#             in [chr(nulb) for nulb in\n",
    "#                 list(snif.get_nullrep(snif.bytes_printable(inpt)))][0]]\n",
    "\n",
    "# get_na_reps(inpt)\n",
    "# # [list(''.encode(encoding).join((line.split())))\n",
    "# #  for line in inpt.splitlines()]\n",
    "# chr(int.from_bytes('0xf8', sys.byteorder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 3+5 ==9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_replace import multiple_replace\n",
    "\n",
    "def clear_nullbytes(inpt: bytes, encoding: str = None):\n",
    "    inpt = snif.bytes_printable(inpt)\n",
    "    encoding = snif.get_bencod(inpt)\n",
    "    toclear = dict((itm.encode(encoding), '|'.encode(encoding))\n",
    "                   for itm in\n",
    "                   )\n",
    "\n",
    "\n",
    "    for narep in nareps:\n",
    "        inpt = inpt.replace(narep, '|'.encode(encoding))\n",
    "    return inpt\n",
    "#     return inpt.replace('|'.encode(encoding), 'nan'.encode(encoding))\n",
    "# multiple_replace(toclear, snif.bytes_printable(abyte), encoding)\n",
    "# help(str.replace)\n",
    "# b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "# b_printable\n",
    "\n",
    "inpt = list(test.values())[0].iloc[8].bsheets\n",
    "encoding = list(test.values())[0].iloc[8].encoding\n",
    "# new = snif.bytes_printable(abyte).replace(tostrip, ''.encode(encoding))\n",
    "clear_nullbytes(inpt, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def bytes_prntble(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Same as is_printable, but for bytes in native file encoding '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "\n",
    "#     b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "#     return ''.encode(encoding).join([str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding) for ch in \n",
    "#                                     list(inpt) if str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding)\n",
    "#                                     in b_printable])\n",
    "    \n",
    "# def get_nullrep(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Returns null byte representation as bytes in native file encoding'''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "\n",
    "# def strip_null(inpt: bytes, encoding: str = None, replace_val: str = None) -> bytes:\n",
    "#     ''' Remove null bytes from byte stream with proper representation\n",
    "#         Adapted from:\n",
    "#         https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "#         All files end by a null byte, so the last byte in a file shows\n",
    "#         how null bytes are represented within this file '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     repval = ['' if not replace_val else replace_val][0].encode(encoding)\n",
    "#     return bytes_prntble(inpt.replace(get_nullrep(inpt, encoding), repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "\n",
    "# snif.get_nullrep(abyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abyte = list(test.values())[0].iloc[1].bsheets\n",
    "enc = snif.get_bencod(abyte)\n",
    "# (abyte.decode(enc, 'replace').splitlines()[0], repr(abyte.decode(enc, 'replace').splitlines()[0]),\n",
    "#  repr(snif.force_utf8(abyte).splitlines()[0].decode())\n",
    "# )\n",
    "repr(abyte.decode(enc, 'replace'))\n",
    "def to_utf8(astring):\n",
    "    return unidecode(repr(astring.decode(enc, 'replace').replace('�', '').encode(\n",
    "              'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', ''))).encode()\n",
    "# len(to_utf8(abyte).splitlines()\n",
    "# )\n",
    "len(abyte.splitlines())\n",
    "new = b'\\n'.join([b'\\t'.join([to_utf8(itm) for itm in to_utf8(line).split()])\n",
    "            for line in abyte.splitlines()]).decode()\n",
    "\n",
    "df([' '.join(line.split(\"\\\\x00\")).split() for line in new.splitlines()])\n",
    "# clean_utf8 = '\\n'.join([['\\t'.join([to_utf8(itm) for itm in line.split()])]\n",
    "#               for line in abyte.splitlines()])\n",
    "# clean_utf8\n",
    "# newsheet = repr(abyte.replace(bytes(str(chr(0)), enc),\n",
    "#               str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x0'\", enc), str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x'\", enc), ''.encode(enc)).replace(\n",
    "#                       bytes(str(np.nan), enc), str(\"\").encode(enc)).decode(\n",
    "#                           'ascii', 'replace').replace('�', '').replace('ÿ', ''))\n",
    "\n",
    "# cleaned = '\\n'.join(['\\t'.join([itm.replace('\\\\x0', ' ').strip().replace('\\\\x', ' ').strip() for itm in line.split()])\n",
    "#            for line in repr(newsheet).replace('\\\\x00', ' ').splitlines()]).strip()[1:].encode().decode()\n",
    "# pd.read_csv(StringIO(unidecode(cleaned)), sep='\\t')\n",
    "# [line.replace('\\\\x00', str(np.nan)).replace('\\\\x0', str(np.nan)) for line in newsheet.splitlines() if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abyte = list(test.values())[0].iloc[2].bsheets\n",
    "# encod = list(test.values())[0].iloc[2].encoding\n",
    "\n",
    "\n",
    "    \n",
    "enc = snif.get_bencod(snif.strip_null(abyte))\n",
    "# snif.strip_null(abyte)\n",
    "# [strip_null(line) for line in strip_null(bytes_prntble(abyte)).splitlines(keepends = True)]\n",
    "\n",
    "# get_nullrep(abyte)+b'1'\n",
    "# # int.from_bytes(b'\\\\x0', 'little'), int.from_bytes(b'\\\\x0', sys.byteorder), int.from_bytes(b'\\\\x0', 'big')\n",
    "\n",
    "# # newsheet = '\\n'.join(['\\t'.join([str(itm).replace(\"'\", \"\").replace(\"'\", '\"') for itm in\n",
    "# #                                  repr(line).replace('ÿ', '\\s').replace('\\\\x0', '\\s').replace(\n",
    "# #                                      '\\\\x', '\\s').replace('\\s', '').split()])\n",
    "# #                       for line in list(test.values())[0].iloc[5].bsheets.decode(\n",
    "# #                encod, 'replace').splitlines()]).splitlines()\n",
    "# # newsheet\n",
    "# # strtest = abyte.splitlines()[0]\n",
    "\n",
    "# repr(abyte.replace(repr(chr(0)).encode(encod), ''.encode(encod)).decode(encod, 'replace').replace('�', '')).encode(\n",
    "#     'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', '').encode()\n",
    "# abyte.decode('ISO-8859-1').encode('ascii', 'ignore').decode('utf8', 'replace').replace('�', '')\n",
    "\n",
    "# unidecode(repr(abyte.decode(encod, 'replace').replace('�', '')))\n",
    "# is_printable(abyte.decode(encod, 'replace').replace('�', '').encode().replace(chr(0).encode(), ''.encode()).decode())\n",
    "\n",
    "# # new = [[unidecode(itm).encode('ascii', 'ignore').decode('ascii', 'replace').replace('�', '')\n",
    "# #         for itm in line.replace(repr(chr(0)),\n",
    "# #                                '').split()\n",
    "# #       if unidecode(is_printable(itm)) != '']\n",
    "# #      for line in abyte.decode(encod, 'replace').replace('�', '').splitlines()\n",
    "# #      if line != []]\n",
    "# # new[0], ''.join(new[0]).replace(repr(chr(0)).encode(encod), ' '.encode(encod))\n",
    "# new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in string.printable #list(string.printable)\n",
    "\n",
    "chr(int.from_bytes(b'z', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(int.from_bytes(b'\\\\x00', 'big'))\n",
    "[chr(itm).encode() for itm in list(string.printable)]\n",
    "# chr(int.from_bytes('\\\\x0'.encode('Windows-1252'), sys.byteorder))\n",
    "# chr(int.from_bytes(b'\\\\x00', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# abyte.decode('utf8', 'replace').replace('�', '').encode().decode()\n",
    "# unidecode(abyte.decode(snif.get_bencod(abyte), 'replace').replace('�', '')).encode()\n",
    "snif.force_utf8(abyte, enc).splitlines()\n",
    "clean1 = re.sub('\\\\x00', '', abyte.decode('ascii', 'replace').replace('�', '').encode('utf16').decode('utf8', 'replace'))\n",
    "# unidecode(clean1.encode().decode()).encode('utf16', 'replace')\n",
    "clean1\n",
    "[re.sub(bytes(str(np.nan), enc), bytes('', enc),\n",
    "        re.sub(bytes(\"\\x00\", enc),\n",
    "               bytes(str(np.nan), enc),\n",
    "               line)).decode('utf8', 'replace').replace('�', '').replace('\\\\', '')#unidecode(line.decode(enc, 'replace'))).encode('ascii', 'replace').decode('utf8', 'replace').replace('�', '')\n",
    "  for line in abyte.splitlines()][0].split()[0]\n",
    "# snif.force_utf8(abyte.decode(enc, 'replace'))\n",
    "#.decode(enc, 'replace')]\n",
    "# re.sub('\\x0', '', clean1)\n",
    "# unidecode(\n",
    "#           .replace('\\x00', '').replace(\n",
    "#     '\\\\x0', '').replace('\\\\x', ''))\n",
    "#.replace(b\"\\x\".decode(), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(0)\n",
    "# str(\"\\xff\")\n",
    "chr(1).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes('n\\a', 'Windows-1252')\n",
    "unidecode(b'\\x0b\\x0c'.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_tsvs(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths'])`.set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                        [loadfiles(loadimages(row[1].fpaths)).dropna(axis = 0).T\n",
    "#                         for row in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[2: 4].iterrows()]).T\n",
    "#     test = dict((grp, dict((sgrp, cimaq.groupby('subids').get_group(grp).groupby(\n",
    "#                'fname').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "#                    'subids').get_group(grp).groupby('fname').groups))\n",
    "#                 for grp in allscans.groupby('subids').groups)\n",
    "# #     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "# #                       [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "# #                 columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "# #                                   for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "# #                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "# #     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'stereonl', 4: 'behavioral',\n",
    "# #                                     5: 'confounds', 6: 'events', 7:'func'})\n",
    "#     return test\n",
    "# cimaq = load_tsvs(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq = fetch_cimaq(xpu(cimaq_dir))\n",
    "#cimaq.sort_values('dccid').set_index('dccid', drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_cimaq(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths']).set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + [loadfiles(loadimages(cimaqpath)).dropna(axis = 0)['fpaths']\n",
    "#                                   for cimaqpath in get_cimaq_dir_paths(cimaq_dir)[0].loc['fpaths'][2: 5]],\n",
    "#                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "#     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'behavioral', 4: 'confounds',\n",
    "#                                     5: 'events'})\n",
    "#     return cimaq.set_index('dccid').sort_index()#.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scans(cimaq_dir):\n",
    "    cimaq = fetch_cimaq(cimaq_dir)\n",
    "\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                   [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "#             columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "#                               for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "#                   axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "    \n",
    "    allscans = pd.concat([loadfiles(loadimages(pjoin(dname(xpu(cimaq_dir)),\n",
    "                                                     sub))).dropna(axis = 0).T\n",
    "                          for sub in [itm for itm in ls(dname(xpu(cimaq_dir)))\n",
    "                        if itm.startswith('sub-')]], axis = 1).T\n",
    "    allscans[['dccid', 'modality', 'general']] = [(bname(row[1].fpaths).split('_')[0].split('-')[1],\n",
    "                                                   bname(row[1].fpaths).split('_')[-1],\n",
    "                                                   bname(dname(row[1].fpaths)))\n",
    "                         for row in allscans.iterrows()]\n",
    "\n",
    "    test = dict((grp, dict((sgrp, allscans.groupby('dccid').get_group(grp).groupby(\n",
    "               'general').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "                   'dccid').get_group(grp).groupby('general').groups))\n",
    "                for grp in allscans.groupby('dccid').groups)\n",
    "    allscans = df.from_dict(test, orient = 'index').sort_index()\n",
    "    indexes = set.intersection(set(cimaq.index), set(allscans.index)) \n",
    "    return pd.concat([allscans.loc[indexes], cimaq.loc[indexes]], axis = 0)\n",
    "test = load_scans(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.iloc[0].fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [pd.concat([row[1][['fname']], df.from_dict(json_read(row[1].fpaths, 'r'), orient = 'index')])\n",
    "#           for row in test[0].iterrows()\n",
    "#           if row[1].ext == '.json']\n",
    "# # allparams = pd.concat(((itm.groupby('fname').get_group(grp)\n",
    "# #                  for grp in itm.groupby('fname').groups)\n",
    "# #              for itm in allparams), axis = 1)\n",
    "# # pd.concat(params, axis =1).T.sort_values('fname')\n",
    "# pd.concat(params, axis = 1).T.set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "    \"\"\"\n",
    "    Context manager to set the timestamp of the path to plus or\n",
    "    minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "    if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "    \"\"\"\n",
    "    stats = os.stat(path)\n",
    "    if set_new:\n",
    "        new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "    else:\n",
    "        new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
