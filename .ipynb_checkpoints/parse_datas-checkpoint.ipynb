{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from blind_rename import rename_imposter_cols\n",
    "\n",
    "from cimaq_utils import flatten\n",
    "from cimaq_utils import get_cimaq_dir_paths\n",
    "from cimaq_utils import loadfiles\n",
    "from cimaq_utils import loadimages\n",
    "from cimaq_utils import sortmap\n",
    "\n",
    "from sniffbytes import *\n",
    "\n",
    "# from zipctl import uzipfiles\n",
    "# from zipctl import getnamelist\n",
    "# from zipctl import getinfolist\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import struct\n",
    "from chardet import UniversalDetector as udet\n",
    "from io import StringIO\n",
    "from os.path import expanduser as xpu\n",
    "from os.path import join as pjoin\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "from multiple_replace import multiple_replace\n",
    "cimaq_dir= '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_derivatives'\n",
    "qc_ok = sorted([str(itm[0]) for itm in\n",
    "                pd.read_csv(get_cimaq_dir_paths(\n",
    "                    cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "qc_ok\n",
    "to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "                     str(bname(itm).split('_')[1]), itm) for itm in\n",
    "                    loadimages(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths)\n",
    "                    if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "               columns = ['pscid', 'dccid', 'fpaths']).set_index(\n",
    "                   'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "# assert len(subs2load) == len(qc_ok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning: 100%|██████████| 94/94 [00:01<00:00, 89.16it/s]\n",
      "sniffing: 100%|██████████| 94/94 [00:02<00:00, 32.95it/s]\n",
      "repairing: 284it [00:01, 256.01it/s]\n",
      "saving: 284it [00:00, 687.25it/s]\n"
     ]
    }
   ],
   "source": [
    "repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "               get_cimaq_dir_paths(cimaq_dir)[0].events_dir.fpaths,\n",
    "               exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sniffing: 100%|██████████| 284/284 [00:00<00:00, 743.39it/s]\n"
     ]
    }
   ],
   "source": [
    "newfiles = scansniff(get_cimaq_dir_paths(cimaq_dir)[0].events_dir.fpaths)\n",
    "newfiles[['pscid', 'dccid', 'subid']] = [fname.split('_')[:2]+['_'.join(fname.split('_')[:2])]\n",
    "                                         for fname in newfiles.fname]\n",
    "newfiles = newfiles.sort_values('subid')\n",
    "newfiles['newsheets'] = [pd.read_csv(row[1].fpaths, sep = '\\t', dtype=str,\n",
    "                                     header = [0 if row[1].has_header else None][0]).dropna(axis = 1, how = 'all')\n",
    "\n",
    "                         for row in newfiles.sort_values('fname').iterrows()]\n",
    "newfiles['shapes'] = [row[1].newsheets.shape for row in newfiles.iterrows()]\n",
    "newfiles = newfiles.iloc[[row[0] for row in newfiles.iterrows()\n",
    "                          if row[1].shapes[0] > 117*0.9]]\n",
    "# newfiles = newfiles.set_index('subid', drop=True).sort_index()\n",
    "newfiles[['onset_event', 'output_responses', 'output_retrieval']] = \\\n",
    "    [[suffix in row[1].fname for suffix in\n",
    "     ['onset_event', 'output_responses', 'output_retrieval']]\n",
    "     for row in newfiles.iterrows()]\n",
    "# newfiles = tuple(group for group in newfiles[['subid', 'newsheets', 'shapes']].groupby('subid').groups)\n",
    "# testnew = newfiles\n",
    "test = (newfiles.groupby('subid').get_group(grp).reset_index(drop = True) for grp\n",
    "        in newfiles.reset_index().groupby('subid').groups)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "testitem = next(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       0    1      2        3  4        5     6\n",
       " 0      1  ctl    NaN    0.025  3    3.034   1.5\n",
       " 1      2  enc    NaN    4.521  3     7.53   0.5\n",
       " 2      3  enc    NaN    8.032  3   11.041   0.5\n",
       " 3      4  ctl    NaN   11.542  3   14.551   5.5\n",
       " 4      5  enc  old60   20.051  3    23.06   0.5\n",
       " ..   ...  ...    ...      ... ..      ...   ...\n",
       " 115  116  enc  old45   694.41  3  697.419     6\n",
       " 116  117  ctl    NaN   703.42  3  706.429  10.5\n",
       " 117  118  enc  old18  716.927  3  719.936     5\n",
       " 118  119  enc  old33  724.934  3  727.943     1\n",
       " 119  120  enc  old14  728.929  3  731.938  18.5\n",
       " \n",
       " [120 rows x 7 columns],\n",
       "     trialnumber category trialcode oldnumber correctsource stim_resp stim_rt\n",
       " 0             1      ctl      ctl0       NaN             5         1    1183\n",
       " 1             2      enc     enc00       NaN             8         1    2759\n",
       " 2             3      enc    enc000       NaN             9         1    1656\n",
       " 3             4      ctl     ctl01       NaN             8         1    1402\n",
       " 4             5      enc     enc01     old60             6       NaN       0\n",
       " ..          ...      ...       ...       ...           ...       ...     ...\n",
       " 115         116      enc     enc75     old45             6         1    1974\n",
       " 116         117      ctl     ctl39       NaN             9         1     836\n",
       " 117         118      enc     enc76     old18             5         1    1201\n",
       " 118         119      enc     enc77     old33             5         1    1249\n",
       " 119         120      enc     enc78     old14             8         1    1150\n",
       " \n",
       " [120 rows x 7 columns],\n",
       "     category                          stim oldnumber recognition_acc  \\\n",
       " 0        old            animal_penguin.bmp     old10               1   \n",
       " 1        old      sporting_bicycle_old.bmp     old55               0   \n",
       " 2        old           fruit_lemon_new.bmp     old69               1   \n",
       " 3        new            vegie_broccoli.bmp     new36               1   \n",
       " 4        old          animal_zebra_old.bmp     old13               1   \n",
       " ..       ...                           ...       ...             ...   \n",
       " 112      new       kitchen_spatula_old.bmp     new37               0   \n",
       " 113      old        musical_violin_old.bmp     old51               1   \n",
       " 114      old              vegie_carrot.bmp     old78               1   \n",
       " 115      old  sporting_karate_clothing.bmp     old60               1   \n",
       " 116      old        fruit_green_grapes.bmp     old68               0   \n",
       " \n",
       "     recognition_resp recognition_rt spatial_resp spatial_rt  \n",
       " 0                  1           3216            8        462  \n",
       " 1                  2           9409            8        462  \n",
       " 2                  1           2746            5       4435  \n",
       " 3                  2           1633            5       4435  \n",
       " 4                  1           1311            8        369  \n",
       " ..               ...            ...          ...        ...  \n",
       " 112                1           7013            5       1150  \n",
       " 113                1           2255            8       1748  \n",
       " 114                1           1765            8        414  \n",
       " 115                1           4964            6        518  \n",
       " 116                2           1240            6        518  \n",
       " \n",
       " [117 rows x 8 columns])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onset, output, retrieval = testitem.T[0].newsheets, testitem.T[1].newsheets.drop(['stim_acc'], axis = 1), testitem.T[2].newsheets\n",
    "# output.columns = range(output.shape[1])\n",
    "# output = output.drop([2], axis = 1)\n",
    "assert all([row[1][1][0] == row[1][2][0] == row[1][4][0] for row in onset.iterrows()])\n",
    "onset = onset.drop([2, 4, 7], axis = 1)\n",
    "onset = onset.rename(dict(((itm[1], itm[0]) for itm in enumerate(onset.columns))), axis =1)\n",
    "onset, output, retrieval\n",
    "\n",
    "#.newsheets.iloc[0].drop(4, axis=1).rename({5:'onset_fix'}, axis = 1)\n",
    "# testitem.newsheets.iloc[1].drop('stim_resp', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 3: 2, 5: 3, 6: 4, 8: 5, 9: 6}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(((itm[1], itm[0]) for itm in enumerate(onset.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fname                                        4509950_108391\n",
       " onsets        category                         stim oldn...\n",
       " Name: 0, dtype: object,\n",
       " fname                                         4509950_108391\n",
       " outputs        trialnumber category trialcode oldnumber c...\n",
       " Name: 1, dtype: object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onsets, outputs = evenodd([item[1] for item in\n",
    "                           list(pd.concat([item[1].reset_index(\n",
    "                               drop = True)[['fname', 'newsheets']].T\n",
    "                                           for item in\n",
    "                                           sorted(list(newfiles.groupby('dccid')))],\n",
    "                                    axis = 1).iteritems())])\n",
    "onsets, outputs = pd.concat(onsets, axis = 1).T.rename(\n",
    "                      columns = {'newsheets': 'onsets'}), \\\n",
    "                  pd.concat(outputs, axis = 1).T.rename(\n",
    "                      columns = {'newsheets': 'outputs'})\n",
    "onsets['fname'] = ['_'.join(fname.split('_')[:2])\n",
    "                   for fname in onsets.fname]\n",
    "outputs['fname'] = ['_'.join(fname.split('_')[:2])\n",
    "                    for fname in outputs.fname]\n",
    "\n",
    "onsets.iloc[0], outputs.iloc[0]\n",
    "# enc = pd.merge(onsets, outputs, on = 'fname')\n",
    "# (enc.fname[0], enc.onsets[0]), enc.outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1a63c89238f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m display(pd.Series([row[1].outputs.shape for row in enc.iterrows()]).unique(),\n\u001b[0m\u001b[1;32m      2\u001b[0m pd.Series([row[1].onsets.shape for row in enc.iterrows()]).unique())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc' is not defined"
     ]
    }
   ],
   "source": [
    "display(pd.Series([row[1].outputs.shape for row in enc.iterrows()]).unique(),\n",
    "pd.Series([row[1].onsets.shape for row in enc.iterrows()]).unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f4a3647edc58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                    [itm[1].values for itm in\n\u001b[1;32m      5\u001b[0m                     row[1].outputs.iteritems()]))]\n\u001b[0;32m----> 6\u001b[0;31m                    for row in enc.iterrows()]\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m enc['renamers'] = [dict(tuple([itm[0] if itm[1] else\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "enc['booltest'] = [[bool(itm[0].all() == itm[1].all()) for\n",
    "                    itm in tuple(zip([itm[1].values for itm in\n",
    "                    row[1].onsets.iteritems()],\n",
    "                   [itm[1].values for itm in\n",
    "                    row[1].outputs.iteritems()]))]\n",
    "                   for row in enc.iterrows()]\n",
    "\n",
    "enc['renamers'] = [dict(tuple([itm[0] if itm[1] else\n",
    "                              (itm[0][0], 'col_'+str(itm[0][0]))][0]\n",
    "                               for itm in \n",
    "                    dict(zip(enumerate(row[1].outputs.columns),\n",
    "                             row[1].booltest)).items()))\n",
    "                   for row in enc.iterrows()]\n",
    "# enc['renamers'] = [df.from_dict(row[1].renamers, orient='index').rename(dict(tuple(zip)))]\n",
    "enc['onsets'] = [row[1].onsets.rename(row[1].renamers, axis = 1)\n",
    "                 for row in enc.iterrows()]\n",
    "# enc['fullevents'] = [pd.concat([row[1].onsets.fillna(False),\n",
    "#                                 row[1].outputs.fillna(False)],\n",
    "#                                axis = 1).convert_dtypes(str).T.drop_duplicates().T\n",
    "#                      for row in enc.iterrows()]\n",
    "# enc['fullevents'] =  [row[1].fullevents.set_index(list(row[1].fullevents.columns)[0])\n",
    "#                          for row in enc.iterrows()]\n",
    "# enc['onsets'] = [row[1].onsets.rename(columns = )\n",
    "#                  for row in enc.iterrows()]\n",
    "# enc['onsets'].iloc[0]\n",
    "#              row[1].onsets.iteritems()],\n",
    "#             [itm[1].values for itm in\n",
    "#              row[1].outputs.iteritems()]))\n",
    "#                    for row in enc.iterrows())\n",
    "# booltest[1]\n",
    "# enc.iloc[0].onsets, enc.iloc[0].outputs\n",
    "# enc['onsets'].iloc[0]\n",
    "# encoding_phase = pd.merge(onsets['onsets'].sort_values('fname'),\n",
    "#                           outputs['outputs'].sort_values('fname'), on = 'fname', how = 'outer')\n",
    "# encoding_phase\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[list(row[1]['fullevents'].index) for row in enc.iterrows()]\n",
    "enc['onsets'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = list(outputs[0].iloc[1].columns)\n",
    "\n",
    "test = tuple(zip([row[1].onsets.fillna('NO').values for itm in\n",
    "                  list(onsets[0].newsheets.iteritems())],\n",
    "          [itm[1].fillna('NO').values for itm in\n",
    "           list(outputs.newsheets.iteritems())]))\n",
    "renamer = dict(itm[0] for itm in\n",
    "               tuple(zip(enumerate(colnames),\n",
    "                         [itm[0].all() == itm[1].all()\n",
    "                          for itm in test])) if itm[1])\n",
    "# onsets = [(row[].fname, row[1].newsheets.rename(renamer, axis = 1))\n",
    "#           for row in pd.concat(onsets, axis = 1).T.iterrows()]\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onsets = pd.concat(onsets, axis = 1, ignore_index = True).T#.rename(columns = {'newsheets': 'onsets'})\n",
    "outputs = pd.concat(outputs, axis = 1, ignore_index = True).T#.rename({'newsheets': 'outputs'}, axis = 1)\n",
    "# sheets = pd.concat([pd.concat(onsets, axis = 1, ignore_index = True).rename({'newsheets': 'onsets'}, axis = 1),\n",
    "#                     pd.concat(outputs, axis = 1, ignore_index = True).rename({'newsheets': 'outputs'}, axis = 1)],\n",
    "#                    axis = 1).T.rename(columns={0: 'onsets', 1: 'outputs'})\n",
    "# sheets\n",
    "outputs#, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# onsets = df(tuple((row[1].fname, )\n",
    "#                for row in onsets.iterrows()]\n",
    "# onsets[0], outputs[0]\n",
    "\n",
    "# outputs[1].iloc[1]\n",
    "# merge_test = [pd.merge(itm[0], itm[1], on = list(renamer.values())[0]) for itm in tuple(zip([itm[1] for itm in onsets],\n",
    "#                                                        [itm[1] for itm in outputs]))]\n",
    "# merge_test\n",
    "# merge_test = [(pd.merge(itm[1][0], itm[1][1])) for itm in\n",
    "#               tuple(zip(onsets.items(), outputs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Fields: trial type, onset time and duration of the events\n",
    "newfiles = scansniff(get_cimaq_dir_paths(cimaq_dir)[0].events_dir.fpaths)\n",
    "\n",
    "# newfiles['has_header'] = [[0 if row[1].has_header else None][0] for row in newfiles.iterrows()]\n",
    "\n",
    "\n",
    "# newfiles\n",
    "\n",
    "# newfiles = tuple(item[1][['fname', 'newsheets']].loc[[row[0] for row in item[1].iterrows()\n",
    "#                                                       if not row[1].output_retrieval]].sort_values(\n",
    "#                                                           'fname').reset_index(drop = True)\n",
    "#                  for item in tuple(newfiles.groupby('dccid')))\n",
    "\n",
    "\n",
    "# checkup = [item.loc[0].newsheets[3].values == item.loc[1].newsheets['trial_number'].values\n",
    "#            for item in newfiles]\n",
    "# checkup\n",
    "\n",
    "# onset_event = newfiles.iloc[[row[0] for row in newfiles.iterrows()\n",
    "#                              if row[1].onset_event]]\n",
    "# output_responses = newfiles.iloc[[row[0] for row in newfiles.iterrows()\n",
    "#                              if row[1].output_responses]]\n",
    "# tocheck = tuple(zip(onset_event.newsheets.iteritems(),\n",
    "#                     output_responses.newsheets.iteritems()))\n",
    "# # tocheck[0][0].values == tocheck[0][1].values\n",
    "# tocheck\n",
    "\n",
    "# checkup = [item[0].fillna(0).values == item[1].fillna(0).values for item in tocheck]\n",
    "# checkup\n",
    "# newfiles['newsheets'] = [item[1] for item in\n",
    "#                          sorted([(row[1].fname,\n",
    "#                                   pd.read_csv(row[1].fpath, sep = '\\t'))\n",
    "#                          for fpath in item[1].fpaths])]\n",
    "# newfiles = sorted(list(newfiles.groupby('dccid')))\n",
    "# newfiles = [(itm[0], itm[1])]\n",
    "# participants = [[force_utf8(fpath).decode()\n",
    "#                 for fpath in item[1].fpaths] for item in\n",
    "#                 ]\n",
    "# newfiles = [(itm[0], [(row[1].fname,\n",
    "#                                pd.read_csv(row[1].fpaths, '\\t').dropna(axis = 1, how = 'all'))\n",
    "#                               for row in itm[1].iterrows()])\n",
    "#             for itm in newfiles]\n",
    "newfiles[0].iloc[1].newsheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(newfiles[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display((newfiles[0][2][0][0], newfiles[0][2][0][1]),\n",
    "(newfiles[0][2][1][0], newfiles[0][2][1][1]),\n",
    "(newfiles[0][2][2][0], newfiles[0][2][2][1]))\n",
    "\n",
    "[[sheet[2][1][0].values == sheet[2][1][1].values]\n",
    " for sheet in newfiles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test = pd.concat((pd.concat((df(row[1]),\n",
    "#                       df.from_dict(scan_bytes(row[1].newsheets),orient = 'index')), axis=1)\n",
    "#            for row in sctest.iterrows()), axis=0)\n",
    "test = scansniff(folderpath = get_cimaq_dir_paths(cimaq_dir)[0].events_dir.fpaths)\n",
    "testzip = scansniff_zip(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths)\n",
    "testzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qcok = pd.read_csv(get_cimaq_dir_paths(cimaq_dir)[0].mean_qc.fpaths)\n",
    "new_files = loadfiles(loadimages(get_cimaq_dir_paths(cimaq_dir)[0].events_dir.fpaths))\n",
    "testbytes = [scan_bytes(get_bytes(apath)) for apath in new_files.fpaths]\n",
    "# meansheets = [len(get_bytes(apath)) for apath in\n",
    "#               .fpaths\n",
    "#               if len(get_bytes(apath)) > 1]\n",
    "# meansheets\n",
    "uzeprimes_files\n",
    "testbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_ids = df([bname(itm).split('_')[:2] for itm in\n",
    "                loadimages(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths)],\n",
    "              columns=['pscid', 'dccid'])\n",
    "cimaq_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dlst, patterns, prefixes= get_cimaq_dir_paths(cimaq_dir)\n",
    "patterns, prefixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.sort_values('dccid').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.compile(get_cimaq_dir_paths(cimaq_dir)[1].loc['dccid'].patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = loadfiles(loadimages(get_cimaq_dir_paths(cimaq_dir)[0].confdir.fpaths))\n",
    "confs['dccid'] = [re.compile('\\d{6}').search(fname).group()\n",
    "                  for fname in confs.fname]\n",
    "confs['newname'] = [re.sub('sub', 'sub_', fname)\n",
    "                    for fname in confs.fname]\n",
    "# nnames = [re.sub('\\w\\d', '\\w_\\d', fname).split('_') for fname in confs.fname]\n",
    "confs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series([itm[1].shape for itm in infos]).unique()\n",
    "[itm for itm in infos if itm[1].shape == (0, 0)][0][1].empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sheet_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "    \"\"\"\n",
    "    Context manager to set the timestamp of the path to plus or\n",
    "    minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "    if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "    \"\"\"\n",
    "    stats = os.stat(path)\n",
    "    if set_new:\n",
    "        new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "    else:\n",
    "        new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
