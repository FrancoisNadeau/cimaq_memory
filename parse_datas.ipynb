{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/.local/lib/python3.6/site-packages/nilearn-0.7.0-py3.6.egg/nilearn/glm/__init__.py:56: FutureWarning: The nilearn.glm module is experimental. It may change in any future release of Nilearn.\n",
      "  'It may change in any future release of Nilearn.', FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import chardet\n",
    "import csv\n",
    "import json\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex as re\n",
    "import string\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from chardet import UniversalDetector as udet\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from string import printable\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from operator import itemgetter\n",
    "\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix as mfldm\n",
    "from nilearn.glm.first_level import first_level_from_bids\n",
    "from nilearn.plotting import plot_design_matrix as plot1dm\n",
    "\n",
    "from sniffbytes import flatten\n",
    "from sniffbytes import loadfiles\n",
    "from sniffbytes import loadimages\n",
    "\n",
    "from bidsify_utils import bidsify_load_scans\n",
    "from bidsify_utils import bidsify_names\n",
    "\n",
    "from json_read import json_read\n",
    "\n",
    "import sniffbytes as snif\n",
    "\n",
    "from scanzip import scanzip\n",
    "\n",
    "\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "from multiple_replace import multiple_replace\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###############################################################################\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "282"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_dir = xpu('~/../../data/simexp/datasets/cimaq_03-19/')\n",
    "# drv_dir = pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data')\n",
    "print(\"#\"*79)\n",
    "testing = snif.loadfiles(snif.loadimages(pjoin(os.getcwd(), 'newtest')))\n",
    "testing\n",
    "94*3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 843.33it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1342.82it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 874.42it/s]\n",
      "  3%|▎         | 3/94 [00:00<00:03, 27.74it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1261.07it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1920.47it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1490.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1187.07it/s]\n",
      "  7%|▋         | 7/94 [00:00<00:02, 29.34it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 901.61it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1371.36it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1237.87it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1560.67it/s]\n",
      " 12%|█▏        | 11/94 [00:00<00:02, 29.28it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1136.90it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1154.61it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1268.18it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1421.64it/s]\n",
      " 16%|█▌        | 15/94 [00:00<00:02, 31.45it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1517.06it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1326.89it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1029.45it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1831.57it/s]\n",
      " 20%|██        | 19/94 [00:00<00:02, 32.71it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1618.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 1533.90it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1245.89it/s]\n",
      " 23%|██▎       | 22/94 [00:00<00:02, 31.40it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1120.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1098.78it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1226.40it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1644.83it/s]\n",
      " 28%|██▊       | 26/94 [00:00<00:02, 32.53it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1659.80it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1635.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1508.04it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1608.66it/s]\n",
      " 32%|███▏      | 30/94 [00:00<00:02, 30.15it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1356.94it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1261.57it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1774.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1482.09it/s]\n",
      " 36%|███▌      | 34/94 [00:01<00:01, 30.43it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 938.32it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 803.15it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1167.68it/s]\n",
      " 39%|███▉      | 37/94 [00:01<00:01, 29.45it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1295.44it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1395.31it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1527.01it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1274.28it/s]\n",
      " 44%|████▎     | 41/94 [00:01<00:01, 29.95it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1300.06it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1363.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1319.17it/s]\n",
      " 47%|████▋     | 44/94 [00:01<00:01, 28.26it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1465.34it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1485.50it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1304.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 920.91it/s]\n",
      " 51%|█████     | 48/94 [00:01<00:01, 28.37it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1488.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1414.29it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1414.61it/s]\n",
      " 54%|█████▍    | 51/94 [00:01<00:01, 27.68it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1000.43it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1240.55it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1443.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1245.59it/s]\n",
      " 59%|█████▊    | 55/94 [00:01<00:01, 30.02it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1497.43it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1513.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1072.99it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1426.63it/s]\n",
      " 63%|██████▎   | 59/94 [00:01<00:01, 30.49it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2092.71it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1234.47it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1812.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1318.55it/s]\n",
      " 67%|██████▋   | 63/94 [00:02<00:01, 28.99it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1514.87it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1341.21it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1251.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1483.48it/s]\n",
      " 71%|███████▏  | 67/94 [00:02<00:00, 30.21it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1440.23it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 966.36it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1128.71it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 818.13it/s]\n",
      " 76%|███████▌  | 71/94 [00:02<00:00, 30.54it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1541.32it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1414.13it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1444.65it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1522.85it/s]\n",
      " 80%|███████▉  | 75/94 [00:02<00:00, 31.45it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 979.21it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1579.38it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1377.44it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 666.22it/s]\n",
      " 84%|████████▍ | 79/94 [00:02<00:00, 32.41it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1467.05it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1212.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1312.98it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1203.96it/s]\n",
      " 88%|████████▊ | 83/94 [00:02<00:00, 31.05it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1105.66it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1020.35it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1493.96it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1039.29it/s]\n",
      " 93%|█████████▎| 87/94 [00:02<00:00, 31.18it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1065.90it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1981.01it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1215.39it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 2152.77it/s]\n",
      " 97%|█████████▋| 91/94 [00:02<00:00, 32.35it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1249.67it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1464.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1404.81it/s]\n",
      "100%|██████████| 94/94 [00:03<00:00, 31.22it/s]\n",
      "fixing cimaq: 100%|██████████| 94/94 [00:11<00:00,  8.41it/s]\n",
      "conforming events.tsv files: 94it [00:01, 61.53it/s]\n",
      "loading subjects: 100%|██████████| 94/94 [00:00<00:00, 433.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from fetch_cimaq import fetch_cimaq\n",
    "cimaq = fetch_cimaq(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# from os.path import basename as bname\n",
    "# from os.path import dirname as dname\n",
    "# from os.path import expanduser as xpu\n",
    "# from os import listdir as ls\n",
    "\n",
    "# # from typing import Union\n",
    "# from pandas import DataFrame as df\n",
    "# import sniffbytes as sniff\n",
    "# from bidsify_utils import bidsify_load_scans\n",
    "# from bidsify_utils import bidsify_names\n",
    "\n",
    "# def xtrct_cimaq(cimaq_dir: Union[str, os.PathLike]) -> np.ndarray:\n",
    "#     return df(tuple(scanzip(apath,\n",
    "#                             exclude = ['Practice', 'Pratique',\n",
    "#                                        'PRATIQUE', 'PRACTICE', 'READ',\n",
    "#                                        'Encoding-scan', 'Retrieval-'],\n",
    "#                             to_xtrct = ['.pdf', '.edat2'],\n",
    "#                             dst_path = pjoin(os.getcwd(), 'newdevs',\n",
    "#                                              'cimaq_uzeprimes'))\n",
    "#                     for apath in\n",
    "#                     tqdm(snif.filter_lst_inc(snif.clean_bytes(\n",
    "#                         xpu(pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "#                                   'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                         ).split()[1:], snif.loadimages(xpu(pjoin(\n",
    "#                         cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "#                         'task_files/zipped_eprime')))))),\n",
    "#              dtype = object)[0].values.flatten()\n",
    "\n",
    "# def fix_cimaq(cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "#     os.makedirs(pjoin(os.getcwd(), 'newtest', 'events'), exist_ok = True)\n",
    "#     os.makedirs(pjoin(os.getcwd(), 'newtest', 'behavioural'), exist_ok = True)\n",
    "#     os.makedirs(pjoin(os.getcwd(), 'newdevs', 'cimaq_uzeprimes'), exist_ok = True)\n",
    "#     for val in tqdm(xtrct_cimaq(cimaq_dir), 'fixing cimaq'):\n",
    "#         # creating events files\n",
    "#         (pd.concat([snif.bytes2df(val['bsheets'].values[1],\n",
    "#                                   has_header = None),\n",
    "#                     snif.bytes2df(val['bsheets'].values[0]).loc[:snif.bytes2df(\n",
    "#                         val['bsheets'].values[1],\n",
    "#                         has_header = None).shape[0] -1, :].drop(\n",
    "#                         columns = [0,1,2,3, 4, 6],\n",
    "#                         index = 0).rename(columns = {5: 'onset',\n",
    "#                                                      7: 'fix_onset',\n",
    "#                                                      8: 'fix_duration'})],\n",
    "#                    axis = 1).to_csv(pjoin(os.getcwd(), 'newtest', 'events', 'sub-_' + \\\n",
    "#                                           '_'.join(val['filename'].values[0].replace('-', '_').split(\n",
    "#             '_')[:2])+'_run-01_task-encoding_events.tsv'), sep = '\\t', index = None),\n",
    "#          # creating behavioural files\n",
    "#          snif.bytes2df(val['bsheets'].values[2], has_header = True).to_csv(pjoin(\n",
    "#              os.getcwd(), 'newtest', 'behavioural', 'sub-_' + '_'.join(\n",
    "#                  val['filename'].values[0].replace('-', '_').split('_')[:2]) + \\\n",
    "#                  '_run-01_task-encoding_behavioural.tsv'), sep = '\\t', index = None))\n",
    "\n",
    "# def cimaq2nilearn(cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "#     fix_cimaq(cimaq_dir)\n",
    "#     events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "#                            os.getcwd(), 'newtest', 'events')))\n",
    "#     os.makedirs(pjoin(os.getcwd(), 'newtest', 'nilearn_events'), exist_ok = True)\n",
    "#     for row in tqdm(events.iterrows(), desc = 'conforming events.tsv files'):\n",
    "#         sheet = pd.read_csv(row[1].fpaths, sep = '\\t')\n",
    "        \n",
    "#         sheet['duration'] = sheet['onset'] + \\\n",
    "#                                     sheet['fix_duration'] + \\\n",
    "#                                     sheet['onset'].sub(\n",
    "#                                     sheet['fix_onset']).round(0)\n",
    "#         sheet.rename(columns = {'category': 'trial_type'}).set_index(\n",
    "#             'trialnumber').to_csv(pjoin(os.getcwd(), 'newtest',\n",
    "#                                         'nilearn_events', row[1].filename + '.tsv'),\n",
    "#                                   sep = '\\t', index = None)\n",
    "\n",
    "# def fetch_cimaq(cimaq_dir: Union[str, os.PathLike]) -> dict:\n",
    "#     cimaq2nilearn(cimaq_dir)    \n",
    "#     scan_infos = bidsify_load_scans(cimaq_dir, snif.clean_bytes(xpu(pjoin(\n",
    "#             cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "#             'participants/sub_list_TaskQC.tsv'))).decode().split()[1:])\n",
    "\n",
    "#     scan_infos['dccid'] = sorted([(filename, filename.split('-')[1].split('_')[0])[1]\n",
    "#                                   for filename in scan_infos.filename])\n",
    "#     behav = snif.loadfiles(snif.loadimages(pjoin(\n",
    "#                            os.getcwd(), 'newtest', 'behavioural')))\n",
    "#     behav[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "#                                   for filename in behav.filename]\n",
    "#     events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "#                            os.getcwd(), 'newtest', 'nilearn_events')))\n",
    "#     events[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "#                                   for filename in events.filename]\n",
    "\n",
    "#     confounds = snif.loadfiles(snif.filter_lst_inc(\n",
    "#         snif.clean_bytes(xpu(pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "#                                    'participants/sub_list_TaskQC.tsv'))).decode().split()[1:],\n",
    "#         snif.loadimages( pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "#                                   'confounds', 'resample'))))\n",
    "#     confounds['bids_names'] = [bidsify_names(filename) for\n",
    "#                                filename in confounds.filename]\n",
    "    \n",
    "#     subs = df(((grp, scan_infos.groupby('dccid').get_group(grp))\n",
    "#                for grp in tqdm(scan_infos.groupby('dccid').groups,\n",
    "#                                desc = 'loading subjects')),\n",
    "#               columns = ['subject', 'scans']).set_index(\n",
    "#                             'subject').sort_index().reset_index(\n",
    "#                          drop = False)\n",
    "#     return dict(zip(['scans', 'behavior', 'confounds', 'events'],\n",
    "#                     [subs, behav, confounds, events]))\n",
    "\n",
    "# def main():    \n",
    "#     if __name__ == \"__main__\":\n",
    "#         fetch_cimaq(cimaq_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilearn First Level Matrix parameters explained:\n",
    "- https://nilearn.github.io/modules/generated/nilearn.glm.first_level.make_first_level_design_matrix.html#nilearn.glm.first_level.make_first_level_design_matrix\n",
    "\n",
    "\n",
    "#### The pd.DataFrame instance might have these keys:\n",
    "##### \"onset\" and \"duration\" are mandatory\n",
    "\n",
    "- ‘onset’: column to specify the start time of each events in\n",
    "   seconds. An error is raised if this key is missing.\n",
    "\n",
    "- ‘trial_type’: column to specify per-event experimental conditions\n",
    "   identifier. If missing each event are labelled ‘dummy’ and considered to form a unique condition.\n",
    "\n",
    "- ‘duration’: column to specify the duration of each events in\n",
    "   seconds. If missing the duration of each events is set to zero.\n",
    "\n",
    "- ‘modulation’: column to specify the amplitude of each\n",
    "   events. If missing the default is set to ones(n_events).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Encoding Type Matters\n",
    "\n",
    "- See how encoding type formats the same \"null-byte value\" differently \\\n",
    "  when printed to be human readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"\\x00\".encode('utf8'), \"\\x00\".encode('utf16'), \"\\x00\".encode('utf32'),\n",
    "        \"\".encode('utf8'), \"\".encode('utf16'), \"\".encode('utf32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retest = list((snif.bytes2df(val['bsheets'].values[1][0]),\n",
    "#                      snif.bytes2df(val['bsheets'].values[1][1]),\n",
    "#                      snif.bytes2df(val['bsheets'].values[1][2]))\n",
    "                                   \n",
    "# #                           snif.bytes2df(val['bsheets'].values[0]).drop(\n",
    "# #                               columns = [0, 1, 2, 3, 4, 6],\n",
    "# #                               index = 0).rename(columns = {5: 'stim_onset',\n",
    "# #                                                            7: 'fix_onset',\n",
    "# #                                                            8: 'duration'})],\n",
    "# #                          axis = 1).to_csv(pjoin(\n",
    "# #                    os.getcwd(), 'newtest', 'events',\n",
    "# #                    'sub-_'+ '_'.join(val['filename'].values[0].replace('-', '_').split(\n",
    "# #                        '_')[:2])+'_run-01_task-encoding_events.tsv'),\n",
    "# #                                          sep = '\\t', index = None),\n",
    "                      \n",
    "# #                       snif.bytes2df(val['bsheets'].values[2],\n",
    "# #                                     has_header = True).to_csv(pjoin(\n",
    "# #                    os.getcwd(), 'newtest', 'behavioural', 'sub-_' + '_'.join(\n",
    "# #                        val['filename'].values[0].replace('-', '_').split('_')[:2]) + \\\n",
    "# #                    '_run-01_task-encoding_behavioural.tsv'), sep = '\\t',\n",
    "# #                                                             index = None))\n",
    "#                      for val in\n",
    "#                          tqdm(enumerate(df(tuple(scanzip(apath,\n",
    "#                                           exclude = ['Practice', 'Pratique',\n",
    "#                                                     'PRATIQUE', 'PRACTICE', 'READ',\n",
    "#                                                      'Encoding-scan', 'Retrieval-'],\n",
    "#                                           to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                           dst_path = pjoin(os.getcwd(), 'newdevs',\n",
    "#                                                            'cimaq_uzeprimes'))\n",
    "#                                  for apath in\n",
    "#                                  tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:], snif.loadimages(xpu(pjoin(\n",
    "#                                          drv_dir, 'task_files/zipped_eprime')))))))\\\n",
    "#                         [0].values.flatten()), 'fast'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(df(tuple(scanzip(apath,\n",
    "#                                               exclude = ['Practice', 'Pratique',\n",
    "#                                                         'PRATIQUE', 'PRACTICE', 'READ',\n",
    "#                                                          'Encoding-scan', 'Retrieval-'],\n",
    "#                                               to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                               dst_path = pjoin(os.getcwd(), 'newdevs',\n",
    "#                                                                'cimaq_uzeprimes'))\n",
    "#                                      for apath in\n",
    "#                                      tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                          drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                              ).split()[1:], snif.loadimages(xpu(pjoin(\n",
    "#                                              drv_dir, 'task_files/zipped_eprime')))))))\\\n",
    "#                             [0].values.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "help(namedtuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "\n",
    "from typing import Union\n",
    "from pandas import DataFrame as df\n",
    "import sniffbytes as sniff\n",
    "from bidsify_utils import bidsify_load_scans\n",
    "from bidsify_utils import bidsify_names\n",
    "\n",
    "def xtrct_cimaq(cimaq_dir: Union[str, os.PathLike]) -> np.ndarray:\n",
    "    return df(tuple(scanzip(apath,\n",
    "                            exclude = ['Practice', 'Pratique',\n",
    "                                       'PRATIQUE', 'PRACTICE', 'READ',\n",
    "                                       'Encoding-scan', 'Retrieval-'],\n",
    "                            to_xtrct = ['.pdf', '.edat2'],\n",
    "                            dst_path = pjoin(os.getcwd(), 'newdevs',\n",
    "                                             'cimaq_uzeprimes'))\n",
    "                    for apath in\n",
    "                    tqdm(snif.filter_lst_inc(snif.clean_bytes(\n",
    "                        xpu(pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                                  'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                        ).split()[1:], snif.loadimages(xpu(pjoin(\n",
    "                        cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                        'task_files/zipped_eprime')))))))[0].values.flatten()\n",
    "\n",
    "def fix_cimaq(cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newtest', 'events'), exist_ok = True)\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newtest', 'behavioural'), exist_ok = True)\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newdevs', 'cimaq_uzeprimes'), exist_ok = True)\n",
    "    for val in tqdm(xtrct_cimaq(cimaq_dir), 'fixing cimaq'):\n",
    "        # creating events files\n",
    "        (pd.concat([snif.bytes2df(val['bsheets'].values[1],\n",
    "                                  has_header = None),\n",
    "                    snif.bytes2df(val['bsheets'].values[0]).loc[:snif.bytes2df(\n",
    "                        val['bsheets'].values[1],\n",
    "                        has_header = None).shape[0] -1, :].drop(\n",
    "                        columns = [0,1,2,3, 4, 6],\n",
    "                        index = 0).rename(columns = {5: 'onset',\n",
    "                                                     7: 'fix_onset',\n",
    "                                                     8: 'fix_duration'})],\n",
    "                   axis = 1).to_csv(pjoin(os.getcwd(), 'newtest', 'events', 'sub-_' + \\\n",
    "                                          '_'.join(val['filename'].values[0].replace('-', '_').split(\n",
    "            '_')[:2])+'_run-01_task-encoding_events.tsv'), sep = '\\t', index = None),\n",
    "         # creating behavioural files\n",
    "         snif.bytes2df(val['bsheets'].values[2], has_header = True).to_csv(pjoin(\n",
    "             os.getcwd(), 'newtest', 'behavioural', 'sub-_' + '_'.join(\n",
    "                 val['filename'].values[0].replace('-', '_').split('_')[:2]) + \\\n",
    "                 '_run-01_task-encoding_behavioural.tsv'), sep = '\\t', index = None))\n",
    "\n",
    "def cimaq2nilearn(cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "    fix_cimaq(cimaq_dir)\n",
    "    events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "                           os.getcwd(), 'newtest', 'events')))\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newtest', 'nilearn_events'), exist_ok = True)\n",
    "    for row in tqdm(events.iterrows(), desc = 'conforming events.tsv files'):\n",
    "        sheet = pd.read_csv(row[1].fpaths, sep = '\\t')\n",
    "        \n",
    "        sheet['duration'] = sheet['onset'] + \\\n",
    "                                    sheet['fix_duration'] + \\\n",
    "                                    sheet['onset'].sub(\n",
    "                                    sheet['fix_onset']).round(0)\n",
    "        sheet.rename(columns = {'category': 'trial_type'}).set_index(\n",
    "            'trialnumber').to_csv(pjoin(os.getcwd(), 'newtest',\n",
    "                                        'nilearn_events', row[1].filename + '.tsv'),\n",
    "                                  sep = '\\t', index = None)\n",
    "\n",
    "def fetch_cimaq(cimaq_dir: Union[str, os.PathLike]) -> dict:\n",
    "    cimaq2nilearn(cimaq_dir)    \n",
    "    scan_infos = bidsify_load_scans(cimaq_dir, snif.clean_bytes(xpu(pjoin(\n",
    "            cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "            'participants/sub_list_TaskQC.tsv'))).decode().split()[1:])\n",
    "\n",
    "    scan_infos['dccid'] = sorted([(filename, filename.split('-')[1].split('_')[0])[1]\n",
    "                                  for filename in scan_infos.filename])\n",
    "    behav = snif.loadfiles(snif.loadimages(pjoin(\n",
    "                           os.getcwd(), 'newtest', 'behavioural')))\n",
    "    behav[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "                                  for filename in behav.filename]\n",
    "    events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "                           os.getcwd(), 'newtest', 'nilearn_events')))\n",
    "    events[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "                                  for filename in events.filename]\n",
    "\n",
    "    confounds = snif.loadfiles(snif.filter_lst_inc(\n",
    "        snif.clean_bytes(xpu(pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                                   'participants/sub_list_TaskQC.tsv'))).decode().split()[1:],\n",
    "        snif.loadimages( pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                                  'confounds', 'resample'))))\n",
    "    confounds['bids_names'] = [bidsify_names(filename) for\n",
    "                               filename in confounds.filename]\n",
    "    \n",
    "    subs = df(((grp, scan_infos.groupby('dccid').get_group(grp))\n",
    "               for grp in tqdm(scan_infos.groupby('dccid').groups,\n",
    "                               desc = 'loading subjects')),\n",
    "              columns = ['subject', 'scans']).set_index(\n",
    "                            'subject').sort_index().reset_index(\n",
    "                         drop = False)\n",
    "    return dict(zip(['scans', 'behavior', 'confounds', 'events'],\n",
    "                    [subs, behav, confounds, events]))\n",
    "\n",
    "class CIMAQ:\n",
    "    def __init__(self, cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "        self.cimaq_dir = cimaq_dir\n",
    "    @ classmethod\n",
    "    def fetch(self, cimaq_dir: Union[str, os.PathLike]):\n",
    "        return fetch_cimaq(cimaq_dir)\n",
    "\n",
    "cimaq = CIMAQ.fetch(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq.fetch(cimaq_dir)\n",
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_infos = bidsify_load_scans(cimaq_dir, snif.clean_bytes(xpu(pjoin(\n",
    "#             drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:])\n",
    "\n",
    "# scan_infos['dccid'] = sorted([(filename, filename.split('-')[1].split('_')[0])[1]\n",
    "#                             for filename in scan_infos.filename])\n",
    "# behav = snif.loadfiles(snif.loadimages(pjoin(\n",
    "#                        os.getcwd(), 'newtest', 'behavioural')))\n",
    "# behav[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "#                               for filename in behav.filename]\n",
    "# events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "#                        os.getcwd(), 'newtest', 'events')))\n",
    "# events[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "#                               for filename in events.filename]\n",
    "\n",
    "# confounds = snif.loadfiles(snif.loadimages(pjoin(drv_dir, 'confounds', 'resample')))\n",
    "# confounds['bids_names'] = [bidsify_names(filename) for\n",
    "#                            filename in confounds.filename]\n",
    "\n",
    "# # confounds[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "# #                                  for filename in confounds.filename]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subs = df(((grp, scan_infos.groupby('dccid').get_group(grp))\n",
    "           for grp in scan_infos.groupby('dccid').groups),\n",
    "          columns = ['subject', 'scans']).set_index(\n",
    "                        'subject').sort_index().reset_index(\n",
    "                     drop = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(events, behav, subs)\n",
    "# subs = df(((grp, scan_infos.groupby('filename').get_group(grp))\n",
    "#            for grp in scan_infos.groupby('filename').groups),\n",
    "#           columns = ['subject', 'scans']).set_index(\n",
    "#                         'subject').sort_index().reset_index(\n",
    "#                      drop = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs.iloc[44].scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byfilename = df((df(((grp, row[1].scans.groupby('filename').get_group(grp))\n",
    "                 for grp in row[1].scans.groupby('filename').groups),\n",
    "                columns = ['modality', 'scans']) for row in subs.iterrows())).values.flat\n",
    "# json.dumps([itm.values.tolist() for itm in byfilename.values.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_infos.modality.unique()\n",
    "\n",
    "list(next((itm))['scans'].values.tolist() for itm in byfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_infos\n",
    "\n",
    "# By subject\n",
    "# subs = df(((grp, scan_infos.groupby('dccid').get_group(grp))\n",
    "#            for grp in scan_infos.groupby('dccid').groups),\n",
    "#           columns = ['subject', 'scans']).set_index(\n",
    "#                         'subject').sort_index().reset_index(\n",
    "#                      drop = False)\n",
    "\n",
    "files_per_sub = pd.Series(row[1].scans.shape[0] for row\n",
    "                          in subs.iterrows()).describe()\n",
    "files_per_sub\n",
    "# By scan type (parent column)\n",
    "# mods = df(((grp, scan_infos.groupby('parent').get_group(grp))\n",
    "#            for grp in scan_infos.groupby('parent').groups),\n",
    "#           columns = ['scan_type', 'scans']).set_index('scan_type')\n",
    "\n",
    "\n",
    "# display(mods.loc['dwi']['scans'],\n",
    "#         mods.loc['anat']['scans'])\n",
    "# dwi_scans = [mods['dwi']] + [dwi_only.groupby('ext').get_group(grp)\n",
    "#                              for grp in dwi_only.groupby('ext').groups]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles\n",
    "from io import StringIO\n",
    "\n",
    "def json_bytes(apath: Union[str, os.PathLike]):\n",
    "    with open(apath, 'rb', buffering = 0) as jfile:\n",
    "        jbytes3 = json.load(jfile)\n",
    "    jfile.close()\n",
    "    return jbytes3\n",
    "\n",
    "# jsons = list((row[1].filename, json_bytes(row[1].fpaths))\n",
    "#              for row in tqdm(jsonfiles.iterrows()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(pd.read_csv(apath, sep = '\\t') for apath in events.fpaths)\n",
    "# display(behav, events, scan_infos, jsonfiles, nifti_only, dwi_only)\n",
    "\n",
    "# nifti_only['jsoninfo'] = jsonfiles.fpaths.tolist()\n",
    "\n",
    "# mods = dict((grp, nifti_only.groupby('parent').get_group(grp))\n",
    "#         for grp in nifti_only.groupby('parent').groups)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "# full_infos = pd.concat([df.from_dict({**dict(nib.load(row[1].fpaths).header),\n",
    "#                                       **json_bytes(row[1].jsoninfo)},\n",
    "#                                      orient = 'index').T\n",
    "#                         for row in tqdm(nifti_only.iterrows())], join = 'inner')\n",
    "# full_infos['filename'] = nifti_only['filename'].tolist()\n",
    "# full_infos = full_infos.set_index('filename')\n",
    "# full_infos.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dwi_scans[0], dwi_scans[1], dwi_scans[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.from_dict(mods['dwi'], orient ='index').T#, dwi_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_infos.to_csv(pjoin(os.getcwd(), 'newdevs',\n",
    "                        'inner_join_nifti_header_json.tsv'),\n",
    "                  sep = '\\t', index = None)\n",
    "\n",
    "test = [row[1].values for row in full_infos.T.iterrows()]\n",
    "\n",
    "test\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ungrouped\n",
    "\n",
    "from nilearn import image\n",
    "from dict2csv import dict2csv\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [itm[1].unique()[0] for itm in\n",
    " df((itm[1].columns for itm\n",
    "     in jsons)).dropna().iteritems()\n",
    " if len(itm[1].unique()) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(unidecode(jsons[0][1][1].decode())).encode().split(\n",
    "    snif.get_lineterminator(json.dumps(str(\n",
    "        jsons[0][1][1].decode())).encode()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(pjoin(os.getcwd(), 'cimaq_json_headers'), exist_ok = True)\n",
    "\n",
    "cimaq_json_parameters = list((df.from_dict(dict(row[1].parameters),\n",
    "                                          orient = 'index').to_csv(\n",
    "                            pjoin(os.getcwd(), 'cimaq_nifti_headers', row[1].filename + \\\n",
    "                                  '_nifti_headers.tsv'), sep = '\\t', index = None))\n",
    "                             for row in nifti_only.iterrows())\n",
    "cimaq_scan_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(pjoin(os.getcwd(), 'cimaq_nifti_headers'), exist_ok = True)\n",
    "\n",
    "cimaq_scan_parameters = list((df.from_dict(dict(row[1].parameters),\n",
    "                                          orient = 'index').to_csv(\n",
    "                            pjoin(os.getcwd(), 'cimaq_nifti_headers', row[1].filename + \\\n",
    "                                  '_nifti_headers.tsv'), sep = '\\t'))\n",
    "                             for row in nifti_only.iterrows())\n",
    "cimaq_scan_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def dict2json(adict, dst_path: Union[str, os.PathLike]):\n",
    "\n",
    "#     adict = {\n",
    "#       'bill': 'tech',\n",
    "#       'federer': 'tennis',\n",
    "#       'ronaldo': 'football',\n",
    "#       'woods': 'golf',\n",
    "#       'ali': 'boxing'\n",
    "#     }\n",
    "\n",
    "    with open(dst_path, 'w') as json_file:\n",
    "        json.dump(adict, json_file)\n",
    "        json_file.close()\n",
    "\n",
    "dict2json(cimaq_scan_parameters, pjoin(os.getcwd(), 'cimaq_scan_parameters.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjoin(os.getcwd(), 'cimaq_scan_parameters.csv')\n",
    "cimaq_scans_json = json.load(pjoin(os.getcwd(), 'cimaq_scan_parameters.json'))\n",
    "cimaq_scans_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict2csv(cimaq_scan_parameters, pjoin(os.getcwd(), 'cimaq_scan_parameters.txt'))\n",
    "\n",
    "list(cimaq_scan_parameters.keys())\n",
    "# with open(pjoin(os.getcwd(), 'cimaq_scan_parameters.json'), 'w') as jfile:\n",
    "#     json.dump(cimaq_scan_parameters, pjoin(os.getcwd(), 'cimaq_scan_parameters.json'))jfile.write(cimaq_scan_parameters)\n",
    "# #     jfile.close()\n",
    "\n",
    "# cimaq_scan_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.concat([df.from_dict(dict(row[1].parameters), orient = 'index')\n",
    "                   for row in nifti_only.iterrows()], axis = 1).T\n",
    "# params['filename'] = nifti_only.filename\n",
    "# infos = df(df(((pd.Series(list(itm[1].values for itm in nifti_only.iteritems()) + \\\n",
    "#         list(itm[1].values for itm in params.iteritems()))))).T.values)\n",
    "display(infos)\n",
    "# nifti_only[params.columns] = [params.iteritems()]\n",
    "# nifti_only\n",
    "\n",
    "# pd.merge(params, nifti_only, on = 'filename', how = 'outer', axis =1)\n",
    "# pd.concat((nifti_only, params), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting anatomical images\n",
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "nif = nifti_only.iloc[:5]\n",
    "display([(row[1].filename,\n",
    "          plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "         for row in tqdm(nif.iterrows(), 'Plotting anatomical images')\n",
    " if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])\n",
    "# # test = df(zip(nifti_only.iterrows(), jsonfiles.iterrows()))\n",
    "\n",
    "# # test = [behav, events, scan_infos, jsonfiles, nifti_only, dwi_only]\n",
    "# # test = [list(((itm.groupby('dccid').get_group(grp).groupby('modality').get_group(mod)\n",
    "# #               for grp in itm.groupby('dccid').groups) for mod in\n",
    "# #              itm.groupby('dccid').get_group(grp).groupby('modality').groups))\n",
    "# #         for itm in test]\n",
    "\n",
    "\n",
    "# # jsonfiles, scan_infos= (scan_infos.drop([[row[0]\n",
    "# #                                 for row in scan_infos.iterrows()\n",
    "# #                                  if row[1].ext != '.json']][0], axis=0),\n",
    "# #             scan_infos.drop([[row[0]\n",
    "# #                                 for row in scan_infos.iterrows()\n",
    "# #                                  if row[1].ext == '.json']][0], axis=0))\n",
    "# # display(behav, events, scan_infos, jsonfiles, nifti_only, dwi_only, test)\n",
    "# image.load_img(nifti_only.iloc[0]['scans'])\n",
    "# # display(test.iloc[0][0].scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epis = nifti_only.iloc[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ TODO: Group by modality ################3333\n",
    "\n",
    "# subs_json = [jsonfiles.groupby('dccid').get_group(grp)\n",
    "#              for grp in jsonfiles.groupby('dccid').groups]\n",
    "# modgrps = [[sjson.groupby('modality').get_group(grp) for grp in sjson.groupby('modality').groups]\n",
    "#            for sjson in subs_json]\n",
    "# modgrps[4][0]\n",
    "# # loaded_json = [pd.read_json(row[1].fpaths) for row in itm.iterrows()]\n",
    "# #                for itm in subs_json]\n",
    "\n",
    "# # loaded_json2 = [row[1].unique() for row in loaded_json.iterrows()]\n",
    "# # loaded_json2 = [Counter(row[1]).most_common(1) for row in loaded_json.iterrows()]\n",
    "# # loaded_json2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "#     \"\"\"\n",
    "#     Context manager to set the timestamp of the path to plus or\n",
    "#     minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "#     if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "#     \"\"\"\n",
    "#     stats = os.stat(path)\n",
    "#     if set_new:\n",
    "#         new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "#     else:\n",
    "#         new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "#     try:\n",
    "#         yield\n",
    "#     finally:\n",
    "#         os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# # Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
