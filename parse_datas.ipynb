{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import byte\n",
    "import chardet\n",
    "import csv\n",
    "import json\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex as re\n",
    "import string\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from chardet import UniversalDetector as udet\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from string import printable\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "from cimaq_utils import get_cimaq_dir_paths\n",
    "from sniffbytes import flatten\n",
    "from sniffbytes import loadfiles\n",
    "from sniffbytes import loadimages\n",
    "from cimaq_utils import repair_enc_task\n",
    "from cimaq_utils import fetch_cimaq\n",
    "\n",
    "from json_read import json_read\n",
    "\n",
    "import sniffbytes as snif\n",
    "\n",
    "import scanzip as scanzip\n",
    "from scanzip import scanzip as sz1\n",
    "from scanzip2 import scanzip2 as sz2\n",
    "from scanzip3 import scanzip3 as sz3\n",
    "from scanzip4 import scanzip4 as sz4\n",
    "\n",
    "from scanzip import getnametuple\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "from multiple_replace import multiple_replace\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'122922'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_dir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/CIMAQ_fmri_memory/data'\n",
    "zeprimes = pjoin(cimaq_dir, 'task_files/zipped_eprime')\n",
    "qc_path = xpu(pjoin(cimaq_dir, 'participants/sub_list_TaskQC.tsv'))\n",
    "# snif.get_bytes(qc_path)\n",
    "# type(qc_path) == str and os.path.isfile(qc_path)\n",
    "qc_sheet = snif.clean_bytes(xpu(pjoin(\n",
    "    cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:]\n",
    "qc_sheet[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5005.99it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6038.69it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5920.57it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5267.57it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 2403.96it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 2434.30it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 2595.49it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5579.65it/s]\n",
      "\n",
      "  9%|▊         | 8/94 [00:00<00:01, 76.47it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5487.87it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 4106.21it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5443.61it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5971.15it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5964.17it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5513.64it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 6351.51it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 2845.80it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6245.51it/s]\n",
      "\n",
      " 18%|█▊        | 17/94 [00:00<00:00, 78.27it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5541.74it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6338.20it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 6488.17it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6437.92it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6446.58it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6029.55it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5360.13it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5491.98it/s]\n",
      "\n",
      " 27%|██▋       | 25/94 [00:00<00:00, 77.46it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6212.63it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5563.66it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6359.14it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 6490.50it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 2571.81it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6086.26it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5358.67it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5602.68it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4704.77it/s]\n",
      "\n",
      " 36%|███▌      | 34/94 [00:00<00:00, 78.22it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 7073.03it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5553.27it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4845.17it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5200.16it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5485.82it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5681.14it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5628.05it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6331.02it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5319.83it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6291.85it/s]\n",
      "\n",
      " 47%|████▋     | 44/94 [00:00<00:00, 82.13it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 4974.61it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6425.59it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5363.56it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5456.26it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5322.72it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5473.81it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4853.11it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5436.17it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6275.38it/s]\n",
      "\n",
      " 56%|█████▋    | 53/94 [00:00<00:00, 83.03it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 5031.15it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5415.00it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5441.09it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 2643.95it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5457.78it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5042.24it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5314.29it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 3358.47it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5260.14it/s]\n",
      "\n",
      " 66%|██████▌   | 62/94 [00:00<00:00, 83.69it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5476.61it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5498.15it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5496.22it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5504.34it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5342.09it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5855.63it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5951.78it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6510.37it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5513.64it/s]\n",
      "\n",
      " 76%|███████▌  | 71/94 [00:00<00:00, 85.22it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4985.31it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4986.54it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5010.12it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5197.40it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5616.54it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4776.43it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5505.24it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5460.32it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 6187.12it/s]\n",
      "\n",
      " 85%|████████▌ | 80/94 [00:00<00:00, 85.04it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5194.64it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 5606.00it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5368.71it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6392.54it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5167.22it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5121.25it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5697.81it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5534.43it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5295.10it/s]\n",
      "\n",
      " 95%|█████████▍| 89/94 [00:01<00:00, 84.04it/s]\u001b[A\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4374.19it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5315.97it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5118.91it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6192.81it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4450.78it/s]\n",
      "100%|██████████| 94/94 [00:01<00:00, 83.08it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n",
      "\n",
      "creating Pandas DataFrames: 0it [00:00, ?it/s]\u001b[A\n",
      "creating Pandas DataFrames: 4it [00:00, 16.13it/s]\u001b[A\n",
      "creating Pandas DataFrames: 5it [00:00,  8.79it/s]\u001b[A\n",
      "creating Pandas DataFrames: 9it [00:00, 10.37it/s]\u001b[A\n",
      "creating Pandas DataFrames: 10it [00:00,  7.36it/s]\u001b[A\n",
      "creating Pandas DataFrames: 14it [00:01,  8.89it/s]\u001b[A\n",
      "creating Pandas DataFrames: 16it [00:01,  8.20it/s]\u001b[A\n",
      "creating Pandas DataFrames: 19it [00:01,  9.58it/s]\u001b[A\n",
      "creating Pandas DataFrames: 21it [00:01,  9.04it/s]\u001b[A\n",
      "creating Pandas DataFrames: 24it [00:02, 10.47it/s]\u001b[A\n",
      "creating Pandas DataFrames: 26it [00:02,  8.94it/s]\u001b[A\n",
      "creating Pandas DataFrames: 29it [00:02, 10.75it/s]\u001b[A\n",
      "creating Pandas DataFrames: 31it [00:02,  9.42it/s]\u001b[A\n",
      "creating Pandas DataFrames: 34it [00:02, 10.83it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating Pandas DataFrames: 36it [00:03,  9.76it/s]\u001b[A\n",
      "creating Pandas DataFrames: 39it [00:03, 11.12it/s]\u001b[A\n",
      "creating Pandas DataFrames: 41it [00:03,  9.84it/s]\u001b[A\n",
      "creating Pandas DataFrames: 44it [00:03, 11.32it/s]\u001b[A\n",
      "creating Pandas DataFrames: 46it [00:04,  9.57it/s]\u001b[A\n",
      "creating Pandas DataFrames: 49it [00:04, 11.45it/s]\u001b[A\n",
      "creating Pandas DataFrames: 51it [00:04, 10.10it/s]\u001b[A\n",
      "creating Pandas DataFrames: 54it [00:04, 11.56it/s]\u001b[A\n",
      "creating Pandas DataFrames: 56it [00:04,  9.61it/s]\u001b[A\n",
      "creating Pandas DataFrames: 59it [00:05, 11.46it/s]\u001b[A\n",
      "creating Pandas DataFrames: 61it [00:05,  9.53it/s]\u001b[A\n",
      "creating Pandas DataFrames: 64it [00:05, 11.39it/s]\u001b[A\n",
      "creating Pandas DataFrames: 66it [00:05,  9.43it/s]\u001b[A\n",
      "creating Pandas DataFrames: 69it [00:05, 11.31it/s]\u001b[A\n",
      "creating Pandas DataFrames: 71it [00:06,  9.31it/s]\u001b[A\n",
      "creating Pandas DataFrames: 74it [00:06, 10.83it/s]\u001b[A\n",
      "creating Pandas DataFrames: 76it [00:06,  9.80it/s]\u001b[A\n",
      "creating Pandas DataFrames: 79it [00:06, 11.71it/s]\u001b[A\n",
      "creating Pandas DataFrames: 81it [00:07,  9.67it/s]\u001b[A\n",
      "creating Pandas DataFrames: 84it [00:07, 11.14it/s]\u001b[A\n",
      "creating Pandas DataFrames: 86it [00:07,  9.94it/s]\u001b[A\n",
      "creating Pandas DataFrames: 89it [00:07, 11.80it/s]\u001b[A\n",
      "creating Pandas DataFrames: 91it [00:08,  9.48it/s]\u001b[A\n",
      "creating Pandas DataFrames: 94it [00:08, 11.00it/s]\u001b[A\n",
      "creating Pandas DataFrames: 98it [00:08, 11.95it/s]\u001b[A\n",
      "creating Pandas DataFrames: 102it [00:08, 13.60it/s]\u001b[A\n",
      "creating Pandas DataFrames: 104it [00:08, 11.07it/s]\u001b[A\n",
      "creating Pandas DataFrames: 107it [00:09, 12.41it/s]\u001b[A\n",
      "creating Pandas DataFrames: 111it [00:09, 14.02it/s]\u001b[A\n",
      "creating Pandas DataFrames: 113it [00:09, 11.17it/s]\u001b[A\n",
      "creating Pandas DataFrames: 116it [00:09, 12.54it/s]\u001b[A\n",
      "creating Pandas DataFrames: 118it [00:09, 10.64it/s]\u001b[A\n",
      "creating Pandas DataFrames: 121it [00:10, 11.88it/s]\u001b[A\n",
      "creating Pandas DataFrames: 123it [00:10, 10.40it/s]\u001b[A\n",
      "creating Pandas DataFrames: 126it [00:10, 10.78it/s]\u001b[A\n",
      "creating Pandas DataFrames: 128it [00:10,  9.26it/s]\u001b[A\n",
      "creating Pandas DataFrames: 131it [00:11, 10.73it/s]\u001b[A\n",
      "creating Pandas DataFrames: 133it [00:11,  9.71it/s]\u001b[A\n",
      "creating Pandas DataFrames: 136it [00:11, 11.19it/s]\u001b[A\n",
      "creating Pandas DataFrames: 138it [00:11,  9.53it/s]\u001b[A\n",
      "creating Pandas DataFrames: 141it [00:11, 11.43it/s]\u001b[A\n",
      "creating Pandas DataFrames: 143it [00:12, 10.08it/s]\u001b[A\n",
      "creating Pandas DataFrames: 146it [00:12, 11.52it/s]\u001b[A\n",
      "creating Pandas DataFrames: 148it [00:12,  9.47it/s]\u001b[A\n",
      "creating Pandas DataFrames: 151it [00:12, 11.36it/s]\u001b[A\n",
      "creating Pandas DataFrames: 153it [00:13,  9.44it/s]\u001b[A\n",
      "creating Pandas DataFrames: 156it [00:13, 11.49it/s]\u001b[A\n",
      "creating Pandas DataFrames: 158it [00:13, 12.02it/s]\u001b[A\n",
      "creating Pandas DataFrames: 161it [00:13, 11.67it/s]\u001b[A\n",
      "creating Pandas DataFrames: 165it [00:13, 13.78it/s]\u001b[A\n",
      "creating Pandas DataFrames: 167it [00:14, 10.34it/s]\u001b[A\n",
      "creating Pandas DataFrames: 170it [00:14, 12.18it/s]\u001b[A\n",
      "creating Pandas DataFrames: 174it [00:14, 13.77it/s]\u001b[A\n",
      "creating Pandas DataFrames: 176it [00:14, 10.44it/s]\u001b[A\n",
      "creating Pandas DataFrames: 179it [00:14, 12.31it/s]\u001b[A\n",
      "creating Pandas DataFrames: 183it [00:15, 13.89it/s]\u001b[A\n",
      "creating Pandas DataFrames: 185it [00:15, 11.19it/s]\u001b[A\n",
      "creating Pandas DataFrames: 188it [00:15, 12.53it/s]\u001b[A\n",
      "creating Pandas DataFrames: 190it [00:15,  9.89it/s]\u001b[A\n",
      "creating Pandas DataFrames: 193it [00:16, 11.78it/s]\u001b[A\n",
      "creating Pandas DataFrames: 195it [00:16, 10.11it/s]\u001b[A\n",
      "creating Pandas DataFrames: 198it [00:16, 11.56it/s]\u001b[A\n",
      "creating Pandas DataFrames: 200it [00:16, 10.09it/s]\u001b[A\n",
      "creating Pandas DataFrames: 203it [00:16, 11.53it/s]\u001b[A\n",
      "creating Pandas DataFrames: 205it [00:17,  9.39it/s]\u001b[A\n",
      "creating Pandas DataFrames: 208it [00:17, 11.30it/s]\u001b[A\n",
      "creating Pandas DataFrames: 210it [00:17,  9.81it/s]\u001b[A\n",
      "creating Pandas DataFrames: 213it [00:17, 11.29it/s]\u001b[A\n",
      "creating Pandas DataFrames: 215it [00:18,  9.37it/s]\u001b[A\n",
      "creating Pandas DataFrames: 218it [00:18, 11.27it/s]\u001b[A\n",
      "creating Pandas DataFrames: 222it [00:18, 12.94it/s]\u001b[A\n",
      "creating Pandas DataFrames: 226it [00:18, 13.37it/s]\u001b[A\n",
      "creating Pandas DataFrames: 230it [00:18, 14.82it/s]\u001b[A\n",
      "creating Pandas DataFrames: 234it [00:19, 16.07it/s]\u001b[A\n",
      "creating Pandas DataFrames: 236it [00:19, 12.34it/s]\u001b[A\n",
      "creating Pandas DataFrames: 239it [00:19, 14.14it/s]\u001b[A\n",
      "creating Pandas DataFrames: 241it [00:19, 11.50it/s]\u001b[A\n",
      "creating Pandas DataFrames: 245it [00:19, 13.62it/s]\u001b[A\n",
      "creating Pandas DataFrames: 247it [00:20, 10.64it/s]\u001b[A\n",
      "creating Pandas DataFrames: 250it [00:20, 12.46it/s]\u001b[A\n",
      "creating Pandas DataFrames: 252it [00:20,  9.85it/s]\u001b[A\n",
      "creating Pandas DataFrames: 255it [00:20, 11.71it/s]\u001b[A\n",
      "creating Pandas DataFrames: 257it [00:21, 10.39it/s]\u001b[A\n",
      "creating Pandas DataFrames: 261it [00:21, 12.50it/s]\u001b[A\n",
      "creating Pandas DataFrames: 263it [00:21, 10.03it/s]\u001b[A\n",
      "creating Pandas DataFrames: 266it [00:21, 11.93it/s]\u001b[A\n",
      "creating Pandas DataFrames: 268it [00:21,  9.60it/s]\u001b[A\n",
      "creating Pandas DataFrames: 271it [00:22, 11.49it/s]\u001b[A\n",
      "creating Pandas DataFrames: 273it [00:22,  9.38it/s]\u001b[A\n",
      "creating Pandas DataFrames: 276it [00:22, 11.30it/s]\u001b[A\n",
      "creating Pandas DataFrames: 278it [00:22,  9.60it/s]\u001b[A\n",
      "creating Pandas DataFrames: 281it [00:22, 11.51it/s]\u001b[A\n",
      "creating Pandas DataFrames: 283it [00:23,  9.43it/s]\u001b[A\n",
      "creating Pandas DataFrames: 286it [00:23, 11.34it/s]\u001b[A\n",
      "creating Pandas DataFrames: 288it [00:23, 10.29it/s]\u001b[A\n",
      "creating Pandas DataFrames: 291it [00:23, 11.75it/s]\u001b[A\n",
      "creating Pandas DataFrames: 293it [00:24, 10.03it/s]\u001b[A\n",
      "creating Pandas DataFrames: 296it [00:24, 11.41it/s]\u001b[A\n",
      "creating Pandas DataFrames: 298it [00:24,  9.94it/s]\u001b[A\n",
      "creating Pandas DataFrames: 301it [00:24, 12.16it/s]\u001b[A\n",
      "creating Pandas DataFrames: 303it [00:24,  8.97it/s]\u001b[A\n",
      "creating Pandas DataFrames: 307it [00:25, 10.73it/s]\u001b[A\n",
      "creating Pandas DataFrames: 309it [00:25,  9.49it/s]\u001b[A\n",
      "creating Pandas DataFrames: 312it [00:25, 10.87it/s]\u001b[A\n",
      "creating Pandas DataFrames: 314it [00:25,  9.00it/s]\u001b[A\n",
      "creating Pandas DataFrames: 317it [00:26, 10.89it/s]\u001b[A\n",
      "creating Pandas DataFrames: 319it [00:26,  9.61it/s]\u001b[A\n",
      "creating Pandas DataFrames: 322it [00:26, 12.14it/s]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# newsh = [({**row[1].drop('bsheets').to_dict(),\n",
    "#            **{'newsheet': snif.bytes2df(row[1].bsheets)}},\n",
    "#           snif.bytes2df(row[1].bsheets).to_csv(pjoin(os.getcwd(), 'cimaq_uzeprimes', row[1].filename),\n",
    "#                                                sep = '\\t', index = None))[0]\n",
    "#          for row in tqdm(pd.concat(\n",
    "#                  val for val in\n",
    "#                  df(tuple(scanzip.scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   include = ['Onset', 'Output'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]\n",
    "\n",
    "# newsh = [((row[1].filename, snif.bytes2df(row[1].bsheets)),\n",
    "#           (pd.concat([row[1].drop('bsheets'), df.from_dict(snif.sniff_bytes(row[1].bsheets), orient = 'index')]).to_csv(\n",
    "#               pjoin(os.getcwd(), 'cimaq_uzeprimes',\n",
    "#                     'before_' + os.path.splitext(row[1].filename)[0] + '.tsv'),\n",
    "#                                                sep = '\\t', index = None)),\n",
    "#           snif.bytes2df(row[1].bsheets).to_csv(pjoin(os.getcwd(), 'cimaq_uzeprimes',\n",
    "#                                                     os.path.splitext(row[1].filename)[0] + '.tsv'),\n",
    "#                                                sep = '\\t', index = None),\n",
    "#           df.from_dict(snif.sniff_bytes(snif.get_bytes(pjoin(os.getcwd(), 'cimaq_uzeprimes',\n",
    "#                                                 row[1].filename))), orient = 'index').to_csv(\n",
    "#               pjoin(os.getcwd(), 'cimaq_uzeprimes', 'after_' + os.path.splitext(row[1].filename)[0] + '.tsv'),\n",
    "#                                                sep = '\\t', index = None))[0]\n",
    "#          for row in tqdm(pd.concat(\n",
    "#                  val for val in\n",
    "#                  df(tuple(scanzip.scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   include = ['Onset', 'Output'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]\n",
    "os.makedirs(pjoin(os.getcwd(), 'cimaq_dialects'), exist_ok = True)\n",
    "os.makedirs(pjoin(os.getcwd(), 'cimaq_uzeprimes'), exist_ok = True)\n",
    "\n",
    "newsh = [(snif.bytes2df(row[1].bsheets),\n",
    "          (pd.concat([row[1].drop('bsheets'),\n",
    "                      df.from_dict(snif.sniff_bytes(row[1].bsheets), orient = 'index')]).to_csv(\n",
    "              pjoin(os.getcwd(), 'cimaq_dialects',\n",
    "                    'before_' + os.path.splitext(row[1].filename)[0] + '.tsv'),\n",
    "                                               sep = '\\t', index = None)),\n",
    "          snif.bytes2df(row[1].bsheets).to_csv(pjoin(os.getcwd(), 'cimaq_uzeprimes',\n",
    "                                                    os.path.splitext(row[1].filename)[0] + '.tsv'),\n",
    "                                               sep = '\\t', index = None),\n",
    "          df.from_dict(snif.sniff_bytes(snif.get_bytes(pjoin(os.getcwd(), 'cimaq_uzeprimes',\n",
    "                                                row[1].filename))), orient = 'index').to_csv(\n",
    "              pjoin(os.getcwd(), 'cimaq_dialects', 'after_' + os.path.splitext(row[1].filename)[0] + '.tsv'),\n",
    "                                               sep = '\\t', index = None))[0]\n",
    "         for row in tqdm(pd.concat(\n",
    "                 val for val in\n",
    "                 df(tuple(scanzip.scanzip(apath,\n",
    "                                  exclude = ['Practice', 'Pratique',\n",
    "                                             'PRATIQUE', 'PRACTICE'],\n",
    "                                  include = ['Onset', 'Output'],\n",
    "                                  to_xtrct = ['.pdf', '.edat2'],\n",
    "                                  dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                          for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "                                         cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "                    dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "                             desc = 'creating Pandas DataFrames')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 3947.85it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6371.56it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6248.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5429.52it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5019.36it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5186.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5475.59it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5551.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5353.78it/s]\n",
      " 10%|▉         | 9/94 [00:00<00:00, 88.06it/s]\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 5286.74it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4046.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6025.06it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6224.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5009.41it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 6396.42it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6101.44it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 2826.89it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 3601.59it/s]\n",
      " 19%|█▉        | 18/94 [00:00<00:00, 85.90it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4321.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 4446.54it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4113.58it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 3866.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 3854.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 3988.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 3714.59it/s]\n",
      " 27%|██▋       | 25/94 [00:00<00:00, 78.87it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4105.52it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5556.29it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6309.94it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5978.58it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5334.57it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6221.68it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5524.01it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5627.11it/s]\n",
      " 35%|███▌      | 33/94 [00:00<00:00, 77.87it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4103.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6175.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5486.85it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4746.48it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5201.09it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5432.03it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5335.29it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 4221.75it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6460.23it/s]\n",
      " 45%|████▍     | 42/94 [00:00<00:00, 80.41it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5483.77it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5804.26it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5425.01it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6488.96it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5527.13it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5199.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5320.79it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5412.01it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5476.49it/s]\n",
      " 54%|█████▍    | 51/94 [00:00<00:00, 81.23it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 4857.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5080.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 5043.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5543.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 3341.69it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5673.73it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5508.85it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5395.76it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5372.15it/s]\n",
      " 64%|██████▍   | 60/94 [00:00<00:00, 81.16it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5563.66it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5220.04it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5468.45it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5123.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6160.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5509.50it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 3927.78it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5866.16it/s]\n",
      " 72%|███████▏  | 68/94 [00:00<00:00, 69.57it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5111.44it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6326.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 4578.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4590.63it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5389.40it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5121.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5176.33it/s]\n",
      " 80%|███████▉  | 75/94 [00:01<00:00, 64.75it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5593.23it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5132.22it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5026.13it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5214.94it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 6034.18it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5313.09it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 5556.04it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5571.97it/s]\n",
      " 88%|████████▊ | 83/94 [00:01<00:00, 67.74it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 6362.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5349.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5221.43it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5454.23it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 5462.35it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 5619.04it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5141.65it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5150.33it/s]\n",
      " 97%|█████████▋| 91/94 [00:01<00:00, 70.50it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5504.34it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 6082.48it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 5384.22it/s]\n",
      "100%|██████████| 94/94 [00:01<00:00, 73.77it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "cimaq_infos = pd.concat(val for val in\n",
    "                        df(tuple(sz1(apath,\n",
    "                                         exclude = ['Practice', 'Pratique',\n",
    "                                                    'PRATIQUE', 'PRACTICE'],\n",
    "                                         include = ['Onset', 'Output'],\n",
    "                                         to_xtrct = ['.pdf', '.edat2'],\n",
    "                                         dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                                 for apath in\n",
    "                                 tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:], snif.loadimages(xpu(pjoin(\n",
    "                                         cimaq_dir, 'task_files/zipped_eprime')))))))\\\n",
    "                        [0].values.flatten()).dropna().reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "322"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# snif.filter_lst_inc(['.pdf', '.edat2'], cimaq_infos.filename)\n",
    "# [itm for itm in cimaq_infos.filename if '.pdf' in itm]\n",
    "# 322/94\n",
    "len(cimaq_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>compress_type</th>\n",
       "      <th>filemode</th>\n",
       "      <th>external_attr</th>\n",
       "      <th>file_size</th>\n",
       "      <th>compress_size</th>\n",
       "      <th>src_names</th>\n",
       "      <th>bsheets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3025432_658178_tache_irm_onset-event-encoding_...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>21511</td>\n",
       "      <td>3011</td>\n",
       "      <td>3025432_658178_Tache_IRM/Onset-Event-Encoding_...</td>\n",
       "      <td>b'1             CTL           CTL0            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3025432_658178_tache_irm_output-responses-enco...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>3557</td>\n",
       "      <td>1206</td>\n",
       "      <td>3025432_658178_Tache_IRM/Output-Responses-Enco...</td>\n",
       "      <td>b'TrialNumber\\tCategory\\tTrialCode\\tOldNumber\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3025432_658178_tache_irm_output_retrieval_cima...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>5816</td>\n",
       "      <td>2174</td>\n",
       "      <td>3025432_658178_Tache_IRM/Output_Retrieval_CIMA...</td>\n",
       "      <td>b'category\\tStim\\tOldNumber\\tRecognition_ACC\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3025432_658178_tache_irm_retrieval-3025432-1.txt</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>126838</td>\n",
       "      <td>6016</td>\n",
       "      <td>3025432_658178_Tache_IRM/Retrieval-3025432-1.txt</td>\n",
       "      <td>b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3291977_748676_tache_irm_encoding-scan-3291977...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>205514</td>\n",
       "      <td>12143</td>\n",
       "      <td>3291977_748676_Tache_IRM/Encoding-scan-3291977...</td>\n",
       "      <td>b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>9929164_197192_tache_irm_encoding-scan-9929164...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>205522</td>\n",
       "      <td>12154</td>\n",
       "      <td>9929164_197192_Tache_IRM/Encoding-scan-9929164...</td>\n",
       "      <td>b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>318</th>\n",
       "      <td>9929164_197192_tache_irm_onset-event-encoding_...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>21511</td>\n",
       "      <td>2780</td>\n",
       "      <td>9929164_197192_Tache_IRM/Onset-Event-Encoding_...</td>\n",
       "      <td>b'1             CTL           CTL0            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>319</th>\n",
       "      <td>9929164_197192_tache_irm_output-responses-enco...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>3125</td>\n",
       "      <td>873</td>\n",
       "      <td>9929164_197192_Tache_IRM/Output-Responses-Enco...</td>\n",
       "      <td>b'TrialNumber\\tCategory\\tTrialCode\\tOldNumber\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>9929164_197192_tache_irm_output_retrieval_cima...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>5876</td>\n",
       "      <td>2216</td>\n",
       "      <td>9929164_197192_Tache_IRM/Output_Retrieval_CIMA...</td>\n",
       "      <td>b'category\\tStim\\tOldNumber\\tRecognition_ACC\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>321</th>\n",
       "      <td>9929164_197192_tache_irm_retrieval-9929164-1.txt</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>126946</td>\n",
       "      <td>6076</td>\n",
       "      <td>9929164_197192_Tache_IRM/Retrieval-9929164-1.txt</td>\n",
       "      <td>b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>322 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename compress_type  \\\n",
       "0    3025432_658178_tache_irm_onset-event-encoding_...       deflate   \n",
       "1    3025432_658178_tache_irm_output-responses-enco...       deflate   \n",
       "2    3025432_658178_tache_irm_output_retrieval_cima...       deflate   \n",
       "3     3025432_658178_tache_irm_retrieval-3025432-1.txt       deflate   \n",
       "4    3291977_748676_tache_irm_encoding-scan-3291977...       deflate   \n",
       "..                                                 ...           ...   \n",
       "317  9929164_197192_tache_irm_encoding-scan-9929164...       deflate   \n",
       "318  9929164_197192_tache_irm_onset-event-encoding_...       deflate   \n",
       "319  9929164_197192_tache_irm_output-responses-enco...       deflate   \n",
       "320  9929164_197192_tache_irm_output_retrieval_cima...       deflate   \n",
       "321   9929164_197192_tache_irm_retrieval-9929164-1.txt       deflate   \n",
       "\n",
       "       filemode external_attr file_size compress_size  \\\n",
       "0    -rwxrwxrwx        0x4000     21511          3011   \n",
       "1    -rwxrwxrwx        0x4000      3557          1206   \n",
       "2    -rwxrwxrwx        0x4000      5816          2174   \n",
       "3    -rwxrwxrwx        0x4000    126838          6016   \n",
       "4    -rwxrwxrwx        0x4000    205514         12143   \n",
       "..          ...           ...       ...           ...   \n",
       "317  -rwxrwxrwx        0x4000    205522         12154   \n",
       "318  -rwxrwxrwx        0x4000     21511          2780   \n",
       "319  -rwxrwxrwx        0x4000      3125           873   \n",
       "320  -rwxrwxrwx        0x4000      5876          2216   \n",
       "321  -rwxrwxrwx        0x4000    126946          6076   \n",
       "\n",
       "                                             src_names  \\\n",
       "0    3025432_658178_Tache_IRM/Onset-Event-Encoding_...   \n",
       "1    3025432_658178_Tache_IRM/Output-Responses-Enco...   \n",
       "2    3025432_658178_Tache_IRM/Output_Retrieval_CIMA...   \n",
       "3     3025432_658178_Tache_IRM/Retrieval-3025432-1.txt   \n",
       "4    3291977_748676_Tache_IRM/Encoding-scan-3291977...   \n",
       "..                                                 ...   \n",
       "317  9929164_197192_Tache_IRM/Encoding-scan-9929164...   \n",
       "318  9929164_197192_Tache_IRM/Onset-Event-Encoding_...   \n",
       "319  9929164_197192_Tache_IRM/Output-Responses-Enco...   \n",
       "320  9929164_197192_Tache_IRM/Output_Retrieval_CIMA...   \n",
       "321   9929164_197192_Tache_IRM/Retrieval-9929164-1.txt   \n",
       "\n",
       "                                               bsheets  \n",
       "0    b'1             CTL           CTL0            ...  \n",
       "1    b'TrialNumber\\tCategory\\tTrialCode\\tOldNumber\\...  \n",
       "2    b'category\\tStim\\tOldNumber\\tRecognition_ACC\\t...  \n",
       "3    b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...  \n",
       "4    b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...  \n",
       "..                                                 ...  \n",
       "317  b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...  \n",
       "318  b'1             CTL           CTL0            ...  \n",
       "319  b'TrialNumber\\tCategory\\tTrialCode\\tOldNumber\\...  \n",
       "320  b'category\\tStim\\tOldNumber\\tRecognition_ACC\\t...  \n",
       "321  b'\\xff\\xfe*\\x00*\\x00*\\x00 \\x00H\\x00e\\x00a\\x00d...  \n",
       "\n",
       "[322 rows x 8 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>ctl</td>\n",
       "      <td>ctl0</td>\n",
       "      <td>nan</td>\n",
       "      <td>control</td>\n",
       "      <td>0.025</td>\n",
       "      <td>fixation</td>\n",
       "      <td>3.034</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>enc</td>\n",
       "      <td>enc00</td>\n",
       "      <td>nan</td>\n",
       "      <td>encoding</td>\n",
       "      <td>4.521</td>\n",
       "      <td>fixation</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>enc</td>\n",
       "      <td>enc000</td>\n",
       "      <td>nan</td>\n",
       "      <td>encoding</td>\n",
       "      <td>8.032</td>\n",
       "      <td>fixation</td>\n",
       "      <td>11.041</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>ctl</td>\n",
       "      <td>ctl01</td>\n",
       "      <td>nan</td>\n",
       "      <td>control</td>\n",
       "      <td>11.542</td>\n",
       "      <td>fixation</td>\n",
       "      <td>14.551</td>\n",
       "      <td>5.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>enc</td>\n",
       "      <td>enc01</td>\n",
       "      <td>old60</td>\n",
       "      <td>encoding</td>\n",
       "      <td>20.051</td>\n",
       "      <td>fixation</td>\n",
       "      <td>23.06</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>116</td>\n",
       "      <td>enc</td>\n",
       "      <td>enc75</td>\n",
       "      <td>old45</td>\n",
       "      <td>encoding</td>\n",
       "      <td>694.41</td>\n",
       "      <td>fixation</td>\n",
       "      <td>697.419</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>ctl</td>\n",
       "      <td>ctl39</td>\n",
       "      <td>nan</td>\n",
       "      <td>control</td>\n",
       "      <td>703.42</td>\n",
       "      <td>fixation</td>\n",
       "      <td>706.429</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>enc</td>\n",
       "      <td>enc76</td>\n",
       "      <td>old18</td>\n",
       "      <td>encoding</td>\n",
       "      <td>716.927</td>\n",
       "      <td>fixation</td>\n",
       "      <td>719.936</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>enc</td>\n",
       "      <td>enc77</td>\n",
       "      <td>old33</td>\n",
       "      <td>encoding</td>\n",
       "      <td>724.934</td>\n",
       "      <td>fixation</td>\n",
       "      <td>727.943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>enc</td>\n",
       "      <td>enc78</td>\n",
       "      <td>old14</td>\n",
       "      <td>encoding</td>\n",
       "      <td>728.929</td>\n",
       "      <td>fixation</td>\n",
       "      <td>731.938</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1       2      3         4        5         6        7     8\n",
       "0      1  ctl    ctl0    nan   control    0.025  fixation    3.034   1.5\n",
       "1      2  enc   enc00    nan  encoding    4.521  fixation     7.53   0.5\n",
       "2      3  enc  enc000    nan  encoding    8.032  fixation   11.041   0.5\n",
       "3      4  ctl   ctl01    nan   control   11.542  fixation   14.551   5.5\n",
       "4      5  enc   enc01  old60  encoding   20.051  fixation    23.06   0.5\n",
       "..   ...  ...     ...    ...       ...      ...       ...      ...   ...\n",
       "115  116  enc   enc75  old45  encoding   694.41  fixation  697.419     6\n",
       "116  117  ctl   ctl39    nan   control   703.42  fixation  706.429  10.5\n",
       "117  118  enc   enc76  old18  encoding  716.927  fixation  719.936     5\n",
       "118  119  enc   enc77  old33  encoding  724.934  fixation  727.943     1\n",
       "119  120  enc   enc78  old14  encoding  728.929  fixation  731.938  18.5\n",
       "\n",
       "[120 rows x 9 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "scanzip() got an unexpected keyword argument 'withbytes'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-88d14f99f31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         for row in tqdm(pd.concat(\n\u001b[1;32m     10\u001b[0m                  \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                  df(tuple(sz1(apath,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                   exclude = ['Practice', 'Pratique',\n\u001b[1;32m     13\u001b[0m                                              'PRATIQUE', 'PRACTICE'],\n",
      "\u001b[0;32m<ipython-input-7-88d14f99f31a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      9\u001b[0m         for row in tqdm(pd.concat(\n\u001b[1;32m     10\u001b[0m                  \u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                  df(tuple(sz1(apath,\n\u001b[0m\u001b[1;32m     12\u001b[0m                                   exclude = ['Practice', 'Pratique',\n\u001b[1;32m     13\u001b[0m                                              'PRATIQUE', 'PRACTICE'],\n",
      "\u001b[0;31mTypeError\u001b[0m: scanzip() got an unexpected keyword argument 'withbytes'"
     ]
    }
   ],
   "source": [
    "# 16s vs 23s (6 seconds faster)\n",
    "newsh = [df((line.split('\\t') for line in unidecode(\n",
    "                  snif.clean_bytes(row[1].bsheets).decode()).split('\\n')),\n",
    "                 dtype = object).T.set_index(0, drop = True).T\n",
    "             if row[1].has_header else\n",
    "        df((line.split('\\t') for line in unidecode(\n",
    "                 snif.clean_bytes(row[1].bsheets).decode()).split('\\n')),\n",
    "                                   dtype = object)\n",
    "        for row in tqdm(pd.concat(\n",
    "                 val.T for val in\n",
    "                 df(tuple(sz1(apath,\n",
    "                                  exclude = ['Practice', 'Pratique',\n",
    "                                             'PRATIQUE', 'PRACTICE'],\n",
    "                                  to_xtrct = ['.pdf', '.edat2'],\n",
    "                                  dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'),\n",
    "                                  withbytes = True)\n",
    "                          for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "                                         cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "                    dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "                             desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'filename': '3025432_658178_tache_irm_onset-event-encoding_cimaq_3025432_session1b.txt',\n",
       "   'compress_type': 'deflate',\n",
       "   'filemode': '-rwxrwxrwx',\n",
       "   'external_attr': '0x4000',\n",
       "   'file_size': '21511',\n",
       "   'compress_size': '3011',\n",
       "   'src_names': '3025432_658178_Tache_IRM/Onset-Event-Encoding_CIMAQ_3025432_session1B.txt',\n",
       "   'encoding': 'ascii',\n",
       "   'delimiter': b'             ',\n",
       "   'has_header': False,\n",
       "   'dup_index': True,\n",
       "   'lineterminator': b'\\r\\n',\n",
       "   'nfields': 7,\n",
       "   'width': 90,\n",
       "   'nrows': 240},\n",
       "         0    1       2      3         4        5         6        7     8\n",
       "  0      1  ctl    ctl0    nan   control    0.025  fixation    3.034   1.5\n",
       "  1      2  enc   enc00    nan  encoding    4.521  fixation     7.53   0.5\n",
       "  2      3  enc  enc000    nan  encoding    8.032  fixation   11.041   0.5\n",
       "  3      4  ctl   ctl01    nan   control   11.542  fixation   14.551   5.5\n",
       "  4      5  enc   enc01  old60  encoding   20.051  fixation    23.06   0.5\n",
       "  ..   ...  ...     ...    ...       ...      ...       ...      ...   ...\n",
       "  115  116  enc   enc75  old45  encoding   694.41  fixation  697.419     6\n",
       "  116  117  ctl   ctl39    nan   control   703.42  fixation  706.429  10.5\n",
       "  117  118  enc   enc76  old18  encoding  716.927  fixation  719.936     5\n",
       "  118  119  enc   enc77  old33  encoding  724.934  fixation  727.943     1\n",
       "  119  120  enc   enc78  old14  encoding  728.929  fixation  731.938  18.5\n",
       "  \n",
       "  [120 rows x 9 columns]),\n",
       " ({'filename': '3025432_658178_tache_irm_output-responses-encoding_cimaq_3025432_session1b.txt',\n",
       "   'compress_type': 'deflate',\n",
       "   'filemode': '-rwxrwxrwx',\n",
       "   'external_attr': '0x4000',\n",
       "   'file_size': '3557',\n",
       "   'compress_size': '1206',\n",
       "   'src_names': '3025432_658178_Tache_IRM/Output-Responses-Encoding_CIMAQ_3025432_session1B.txt',\n",
       "   'encoding': 'ascii',\n",
       "   'delimiter': b'\\t',\n",
       "   'has_header': True,\n",
       "   'dup_index': False,\n",
       "   'lineterminator': b'\\r\\n',\n",
       "   'nfields': 8,\n",
       "   'width': 30,\n",
       "   'nrows': 121},\n",
       "  0   trialnumber category trialcode oldnumber correctsource stim_resp stim_rt  \\\n",
       "  1             1      ctl      ctl0       nan             5         1    1183   \n",
       "  2             2      enc     enc00       nan             8         1    2759   \n",
       "  3             3      enc    enc000       nan             9         1    1656   \n",
       "  4             4      ctl     ctl01       nan             8         1    1402   \n",
       "  5             5      enc     enc01     old60             6       nan       0   \n",
       "  ..          ...      ...       ...       ...           ...       ...     ...   \n",
       "  116         116      enc     enc75     old45             6         1    1974   \n",
       "  117         117      ctl     ctl39       nan             9         1     836   \n",
       "  118         118      enc     enc76     old18             5         1    1201   \n",
       "  119         119      enc     enc77     old33             5         1    1249   \n",
       "  120         120      enc     enc78     old14             8         1    1150   \n",
       "  \n",
       "  0   stim_acc  \n",
       "  1          0  \n",
       "  2          0  \n",
       "  3          0  \n",
       "  4          0  \n",
       "  5          1  \n",
       "  ..       ...  \n",
       "  116        0  \n",
       "  117        0  \n",
       "  118        0  \n",
       "  119        0  \n",
       "  120        0  \n",
       "  \n",
       "  [120 rows x 8 columns]),\n",
       " ({'filename': '3025432_658178_tache_irm_output_retrieval_cimaq_3025432_1.txt',\n",
       "   'compress_type': 'deflate',\n",
       "   'filemode': '-rwxrwxrwx',\n",
       "   'external_attr': '0x4000',\n",
       "   'file_size': '5816',\n",
       "   'compress_size': '2174',\n",
       "   'src_names': '3025432_658178_Tache_IRM/Output_Retrieval_CIMAQ_3025432_1.txt',\n",
       "   'encoding': 'ISO-8859-1',\n",
       "   'delimiter': b'\\t',\n",
       "   'has_header': True,\n",
       "   'dup_index': False,\n",
       "   'lineterminator': b'\\r\\n',\n",
       "   'nfields': 11,\n",
       "   'width': 57,\n",
       "   'nrows': 118},\n",
       "  0   category                          stim oldnumber recognition_acc  \\\n",
       "  1        old            animal_penguin.bmp     old10               1   \n",
       "  2        old      sporting_bicycle_old.bmp     old55               0   \n",
       "  3        old           fruit_lemon_new.bmp     old69               1   \n",
       "  4        new            vegie_broccoli.bmp     new36               1   \n",
       "  5        old          animal_zebra_old.bmp     old13               1   \n",
       "  ..       ...                           ...       ...             ...   \n",
       "  113      new       kitchen_spatula_old.bmp     new37               0   \n",
       "  114      old        musical_violin_old.bmp     old51               1   \n",
       "  115      old              vegie_carrot.bmp     old78               1   \n",
       "  116      old  sporting_karate clothing.bmp     old60               1   \n",
       "  117      old        fruit_green grapes.bmp     old68               0   \n",
       "  \n",
       "  0   recognition_resp recognition_rt spatial_resp spatial_rt  \\\n",
       "  1                  1           3216            8        462   \n",
       "  2                  2           9409            8        462   \n",
       "  3                  1           2746            5       4435   \n",
       "  4                  2           1633            5       4435   \n",
       "  5                  1           1311            8        369   \n",
       "  ..               ...            ...          ...        ...   \n",
       "  113                1           7013            5       1150   \n",
       "  114                1           2255            8       1748   \n",
       "  115                1           1765            8        414   \n",
       "  116                1           4964            6        518   \n",
       "  117                2           1240            6        518   \n",
       "  \n",
       "  0   spatial_acc( corriger voir output-encodage)  \n",
       "  1                                          None  \n",
       "  2                                          None  \n",
       "  3                                          None  \n",
       "  4                                          None  \n",
       "  5                                          None  \n",
       "  ..                                          ...  \n",
       "  113                                        None  \n",
       "  114                                        None  \n",
       "  115                                        None  \n",
       "  116                                        None  \n",
       "  117                                        None  \n",
       "  \n",
       "  [117 rows x 9 columns]),\n",
       " ({'filename': '3025432_658178_tache_irm_retrieval-3025432-1.txt',\n",
       "   'compress_type': 'deflate',\n",
       "   'filemode': '-rwxrwxrwx',\n",
       "   'external_attr': '0x4000',\n",
       "   'file_size': '126838',\n",
       "   'compress_size': '6016',\n",
       "   'src_names': '3025432_658178_Tache_IRM/Retrieval-3025432-1.txt',\n",
       "   'encoding': 'UTF-16',\n",
       "   'delimiter': b'\\r',\n",
       "   'has_header': False,\n",
       "   'dup_index': False,\n",
       "   'lineterminator': b'\\r',\n",
       "   'nfields': 14,\n",
       "   'width': 1069,\n",
       "   'nrows': 5489},\n",
       "                                                        0     1\n",
       "  0              *\u0000*\u0000*\u0000 \u0000h\u0000e\u0000a\u0000d\u0000e\u0000r\u0000 \u0000s\u0000t\u0000a\u0000r\u0000t\u0000 \u0000*\u0000*\u0000*\u0000  None\n",
       "  1                                                     \u0000  None\n",
       "  2                   \u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000p\u0000e\u0000r\u0000s\u0000i\u0000s\u0000t\u0000:\u0000 \u00001\u0000  None\n",
       "  3                                                     \u0000  None\n",
       "  4                 \u0000l\u0000e\u0000v\u0000e\u0000l\u0000n\u0000a\u0000m\u0000e\u0000:\u0000 \u0000s\u0000e\u0000s\u0000s\u0000i\u0000o\u0000n\u0000  None\n",
       "  ...                                                 ...   ...\n",
       "  5484  \u0000e\u0000x\u0000p\u0000e\u0000r\u0000i\u0000m\u0000e\u0000n\u0000t\u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000:\u0000 \u00001\u0000.\u00000\u0000....  None\n",
       "  5485                                                  \u0000  None\n",
       "  5486          \u0000*\u0000*\u0000*\u0000 \u0000l\u0000o\u0000g\u0000f\u0000r\u0000a\u0000m\u0000e\u0000 \u0000e\u0000n\u0000d\u0000 \u0000*\u0000*\u0000*\u0000  None\n",
       "  5487                                                  \u0000  None\n",
       "  5488                                                  \u0000  None\n",
       "  \n",
       "  [5489 rows x 2 columns]),\n",
       " ({'filename': '3291977_748676_tache_irm_encoding-scan-3291977-1.txt',\n",
       "   'compress_type': 'deflate',\n",
       "   'filemode': '-rwxrwxrwx',\n",
       "   'external_attr': '0x4000',\n",
       "   'file_size': '205514',\n",
       "   'compress_size': '12143',\n",
       "   'src_names': '3291977_748676_Tache_IRM/Encoding-scan-3291977-1.txt',\n",
       "   'encoding': 'UTF-16',\n",
       "   'delimiter': b'\\r',\n",
       "   'has_header': False,\n",
       "   'dup_index': False,\n",
       "   'lineterminator': b'\\r',\n",
       "   'nfields': 14,\n",
       "   'width': 1069,\n",
       "   'nrows': 9623},\n",
       "                                                    0     1     2\n",
       "  0          *\u0000*\u0000*\u0000 \u0000h\u0000e\u0000a\u0000d\u0000e\u0000r\u0000 \u0000s\u0000t\u0000a\u0000r\u0000t\u0000 \u0000*\u0000*\u0000*\u0000  None  None\n",
       "  1                                                 \u0000  None  None\n",
       "  2               \u0000v\u0000e\u0000r\u0000s\u0000i\u0000o\u0000n\u0000p\u0000e\u0000r\u0000s\u0000i\u0000s\u0000t\u0000:\u0000 \u00001\u0000  None  None\n",
       "  3                                                 \u0000  None  None\n",
       "  4             \u0000l\u0000e\u0000v\u0000e\u0000l\u0000n\u0000a\u0000m\u0000e\u0000:\u0000 \u0000s\u0000e\u0000s\u0000s\u0000i\u0000o\u0000n\u0000  None  None\n",
       "  ...                                             ...   ...   ...\n",
       "  9618  \u0000s\u0000t\u0000a\u0000r\u0000t\u0000t\u0000i\u0000m\u0000e\u0000s\u0000t\u0000a\u0000m\u0000p\u0000:\u0000 \u00001\u00000\u00000\u00008\u00003\u00000\u0000  None  None\n",
       "  9619                                              \u0000  None  None\n",
       "  9620      \u0000*\u0000*\u0000*\u0000 \u0000l\u0000o\u0000g\u0000f\u0000r\u0000a\u0000m\u0000e\u0000 \u0000e\u0000n\u0000d\u0000 \u0000*\u0000*\u0000*\u0000  None  None\n",
       "  9621                                              \u0000  None  None\n",
       "  9622                                              \u0000  None  None\n",
       "  \n",
       "  [9623 rows x 3 columns])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsh[0:5]\n",
    "\n",
    "newdir = pjoin(os.getcwd(), 'cimaq_clean_eprime_datas3')\n",
    "os.makedirs(newdir, exist_ok = True)\n",
    "[itm[1].to_csv(pjoin(newdir, itm[0]['filename']), sep = '\\t',\n",
    "               header = [0 if itm[0]['has_header'] else None][0],\n",
    "               index = None)\n",
    " for itm in newsheets]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2945.96it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5880.96it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6126.65it/s]\n",
      "  3%|▎         | 3/94 [00:00<00:04, 18.22it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5317.32it/s]\n",
      "  4%|▍         | 4/94 [00:00<00:06, 14.01it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5402.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5396.69it/s]\n",
      "  6%|▋         | 6/94 [00:00<00:06, 13.22it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5283.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4604.07it/s]\n",
      "  9%|▊         | 8/94 [00:00<00:06, 12.64it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2679.04it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2654.96it/s]\n",
      " 11%|█         | 10/94 [00:00<00:06, 12.88it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5236.33it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6230.40it/s]\n",
      " 13%|█▎        | 12/94 [00:00<00:06, 12.46it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5807.68it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5385.60it/s]\n",
      " 15%|█▍        | 14/94 [00:01<00:06, 12.21it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6168.09it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4706.36it/s]\n",
      " 17%|█▋        | 16/94 [00:01<00:06, 12.16it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5775.69it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4928.68it/s]\n",
      " 19%|█▉        | 18/94 [00:01<00:06, 12.07it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5539.23it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5949.37it/s]\n",
      " 21%|██▏       | 20/94 [00:01<00:06, 11.91it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6160.85it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6078.70it/s]\n",
      " 23%|██▎       | 22/94 [00:01<00:06, 11.63it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6107.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2581.75it/s]\n",
      " 26%|██▌       | 24/94 [00:02<00:06, 10.97it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5358.08it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5836.77it/s]\n",
      " 28%|██▊       | 26/94 [00:02<00:06, 10.49it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4710.58it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5722.11it/s]\n",
      " 30%|██▉       | 28/94 [00:02<00:06, 10.84it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6177.18it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5285.16it/s]\n",
      " 32%|███▏      | 30/94 [00:02<00:05, 11.25it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2747.12it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5232.42it/s]\n",
      " 34%|███▍      | 32/94 [00:02<00:05, 11.26it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4768.42it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5112.51it/s]\n",
      " 36%|███▌      | 34/94 [00:02<00:05, 10.65it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6353.08it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3517.53it/s]\n",
      " 38%|███▊      | 36/94 [00:03<00:05, 10.95it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4845.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5367.68it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5348.51it/s]\n",
      " 41%|████▏     | 39/94 [00:03<00:04, 12.15it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5277.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5333.55it/s]\n",
      " 44%|████▎     | 41/94 [00:03<00:04, 12.58it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5579.02it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5402.25it/s]\n",
      " 46%|████▌     | 43/94 [00:03<00:04, 12.35it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6166.28it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5003.94it/s]\n",
      " 48%|████▊     | 45/94 [00:03<00:04, 12.03it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5861.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3942.02it/s]\n",
      " 50%|█████     | 47/94 [00:03<00:03, 11.91it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3345.27it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3525.22it/s]\n",
      " 52%|█████▏    | 49/94 [00:04<00:03, 11.98it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3300.00it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2954.57it/s]\n",
      " 54%|█████▍    | 51/94 [00:04<00:03, 11.91it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5309.25it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5681.80it/s]\n",
      " 56%|█████▋    | 53/94 [00:04<00:03, 11.34it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 2741.97it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3725.62it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5336.26it/s]\n",
      " 60%|█████▉    | 56/94 [00:04<00:02, 12.92it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5232.42it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4843.31it/s]\n",
      " 62%|██████▏   | 58/94 [00:04<00:02, 12.49it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5053.38it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4029.11it/s]\n",
      " 64%|██████▍   | 60/94 [00:04<00:02, 13.10it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5052.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5549.49it/s]\n",
      " 66%|██████▌   | 62/94 [00:05<00:02, 12.76it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5254.70it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5395.30it/s]\n",
      " 68%|██████▊   | 64/94 [00:05<00:02, 13.60it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5831.90it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5048.51it/s]\n",
      " 70%|███████   | 66/94 [00:05<00:02, 13.39it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5265.26it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5742.48it/s]\n",
      " 72%|███████▏  | 68/94 [00:05<00:01, 13.21it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5997.00it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6047.15it/s]\n",
      " 74%|███████▍  | 70/94 [00:05<00:01, 13.02it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 3640.10it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4826.59it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5282.50it/s]\n",
      " 78%|███████▊  | 73/94 [00:05<00:01, 14.60it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5110.33it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4964.85it/s]\n",
      " 80%|███████▉  | 75/94 [00:05<00:01, 14.77it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5091.20it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4751.14it/s]\n",
      " 82%|████████▏ | 77/94 [00:06<00:01, 13.80it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5444.79it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5458.49it/s]\n",
      " 84%|████████▍ | 79/94 [00:06<00:01, 13.34it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5542.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5440.08it/s]\n",
      " 86%|████████▌ | 81/94 [00:06<00:01, 12.07it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5395.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5064.36it/s]\n",
      " 88%|████████▊ | 83/94 [00:06<00:00, 11.67it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3353.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4918.27it/s]\n",
      " 90%|█████████ | 85/94 [00:06<00:00, 11.53it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2926.94it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2769.25it/s]\n",
      " 93%|█████████▎| 87/94 [00:07<00:00, 12.36it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5261.29it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5437.73it/s]\n",
      " 95%|█████████▍| 89/94 [00:07<00:00, 12.33it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3102.75it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3291.20it/s]\n",
      " 97%|█████████▋| 91/94 [00:07<00:00, 12.19it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3177.98it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3481.33it/s]\n",
      " 99%|█████████▉| 93/94 [00:07<00:00, 12.30it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3661.87it/s]\n",
      "100%|██████████| 94/94 [00:07<00:00, 12.41it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n"
     ]
    }
   ],
   "source": [
    "# qcshet = snif.clean_bytes(xpu(pjoin(\n",
    "#     cimaq_dir, 'participants/sub_list_TaskQC.tsv'\n",
    "# ))).decode().split()[1:]\n",
    "\n",
    "# snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#     cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:],\n",
    "#                     snif.loadimages(xpu(pjoin(cimaq_dir, 'task_files/zipped_eprime'))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "int"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_infos.delimiter.unique()#[-1].decode('utf16')\n",
    "snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "    cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:],\n",
    "                cimaq_infos.filename).__len__()\n",
    "type(cimaq_infos.width.unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m6\u001b[0m\n\u001b[0;31m    df((line.split('\\t') for line in unidecode(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "# def bytes2df(inpt, **kwargs):\n",
    "#     return [df((line.split('\\t') for line in unidecode(\n",
    "#                   clean_bytes(inpt, **kwargs).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T\n",
    "#              if get_has_header(inpt) else\n",
    "#         df((line.split('\\t') for line in unidecode(\n",
    "#                  clean_bytes(inpt, **kwargs).decode()).split('\\n')),\n",
    "#                                    dtype = object)][0]\n",
    "#         [for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newsh = [df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T\n",
    "#              if row[1].has_header else\n",
    "#         df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object)\n",
    "#         for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5076.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3316.70it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3461.79it/s]\n",
      "creating Pandas DataFrames: 0it [01:05, ?it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5344.42it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5386.98it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5435.85it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5085.24it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5393.91it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5334.91it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5076.62it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5266.58it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6031.50it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5966.29it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5369.05it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6105.25it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6139.20it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6135.61it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5341.70it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6151.81it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6151.81it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5937.58it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6085.76it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5491.36it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5132.53it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5334.91it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5352.61it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5340.34it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5427.41it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5848.17it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5169.22it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5986.73it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4365.43it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4651.04it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4881.64it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6833.34it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5258.66it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4689.87it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4786.93it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5369.05it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5181.35it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5249.44it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6179.00it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4477.27it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5747.20it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5229.81it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6012.48it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5233.72it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5043.66it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4211.15it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5430.22it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5356.71it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5253.83it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6186.29it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 5081.95it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5428.82it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5041.23it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5445.73it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5452.81it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5143.23it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4940.29it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5153.97it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5410.61it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5198.69it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5326.78it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5957.82it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5314.63it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5336.26it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5995.29it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5748.77it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3631.43it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5205.47it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4239.53it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3264.56it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 4978.40it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2965.43it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 3873.45it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4984.91it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 4985.31it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5263.94it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4958.98it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4980.18it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5275.85it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5112.51it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5856.33it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5015.91it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5293.17it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4237.53it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5333.55it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 4098.00it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5448.56it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2393.46it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5513.02it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5675.65it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5386.98it/s]\n",
      "100%|██████████| 94/94 [00:07<00:00, 12.05it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n",
      "creating Pandas DataFrames: 322it [00:13, 23.19it/s]\n"
     ]
    }
   ],
   "source": [
    "newsh2 = [df((line.split('\\t') for line in unidecode(\n",
    "                  snif.clean_bytes(row[1].bsheets,\n",
    "                                   [snif.sniff_bytes(row[1].bsheets).values()]\\\n",
    "                                   [1:4]).decode()).split('\\n')),\n",
    "                 dtype = object).T.set_index(0, drop = True).T\n",
    "             if row[1].has_header else\n",
    "        df((line.split('\\t') for line in unidecode(\n",
    "                 snif.clean_bytes(row[1].bsheets,\n",
    "                                  [snif.sniff_bytes(row[1].bsheets).values()]\\\n",
    "                                  [1:4]).decode()).split('\\n')),\n",
    "                                   dtype = object)\n",
    "        for row in tqdm(pd.concat(\n",
    "                 val.T for val in\n",
    "                 df(tuple(scanzip(apath,\n",
    "                                  exclude = ['Practice', 'Pratique',\n",
    "                                             'PRATIQUE', 'PRACTICE'],\n",
    "                                  to_xtrct = ['.pdf', '.edat2'],\n",
    "                                  dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                          for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "                                         cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "                    dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "                             desc = 'creating Pandas DataFrames')]\n",
    "# newsheets = [(row[1].to_dict(),\n",
    "#               df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T)\n",
    "#              if row[1].has_header else\n",
    "#              (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object))\n",
    "#              for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### MODEL ##################################\n",
    "# newsheets = [(row[1].to_dict(),\n",
    "#               df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T)\n",
    "#              if row[1].has_header else\n",
    "#              (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object))\n",
    "#              for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      cimaq_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          cimaq_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #             (\"encoding\", \"delimiter\", \"has_header\", \"dup_index\",\n",
    "# #              \"lineterminator\", \"nfields\", \"width\", \"nrows\").\n",
    "######### MODEL #############################################33\n",
    "# newsheets = [(row[1].to_dict(),\n",
    "#               df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T)\n",
    "#              if row[1].has_header else\n",
    "#              (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object))\n",
    "#              for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.loadimages(xpu(zeprimes)))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdir = pjoin(os.getcwd(), 'cimaq_clean_eprime_datas3')\n",
    "os.makedirs(newdir, exist_ok = True)\n",
    "[itm[1].to_csv(pjoin(newdir, itm[0]['filename']), sep = '\\t',\n",
    "               header = [0 if itm[0]['has_header'] else None][0],\n",
    "               index = None)\n",
    " for itm in newsheets]\n",
    "\n",
    "# json.dumps(newsheets)\n",
    "# newsheets[0][0]\n",
    "# def megamerge_dicts(dict_list: list) -> dict:\n",
    "#     return reduce(\n",
    "#         lambda x, y: {**x, **y}, dict_list\n",
    "#     )\n",
    "# allinfos = megamerge_dicts([itm[0] for itm in newsheets])\n",
    "# allinfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfiles(sorted(loadimages(newdir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index'])\n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.clean_bytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     has_header = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.clean_bytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                has_header = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['as_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[row[1].as_df.shape for row in cimaq_infos.iterrows()]\n",
    "cimaq_infos.as_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([itm[1] for itm in\n",
    "           list(cimaq_infos['as_df'].iloc[0].iteritems())[[0, 1, 2, 3][-1]:]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_infos = df(tuple(scanzip(apath) for apath in\n",
    "#                        tqdm(loadimages(xpu(zeprimes)),\n",
    "#                             desc = 'sniffing')))\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.mkfrombytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     hdr = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n",
    "\n",
    "# cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                           dtype = object).T.set_index(0, drop = True).T\n",
    "#                          if row[1].has_header else\n",
    "#                          df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                            dtype = object)\n",
    "#                          for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                          desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['as_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import collections\n",
    "help(collections.abc)\n",
    "# help(typing.Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index\n",
    "\n",
    "tsheets = cimaq_infos.iloc[:3, :]\n",
    "# bsheets\n",
    "def clean_bytes(inpt, encoding, hdr, delimiter, lineterminator, dup_index, nfields):\n",
    "    newsheet = b'\\n'.join([b'\\t'.join(itm.strip(b'\\\\s') for itm in re.sub(b'\\\\s{2,}',\n",
    "                                         b'\\\\s'+delimiter+b'\\\\s',\n",
    "                                         line).split(delimiter))\n",
    "                       for line in fix_na_reps(inpt.lower(), encoding,\n",
    "                                               delimiter,\n",
    "                                               lineterminator).decode(\n",
    "                           \"utf8\", \"replace\").replace(\"�\", \"\").strip().encode(\n",
    "                           \"utf8\").splitlines()])\n",
    "    return [fix_dup_index(newsheet, encoding, hdr, delimiter, nfields)\n",
    "            if dup_index else newsheet][0]\n",
    "\n",
    "newsheets = [clean_bytes(row[1].bsheets, row[1].encoding, row[1].has_header,\n",
    "                                row[1].delimiter, row[1].lineterminator,\n",
    "                                row[1].dup_index, row[1].nfields)\n",
    "                    for row in tsheets.iterrows()]\n",
    "\n",
    "newsheets[0].splitlines()\n",
    "\n",
    "\n",
    "# udec = pd.read_csv(StringIO(unidecode(newsheet.decode())), sep = '\\t')\n",
    "# no_udec = pd.read_csv(StringIO(newsheet.decode()), sep = '\\t')\n",
    "# all(no_udec == udec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0].values == itm[1][1].values)]\n",
    "    return b'\\n'.join(b'\\t'.join(row[1].values.tolist()) for row in\n",
    "                      pd.concat((evdf[booltest],\n",
    "                       pd.Series([str(np.nan).encode()]*evdf.shape[1]),\n",
    "#                       pd.Series((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1)\n",
    "#                                 for row in evdf.iterrows()),\n",
    "                       oddf[booltest[-1]:]), axis = 1).iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # help(os.PathLike)\n",
    "\n",
    "# def fix_dup_index(\n",
    "#     inpt: Union[bytes, str, os.PathLike],\n",
    "#     encoding: str = None,\n",
    "#     hdr: bool = False,\n",
    "#     delimiter: bytes = None,\n",
    "#     nfields: int = None\n",
    "# ) -> bytes:\n",
    "#     inpt = get_bytes(inpt)\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "#     evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "#                   in evenodd(inpt.splitlines()))\n",
    "#     booltest = [itm[0] for itm in enumerate(\n",
    "#                    tuple(zip([itm[1] for itm in\n",
    "#                               evdf.iteritems()],\n",
    "#                               [itm[1] for itm in\n",
    "#                                oddf.iteritems()])))\n",
    "#                 if all(itm[1][0].values == itm[1][1].values)]\n",
    "#     datas = pd.concat((evdf[booltest],\n",
    "#                       pd.Series(bytes((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1), 'utf8')\n",
    "#                                 for row in evdf.iterrows()),\n",
    "#                       oddf[booltest[-1]:]), axis = 1)\n",
    "#     return b'\\n'.join(b'\\t'.join(itm for\n",
    "#                                  itm in row[1].values.tolist())\n",
    "#                       for row in datas.iterrows())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bytes(inpt: Union[bytes, str, os.PathLike, object]) -> bytes:\n",
    "#     \"\"\" Returns bytes from file either from memory or from reading \"\"\"\n",
    "#     if type(input) == object:\n",
    "#         return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                      itm in row[1].values.tolist())\n",
    "#                           for row in inpt.iterrows())    \n",
    "#     elif type(inpt) == bytes:\n",
    "#         return [inpt if bool(len(inpt.splitlines()) > \\\n",
    "#                   0 and inpt != None) else b\"1\"][0]\n",
    "#     elif type(inpt) == str:\n",
    "# #         try:\n",
    "#         with open(inpt, \"rb\", buffering=0) as myfile:\n",
    "#             outpt = myfile.read()\n",
    "#             if bool(len(myfile.read().splitlines()) > \\\n",
    "#                       0 and outpt != None):\n",
    "#                 return (outpt, myfile.close())[0]\n",
    "#             else:\n",
    "#                 return (b\"1\"[0], myfile.close())[0]\n",
    "# inpt = loadfiles(loadimages(xpu(zeprimes))).iloc[:, :-1]\n",
    "# tsheet = b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                itm in row[1].values.tolist())\n",
    "#                       for row in inpt.iterrows())\n",
    "# inpt.shape, tsheet.splitlines().__len__(), tsheet.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     return '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#             row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#             ['non'.encode(encoding)] + \\\n",
    "#             row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#             oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                         for row in evdf.fillna(\n",
    "#                                             'non'.encode(encoding)).iterrows())).replace(\n",
    "#             'non'.encode(encoding), str(np.nan).encode(encoding))\n",
    "\n",
    "# str(np.nan).encode(encoding)\n",
    "\n",
    "\n",
    "#     booltest = [itm[0] for itm in evdf.dropna().iteritems()\n",
    "#                 if all(itm[1].values == oddf.dropna()[itm[0]].values)]\n",
    "#     booltest = [itm[0][0] for itm in\n",
    "#                 tuple(zip(list(itm for itm in evdf.dropna().iteritems()),\n",
    "#                           list(itm for itm in oddf.dropna().iteritems())))\n",
    "#                if bool(all(itm[0][1].values) == all(itm[1][1].values))]\n",
    "#     tmp = [pd.Series(itm[0] + [str(np.nan).encode(encoding)] \\\n",
    "#                      + itm[1]).unique()\n",
    "#            if len(pd.Series(itm[0] + itm[1]).unique().tolist()) < nfields\n",
    "#            else pd.Series(itm[0] + itm[1]).unique()\n",
    "#            for itm in\n",
    "#            tuple(zip([line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[0]],\n",
    "#                     [line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[1]]))]\n",
    "#     return tmp\n",
    "#     evdf = df([line.decode().split()\n",
    "#                for line in tmp[0]])\n",
    "#     oddf = df([line.decode().split() for line in tmp[1]])\n",
    "\n",
    "#     return evdf.replace('None', np.nan)\n",
    "#     return pd.concat([evdf, df(oddf[[col[0] for col in\n",
    "#                                      enumerate(oddf.columns)\n",
    "#                                      if col[0] not in\n",
    "#                                      booltest]].iteritems())[1:]],\n",
    "#                      axis = 1)\n",
    "#pd.merge(evdf, oddf[list(oddf.columns)], on = booltest)\n",
    "# .drop(columns = booltest)\n",
    "#     booltest = [col for col in evdf.columns if\n",
    "#                 evdf[col].values.all() == oddf[col].values.all()]\n",
    "#     dup_test = [col for col in oddf.columns if evdf[col].values.all() == oddf[col].values.all()]\n",
    "\n",
    "#     return \"\\\\n\".join(\n",
    "#         [\n",
    "#             \"\\t\".join([itm for itm in line.split(=)])\n",
    "#             for line in newsheet.rename(\n",
    "#                 dict(enumerate(newsheet.columns))\n",
    "#             ).values.tolist()\n",
    "#         ]\n",
    "#     ).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(pd.Series)\n",
    "\n",
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes = cimaq_infos.loc[[row[0] for row in cimaq_infos.iterrows()\n",
    "                                   if 'onset' in row[1].filename]].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsheets = [fix_dup_index(fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter),\n",
    "                               row[1].encoding, row[1].has_header, row[1].delimiter, row[1].nfields)\n",
    "                             for row in cimaq_infos.iloc[:3, :].iterrows()]\n",
    "testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsheets = cimaq_infos.iloc[:3, :]\n",
    "# def fix_na_reps(inpt: bytes, encoding: str = None, delimiter: bytes = None) - > bytes:\n",
    "#     return '\\\\n'.encode(encoding).join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "#                       delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "#                       line) for line in inpt.splitlines())\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "\n",
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = snif.get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else snif.get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in snif.evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0] == itm[1][1])]\n",
    "    datas = pd.concat((evdf[booltest], pd.Series((int(len(row[1].values)) == nfields[0] -1)\n",
    "                                         for row in evdf.iterrows()),\n",
    "                     oddf[booltest[-1]:]), axis = 1)\n",
    "    return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "                               itm in row[1].values.tolist())\n",
    "                      for row in datas.iterrows())\n",
    "\n",
    "def fix_na_reps(inpt: bytes,\n",
    "                encoding: str = None,\n",
    "                delimiter: bytes = None,\n",
    "                lterm: bytes = None) -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    delimiter = [delimiter if delimiter else get_delimiter(inpt, encoding)][0]\n",
    "    lterm = [lterm if lterm else get_lineterminator(inpt)][0]\n",
    "    return lterm.join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "                      delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "                      line) for line in inpt.split(lterm))\n",
    "# newsheets = [fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter)\n",
    "#                              for row in testsheets.iterrows()]\n",
    "# testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "# testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = fix_dup_index(all_dup_indexes.bsheets[55],\n",
    "              all_dup_indexes.encoding[55],\n",
    "              all_dup_indexes.has_header[55],\n",
    "              all_dup_indexes.delimiter[55])\n",
    "\n",
    "fid\n",
    "#     '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#         row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#         [str(np.nan).encode(encoding)] + \\\n",
    "#         row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#         oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                  for row in evdf.iterrows()))\n",
    "#  for line in evdf.iterrows()fid[0][0].values.tolist()]\n",
    "# all(fid[0][0][3].values == fid[0][1][3].values)\n",
    "df((snif.evenodd((line.split() for line in\n",
    "                  all_dup_indexes.iloc[55].bsheets.splitlines()))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fdi[0][3] == fdi[1][3]\n",
    "# def get_nfields(inpt: bytes, hdr: bool = None) -> bytes:\n",
    "#     inpt = [get_bytes(inpt).splitlines()[1:] if hdr else\n",
    "#             get_bytes(inpt).splitlines()][0]\n",
    "#     return pd.Series(len(line.split())\n",
    "#                       for line in inpt).max()\n",
    "#             if not hdr\n",
    "#             else pd.Series(len(line.split()) for line in\n",
    "#                            inpt.splitlines()[1:]).max()][0]\n",
    "# fix_dup_index(all_dup_indexes.bsheets[55])\n",
    "\n",
    "# [chr(itm) for itm in list(all_dup_indexes.delimiter[0])].__len__()\n",
    "# fxd = fix_dup_index()\n",
    "# fxd\n",
    "# all_dup_indexes['na_reps'] = [chr(list(row[1].bsheets)[-1])\n",
    "#                               for row in all_dup_indexes.iterrows()]\n",
    "\n",
    "\n",
    "# all_dup_indexes\n",
    "# all_dup_indexes['na_reps']\n",
    "# all_dup_indexes.bsheets['na_rep'] = [row[1].bsheets]\n",
    "#              encoding: str = None,\n",
    "#              hdr: bool = False,\n",
    "#              delimiter: bytes = None)\n",
    "# chardet.detect('�'.encode())\n",
    "# '�'.encode('utf32')\n",
    "# [line.split().__len__() for line in\n",
    "#  all_dup_indexes.bsheets[55].splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets'] = ['\\\\n'.encode(row[1].encoding).join(['\\t'.encode(row[1].encoding).join(\n",
    "                                   re.sub(b'\\\\s' + b'{2,}',\n",
    "                                          b'\\\\s',\n",
    "                                          re.sub(row[1].delimiter + b'{2,}',\n",
    "                                          row[1].delimiter + \\\n",
    "                                          str(np.nan).encode(row[1].encoding) + \\\n",
    "                                          row[1].delimiter,\n",
    "                                          line)) \\\n",
    "                                for line in row[1].bsheets.splitlines())])\n",
    "    for row in cimaq_infos.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes['incr_delim'] = [Counter([snif.get_delimiter(line, 'ascii')\n",
    "                                          for line in row[1]['bsheets'].splitlines()]).most_common(1)[0][0]\n",
    "                                 for row in all_dup_indexes.iterrows()]\n",
    "all_dup_indexes['incr_delim'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['force_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "                             for row in cimaq_infos.iterrows()]\n",
    "# with open(pjoin(os.getcwd(), 'test.txt'), 'wb') as newfile:\n",
    "#     newfile.write(cimaq_infos.iloc[124]['bsheets'])\n",
    "#     newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded'] = [row[1].force_utf8.decode()\n",
    "                          for row in cimaq_infos.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index']).decode(\n",
    "#                                     'utf8', 'replace').replace(\n",
    "#                                         '�', '').encode().decode().strip()\n",
    "                            \n",
    "                            \n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n'\n",
    "                                            ))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.iloc[667]['as_df']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2 = pd.concat(val.T for val in cimaq_infos[0].values.flatten())\n",
    "cimaq_infos2 = cimaq_infos2.dropna()\n",
    "cimaq_infos2['newsheets'] = [snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode()\n",
    "                             for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                             desc = 'repairing')]\n",
    "\n",
    "cimaq_infos2['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n'))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['as_df'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[99].as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['to_csv'] = [pd.read_csv(StringIO(row[1].unidecoded),\n",
    "                                      sep = '\\t',\n",
    "                                      header = [0 if row[1].has_header\n",
    "                                                else None][0],\n",
    "                                      engine = 'c')\n",
    "#                                       quoting = 1)\n",
    "                              for row in cimaq_infos2.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[667].to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['cleaned'] = ['\\n'.join(['\\t'.join(line.split()) for line in\n",
    "                                      snif.is_printable(row[1].newsheets.decode(\n",
    "                                          row[1].encoding).encode(\n",
    "                                          'utf8', 'replace').decode()).splitlines()]).encode().decode()\n",
    "                           for row in cimaq_infos2.iterrows()]\n",
    "cimaq_infos2['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pd.read_csv(StringIO(row[1].newsheets.decode(row[1].encoding)), sep = '\\t')\n",
    " for row in cimaq_infos2.iterrows()][666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(scanzip(loadimages(xpu(zeprimes))[0]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chardet.detect(b'0xf8')#0xf\n",
    "# chr(int.from_bytes(b'0xf', sys.byteorder))\n",
    "# bytes(int.from_bytes(b'0xf8', sys.byteorder))\n",
    "# import codecs\n",
    "# help(codecs)\n",
    "[chr(item) for item in list(b'0xf')]\n",
    "[itm for itm in list]\n",
    "int.from_bytes(b'0xf', sys.byteorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniffzip(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    scan_bytes(myzip.open(row[1].))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "        return pd.concat([df(snif.evenodd(repr(itm)[8:-1].strip().replace('=', ' ').split())).T[1]\n",
    "                          for itm in\n",
    "                (ZipFile(archv_path).__dict__['filelist'][1:],\n",
    "                ZipFile(archv_path).close())[0]\n",
    "                if '__MACOSX' not in repr(itm)], axis = 1).T\n",
    "\n",
    "# atest = list(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'][0].values())\n",
    "# btest = get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['filelist'][0]\n",
    "# atest# json.dumps(repr(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'].values.tolist()).split())\n",
    "# #.replace('=', '\":').replace('\\\\', '')\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])\n",
    "df(repr(itm[1])[8:-1].replace('=', ' ').split() for itm in\n",
    "      ZipFile(loadimages(xpu(zeprimes))[0]).__dict__['NameToInfo'].items()\n",
    "      if '__MACOSX' not in itm)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GOOD ONE ######################333\n",
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    return ((df(zip(tuple(itm.strip(\"'\") for itm in repr(\n",
    "               ZipFile(archv_path).getinfo(nm))[8:-1].strip().replace(\n",
    "                   b'='.decode(), ' ').split()#.replace(chr(34), '').split())\n",
    "                   for nm in snif.filter_lst_exc(\n",
    "                       exclude, [ntpl if ntpl\n",
    "                       else getnametuple(ZipFile(archv_path))][0])))),\n",
    "            ZipFile(archv_path).close())[0])\n",
    "\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])[0]\n",
    "#.replace('=', '\":').replace('\\\\', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_archv(apath: Union[str, os.PathLike], ntpl: Union[str, list, tuple] = []) -> object:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_contents(\n",
    "    archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = [],\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    withbytes: bool = False,\n",
    "    to_sniff: bool = False,\n",
    "    to_close: bool = True,\n",
    ") -> object:\n",
    "    myzip = ZipFile(archv_path)\n",
    "    ntpl = snif.filter_list_exc(exclude,\n",
    "                                [ntpl if ntpl else snif.getnametuple(myzip)][0])\n",
    "\n",
    "    vals = (\n",
    "        df(\n",
    "            tuple(\n",
    "                dict(zip(evenodd(itm)[0], evenodd(itm)[1]))\n",
    "                for itm in tuple(\n",
    "                    tuple(\n",
    "                        force_ascii(repr(itm.lower()))\n",
    "                        .strip()\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"=\", \" \")[:-2]\n",
    "                        .split()\n",
    "                    )[1:]\n",
    "                    for itm in set(\n",
    "                        repr(myzip.getinfo(itm))\n",
    "                        .strip(\" \")\n",
    "                        .replace(itm, itm.replace(\" \", \"_\"))\n",
    "                        if \" \" in itm\n",
    "                        else repr(myzip.getinfo(itm)).strip(\" \")\n",
    "                        for itm in ntpl\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            dtype=\"object\",\n",
    "        )\n",
    "        .sort_values(\"filename\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    vals[[\"src_name\", \"ext\"]] = [(nm, os.path.splitext(nm)[1]) for nm in ntpl]\n",
    "    vals[\"filename\"] = [\n",
    "        \"_\".join(\n",
    "            pd.Series(\n",
    "                row[1].filename.lower().replace(\"/\",\n",
    "                                                \"_\").replace(\"-\",\n",
    "                                                             \"_\").split(\"_\")\n",
    "            ).unique()\n",
    "            .__iter__()\n",
    "        )\n",
    "        for row in vals.iterrows()\n",
    "    ]\n",
    "    if exclude:\n",
    "        vals = vals.drop(\n",
    "            [\n",
    "                row[0]\n",
    "                for row in vals.iterrows()\n",
    "                if row[1].filename\n",
    "                not in filter_lst_exc(exclude, [itm.lower() for itm in vals.filename])\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    if withbytes:\n",
    "        vals[\"bsheets\"] = [\n",
    "#             snif.strip_null() \n",
    "            myzip.open(row[1].src_name).read().lower() for row in vals.iterrows()\n",
    "        ]\n",
    "        \n",
    "    if to_sniff:\n",
    "        vals[[\"encoding\", \"delimiter\", \"has_header\", \"width\", \"dup_index\", \"nrows\"]] = \\\n",
    "            [tuple(snif.scan_bytes(row[1].bsheets).values()) for row in vals.iterrows()]\n",
    "    if to_close:\n",
    "        myzip.close()\n",
    "        return vals\n",
    "    else:\n",
    "        return (myzip, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azpath = loadfiles(loadimages(xpu(zeprimes))).fpaths[0]\n",
    "# with zipfile.Zipfile(azpath) as myzip:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  dict((bname(apath.replace('-', '_').strip()),\n",
    "                    get_zip_contents(apath, withbytes = True,\n",
    "                         to_sniff = True, to_close = True))\n",
    "                    for apath in tqdm(loadimages(zeprimes),\n",
    "                          desc = 'scan sniff'))\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sniffbytes import no_ascii\n",
    "# # chardet.detect(no_ascii('bon matin tin�tin!').encode())\n",
    "# # Union[str, bytes]\n",
    "# ''.join([chr(int.from_bytes(int.from_bytes(itm.encode(), sys.byteorder)))#.encode(encoding)\n",
    "#  for itm in ])\n",
    "# # ==  dict(json.dumps(set(string.printable)))\n",
    "set([chr(int.from_bytes(itm, sys.byteorder)).encode('utf32')\n",
    " for itm in [itm.encode() for itm in list(string.printable)]])\n",
    "[int.from_bytes(itm.encode(), sys.byteorder) for itm in list(string.printable)[-2:]]\n",
    "# '\\x00'.encode('ISO-8859-1')\n",
    "encodings = ['Windows-1252', 'utf32', 'utf8', 'ISO-8859-1']\n",
    "[int.from_bytes(itm.encode(encoding), sys.byteorder) for itm in list(string.printable)]\n",
    "int.from_bytes(b'', sys.byteorder)\n",
    "chardet.detect(b'\\x00'), chardet.detect(b'None')\n",
    "is_printable = [itm.encode('utf32') for itm in list(string.printable)], '\\x00'.encode('ascii')\n",
    "\n",
    "def force_encoding(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "    \"\"\"\n",
    "    notnull_printable = set(itm.encode(encoding) for itm in list(string.printable)\n",
    "                            if int.from_bytes(itm, sys.byteorder) != 0)\n",
    "#                             if chr(int.from_bytes(itm, sys.byteorder)) != \"\\x00\")\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    return \"\".encode(encoding).join(filter(lambda x: x in notnull_printable, inpt))\n",
    "\n",
    "\n",
    "# is_printable = , '\\x00'.encode('ascii')\n",
    "# '0xff'.encode('utf32') in is_printable\n",
    "utf32p = ''.encode('utf32').join(set(itm.encode('utf32') for itm in list(string.printable)))\n",
    "windows1252p = ''.encode('Windows-1252').join(set(itm.encode('Windows-1252') for itm in list(string.printable)))\n",
    "utf8p = list(itm.encode('utf8') for itm in list(string.printable))\n",
    "utf32p = list(itm.encode('utf32') for itm in list(string.printable))\n",
    "test = set(itm.encode('utf32') for itm in list(string.printable)\n",
    "           if int.from_bytes(itm.encode('utf32'),\n",
    "                                 'little') != '\\x00'.encode('utf32'))\n",
    "# test == utf32set\n",
    "# force_encoding(utf8p, 'Windows-1252')\n",
    "# chr(0)\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in utf32set]\n",
    "\n",
    "astring = \"    Salut\\tbébé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\"\n",
    "chardet.detect((\"Salut bébé, mon nom c'est François!\").encode())\n",
    "# weird = list(chr(itm) for itm in tuple(''.join(utf32set).encode()))[-5]\n",
    "# astring.encode('ascii')\n",
    "\n",
    "# clean_astring = ''.join(itm for itm in list(astring) if\n",
    "#                         int.from_bytes(itm.encode('utf32'), sys.byteorder) != 0)\n",
    "\n",
    "def clean_bytes(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    return ''.join(itm for itm in list(inpt) if\n",
    "                        chr(int.from_bytes(itm)) != chr(0).encode(encoding) and itm in\n",
    "                       set(itm.encode(encoding) for itm in list(string.printable)))\n",
    "# chr(int.from_bytes(weird.encode('ascii'), sys.byteorder))\n",
    "# list(astring)\n",
    "# int.from_bytes('\\\\x00'.encode(), 'big')\n",
    "# int.from_bytes('\\\\x00'.encode(), sys.byteorder) == int.from_bytes('\\\\x00'.encode(), 'little')\n",
    "maybe_null == notnull_printable\n",
    "(astring, clean_astring)\n",
    "# '\\x00'.encode('utf32')\n",
    "nareps = [chr(0).encode('ascii'), chr(0).encode('utf32'), chr(0).encode('ISO-8859-1')]\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in nareps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astring = \"    \\\\\\tSalut\\tbé bé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\\S\"\n",
    "bstring = astring.encode('utf16')\n",
    "narep = chr(0).encode(snif.get_bencod(bstring))\n",
    "chr(0).encode('utf16') == chr(0).encode(snif.get_bencod(bstring))\n",
    "[chr(itm) for itm in list(bstring)]# chardet.detect('ç'.encode())\n",
    "# [chr(itm) for itm in list(bstring) if chr(itm) != ]\n",
    "narep, bstring, bstring.replace(narep, ''.encode(snif.get_bencod(bstring)))\n",
    "tuple(zip(list(astring), list(bstring)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(int.from_bytes(''.encode(), sys.byteorder)).encode('utf32'), chr(int.from_bytes('0'.encode(), sys.byteorder)).encode('utf32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import bytes\n",
    "# # import format as fmt\n",
    "\n",
    "# def strip_null(inpt: bytes, nullrep: bytes = None, encoding: str = None):\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     nullrep = [nullrep if nullrep else snif.get_nullrep(inpt, encoding)][0]\n",
    "#     return {\n",
    "#         format(print(\"[%q]\",\n",
    "#                    bytes.Trim(byte(\" !!! Achtung! Achtung! !!! \"), \"! \")))}\n",
    "\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "#         ''.join(get_nullrep(inpt, encoding)), '').encode()\n",
    "    try:\n",
    "        return inpt.replace(chr(int.from_bytes(\n",
    "                   b\"\\x00\", sys.byteorder)).encode(encoding), ''.encode(encoding))\n",
    "    except UnidecodeError:\n",
    "        return inpt.replace(get_nullrep(inpt, encoding), ''.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbytes = b'bonjour\\xff\\xfe\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "chardet.detect(snif.strip_null(testbytes))#.encode('ISO-8859-1')\n",
    "chardet.detect(testbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nullrep(inpt, encoding: str = None) -> bytes:\n",
    "#     return [itm for itm in list(inpt) if\n",
    "#             chr(int.from_bytes(itm, sys.byteorder)).encode(encoding) == \"\\x00\".encode(encoding)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "help(codecs.lookup('utf8').streamreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest = [dict((subject[0], \n",
    "                df.from_dict(dict((itm for itm in\n",
    "                                   list(snif.scan_bytes(snif.force_utf8(\n",
    "                  row[1].bsheets, row[1].encoding)).items()) + \\\n",
    "                      [('filename', row[1].filename),\n",
    "                       (('pscid', 'dccid'), row[1].filename.split('_')[:2])])),\n",
    "                             orient = 'index'))\n",
    "               for row in tqdm(subject[1].iterrows(), desc = 'new sniff'))\n",
    "           for subject in list(test.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtest\n",
    "# # both = {**test, **newtest}\n",
    "# both = tuple({**itm[0], **itm[1]} for itm\n",
    "#              in )\n",
    "# both\n",
    "# both = tuple(zip([itm.values() for itm in test], [itm.values() for itm in newtest]))\n",
    "# both\n",
    "tuple(zip(test.values(), newtest.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_xtrct(archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = None,\n",
    "    to_xtrct: Union[str, list, tuple] = 'all',\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    to_close: bool = True,\n",
    "    withbytes: bool = False,\n",
    "    dst_path: Union[os.PathLike, str] = None) -> object:\n",
    "    dst_path = [dst_path if dst_path\n",
    "                else pjoin(os.get_cwd(),\n",
    "                           os.path.splitext(bname(archv_path))[0])][0]\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "    myzip = zipfile.ZipFile(archv_path)\n",
    "    ntpl = filter_lst_exc([ntpl if ntpl else getnametuple(myzip)][0])\n",
    "    contents = get_zip_contents(archv_path, ntpl, excllude, to_xtrct,\n",
    "                                to_close = True, withbytes = True, to_sniff = True)\n",
    "    xtrct_lst = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename\n",
    "            in filter_lst_inc(to_xtrct, list(vals.filename), sort=True)\n",
    "        ]\n",
    "    ]\n",
    "    [\n",
    "        shutil.move(\n",
    "            myzip.extract(member=row[1].src_name, path=dst_path),\n",
    "            pjoin(\n",
    "                dst_path,\n",
    "                \"_\".join(\n",
    "                    pd.Series(\n",
    "                        row[1].filename.lower().replace(\"-\", \"_\").split(\"_\")\n",
    "                    ).unique()\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        for row in tqdm(xtrct_lst.iterrows(), desc=\"extracting\")\n",
    "    ]\n",
    "    vals = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename not in xtrct_lst.values\n",
    "        ]\n",
    "    ]\n",
    "    removeEmptyFolders(dst_path, False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_printables = [chr(int.from_bytes(itm.encode(), sys.byteorder)).encode() for itm in string.printable]\n",
    "str(b_printables[-1]) == repr(b_printables[-1])\n",
    "\n",
    "def force_to(inpt, src_enc: str = None, dst_enc: str = 'utf8'):\n",
    "    inpt = get_bytes(inpt)\n",
    "    as_ints = [[line.split()]]\n",
    "    \n",
    "    \n",
    "chr(list(asheet)[0])\n",
    "list(asheet).__len__()\n",
    "# chr(int.from_bytes(b_printables[-1], sys.byteorder))\n",
    "\n",
    "# [chr(itm) for itm in list(asheet)]\n",
    "# help(chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_topdir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/'\n",
    "# big = loadfiles([apath for apath in loadimages(xpu(cimaq_topdir))\n",
    "#                  if os.path.isfile(apath)])\n",
    "# big[['dccid', 'pscid']] = [([re.compile('\\d{6}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{6}').search(row[1].fname) != None\n",
    "#                              else None][0],\n",
    "#                             [re.compile('\\d{7}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{7}').search(row[1].fname) != None\n",
    "#                              else None][0])\n",
    "#                            for row in big.iterrows()]\n",
    "'\\\\x0'.encode('utf16')\n",
    "chr(int.from_bytes(b\"\\0xff\", sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(asheet)\n",
    "\n",
    "# test2 = pd.concat([itm[1] for itm in tqdm(sorted(list(test)), desc = 'concatenate')])\n",
    "\n",
    "# test2['forced_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "#                         for row in test2.iterrows()]\n",
    "\n",
    "#.filename,\n",
    "#                    snif.scan_bytes(row[1].forced_utf8, encoding='utf'))\n",
    "#                         for row in itm[1].iterrows())\n",
    "#              for itm in tqdm(list(test.items()),  desc = 'scan & sniff utf8')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert newtest.loc['encoding'].all() == 'ascii' or 'UTF-8'\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder))\n",
    "\n",
    "b'n\\a' == 'n\\a'.encode()\n",
    "\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder)).encode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_item(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Returns null byte representation as bytes in native file encoding'''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "#     rep = chr(list(inpt)[-1]).encode(encoding)\n",
    "    last1 = inpt.splitlines()[-1].split()[-1]\n",
    "    last2 = [chr(itm).encode(encoding) for itm in\n",
    "             list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]\n",
    "    return (last1, last2)\n",
    "#     return [chr(itm) for itm in list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(chr(int.from_bytes(b'\\x00', sys.byteorder)))\n",
    "# snif.get_bencod(chr(0))\n",
    "int.from_bytes(b'\\x00', sys.byteorder)\n",
    "aguy = random.sample(list(test.values()), 1)\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aguy = list(test.values())[0]\n",
    "aguy['size_check'] = [int(row[1].file_size) == len(list(row[1].bsheets))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy['na_reps'] = [snif.get_nullrep(row[1].bsheets) for row in aguy.iterrows()]\n",
    "aguy['last_item'] = [last_item(row[1].bsheets, row[1].encoding) for row in aguy.iterrows()]\n",
    "\n",
    "aguy[['n_zbytes', 'n_scanbytes', 'chkup']] = [(int(len(list(row[1].bsheets))), int(row[1].file_size),\n",
    "                                      (int(len(list(row[1].bsheets))) == int(row[1].file_size)))\n",
    "                                     for row in aguy.iterrows()]\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aguy['utf8len'] = [len(row[1].forced_utf8.splitlines()) for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_tophalf'] = [row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))] == \\\n",
    "                                 row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]\n",
    "                              for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_bothalf'] = [[line.strip() for line in\n",
    "                               row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))]] == \\\n",
    "                                 [line.strip() for line in\n",
    "                                  row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]]\n",
    "                              for row in aguy.iterrows()]\n",
    "\n",
    "aguy['nrows_test'] = [(len(row[1].bsheets.splitlines()) == len(row[1].forced_utf8.splitlines()))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy[['missing_line_index', 'missing_line_values']] = [(row[1].nrows_test)*2 if row[1].nrows_test else\n",
    "                        sorted([(line[0], line[1]) for line in enumerate(row[1].bsheets.splitlines()) if\n",
    "                                snif.force_utf8(line[1], 'utf8') not in row[1].forced_utf8.splitlines()])\n",
    "                       for row in aguy.iterrows()]\n",
    "# aguy['eq_lines'] = [len([line[0] for line in enumerate(tuple(zip(row[1].bsheets.splitlines(),\n",
    "#                       row[1].forced_utf8.splitlines()))) if line[1][0] == line[1][1]])\n",
    "#                     == len\n",
    "#                     for row in aguy.iterrows()]\n",
    "\n",
    "aguy.last_item.iloc[6][1][0].decode('utf16').encode('utf8').decode('utf8')\n",
    "aguy.forced_utf8, aguy.bsheets\n",
    "aguy['missing_line'].iloc[5]\n",
    "# aguy.iloc[5].bsheets.splitlines(), aguy.iloc[5].forced_utf8.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asheet = aguy.iloc[4].bsheets[0]\n",
    "utfsheet = aguy.iloc[4].forced_utf8[0]\n",
    "asheet == utfsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asheet = aguy.sample(1).bsheets.values[0]\n",
    "# last_item(asheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = snif.get_bencod(abyte)\n",
    "nonull = strip_null(abyte, enc)\n",
    "nonull.splitllines()[:4], abyte.splitlines()[:4]\n",
    "# aguy['no_null'] = [strip_null(row[1].bsheets,\n",
    "#                               row[1].encoding)#.encode(row[1].encoding)\n",
    "#                    for row in aguy.iterrows()]\n",
    "# aguy['bprints'] = [snif.bytes_printable(row[1].bsheets).encode(snif.get_bencod(row[1].bsheets))\n",
    "#                  for row in aguy.iterrows()]\n",
    "# checkfx = []\n",
    "\n",
    "\n",
    "# list(abyte.splitlines()[-1])\n",
    "# [chr(itm).encode() for itm in\n",
    "#  [list(row[1].bsheets.splitlines()[-1])[-1]\n",
    "# for row in aguy.iterrows()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = list(test.values())[0].iloc[0].bsheets\n",
    "encoding = list(test.values())[0].iloc[0].encoding\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    return '\\n'.join(inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "        ''.join(snif.get_nullrep(inpt, encoding)), '').splitlines())\n",
    "\n",
    "pd.read_csv(StringIO(snif.mkfrombytes(inpt.strip()).decode()), sep = '\\t')\n",
    "#     narep =''.encode(encoding).join([itm.encode(encoding) for itm in\n",
    "#                                            snif.get_nullrep(inpt, encoding)])\n",
    "#     return inpt.replace(narep, '|'.encode(encoding))\n",
    "#         inpt = inpt.replace(narep.encode(encoding),\n",
    "#                             '|'.encode(encoding)).replace('|'.encode(encoding),\n",
    "#                                                           ''.encode(encoding))\n",
    "#         return inpt\n",
    "#     return snif.bytes_printable(inpt.replace(,\n",
    "#                                              repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "# snif.mkfrombytes(strip_null(inpt, 'utf8').encode())\n",
    "# pd.read_csv(StringIO(snif.mkfrombytes(inpt).decode(encoding)),\n",
    "#             sep='\\t', header = [0 if snif.get_has_header(strip_null(inpt).encode(encoding)) else None][0])\n",
    "# inpt.decode(snif.get_bencod(strip_null(inpt)), 'ignore').replace('�', '')\n",
    "# pd.read_csv(StringIO(), sep='\\t')\n",
    "# [[list(itm for itm in line.split() if itm not in nareps)]\n",
    "#  for line in snif.bytes_printable(inpt)]\n",
    "# def get_na_reps(inpt: bytes, encoding: str = None) -> Union[list, int, bytes]:\n",
    "#     inpt = snif.bytes_printable(inpt)\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)]\n",
    "#     return [chr(itm).encode(encoding) for itm\n",
    "#             in [chr(nulb) for nulb in\n",
    "#                 list(snif.get_nullrep(snif.bytes_printable(inpt)))][0]]\n",
    "\n",
    "# get_na_reps(inpt)\n",
    "# # [list(''.encode(encoding).join((line.split())))\n",
    "# #  for line in inpt.splitlines()]\n",
    "# chr(int.from_bytes('0xf8', sys.byteorder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 3+5 ==9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_replace import multiple_replace\n",
    "\n",
    "def clear_nullbytes(inpt: bytes, encoding: str = None):\n",
    "    inpt = snif.bytes_printable(inpt)\n",
    "    encoding = snif.get_bencod(inpt)\n",
    "    toclear = dict((itm.encode(encoding), '|'.encode(encoding))\n",
    "                   for itm in\n",
    "                   )\n",
    "\n",
    "\n",
    "    for narep in nareps:\n",
    "        inpt = inpt.replace(narep, '|'.encode(encoding))\n",
    "    return inpt\n",
    "#     return inpt.replace('|'.encode(encoding), 'nan'.encode(encoding))\n",
    "# multiple_replace(toclear, snif.bytes_printable(abyte), encoding)\n",
    "# help(str.replace)\n",
    "# b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "# b_printable\n",
    "\n",
    "inpt = list(test.values())[0].iloc[8].bsheets\n",
    "encoding = list(test.values())[0].iloc[8].encoding\n",
    "# new = snif.bytes_printable(abyte).replace(tostrip, ''.encode(encoding))\n",
    "clear_nullbytes(inpt, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def bytes_prntble(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Same as is_printable, but for bytes in native file encoding '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "\n",
    "#     b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "#     return ''.encode(encoding).join([str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding) for ch in \n",
    "#                                     list(inpt) if str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding)\n",
    "#                                     in b_printable])\n",
    "    \n",
    "# def get_nullrep(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Returns null byte representation as bytes in native file encoding'''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "\n",
    "# def strip_null(inpt: bytes, encoding: str = None, replace_val: str = None) -> bytes:\n",
    "#     ''' Remove null bytes from byte stream with proper representation\n",
    "#         Adapted from:\n",
    "#         https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "#         All files end by a null byte, so the last byte in a file shows\n",
    "#         how null bytes are represented within this file '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     repval = ['' if not replace_val else replace_val][0].encode(encoding)\n",
    "#     return bytes_prntble(inpt.replace(get_nullrep(inpt, encoding), repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "\n",
    "# snif.get_nullrep(abyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abyte = list(test.values())[0].iloc[1].bsheets\n",
    "enc = snif.get_bencod(abyte)\n",
    "# (abyte.decode(enc, 'replace').splitlines()[0], repr(abyte.decode(enc, 'replace').splitlines()[0]),\n",
    "#  repr(snif.force_utf8(abyte).splitlines()[0].decode())\n",
    "# )\n",
    "repr(abyte.decode(enc, 'replace'))\n",
    "def to_utf8(astring):\n",
    "    return unidecode(repr(astring.decode(enc, 'replace').replace('�', '').encode(\n",
    "              'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', ''))).encode()\n",
    "# len(to_utf8(abyte).splitlines()\n",
    "# )\n",
    "len(abyte.splitlines())\n",
    "new = b'\\n'.join([b'\\t'.join([to_utf8(itm) for itm in to_utf8(line).split()])\n",
    "            for line in abyte.splitlines()]).decode()\n",
    "\n",
    "df([' '.join(line.split(\"\\\\x00\")).split() for line in new.splitlines()])\n",
    "# clean_utf8 = '\\n'.join([['\\t'.join([to_utf8(itm) for itm in line.split()])]\n",
    "#               for line in abyte.splitlines()])\n",
    "# clean_utf8\n",
    "# newsheet = repr(abyte.replace(bytes(str(chr(0)), enc),\n",
    "#               str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x0'\", enc), str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x'\", enc), ''.encode(enc)).replace(\n",
    "#                       bytes(str(np.nan), enc), str(\"\").encode(enc)).decode(\n",
    "#                           'ascii', 'replace').replace('�', '').replace('ÿ', ''))\n",
    "\n",
    "# cleaned = '\\n'.join(['\\t'.join([itm.replace('\\\\x0', ' ').strip().replace('\\\\x', ' ').strip() for itm in line.split()])\n",
    "#            for line in repr(newsheet).replace('\\\\x00', ' ').splitlines()]).strip()[1:].encode().decode()\n",
    "# pd.read_csv(StringIO(unidecode(cleaned)), sep='\\t')\n",
    "# [line.replace('\\\\x00', str(np.nan)).replace('\\\\x0', str(np.nan)) for line in newsheet.splitlines() if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abyte = list(test.values())[0].iloc[2].bsheets\n",
    "# encod = list(test.values())[0].iloc[2].encoding\n",
    "\n",
    "\n",
    "    \n",
    "enc = snif.get_bencod(snif.strip_null(abyte))\n",
    "# snif.strip_null(abyte)\n",
    "# [strip_null(line) for line in strip_null(bytes_prntble(abyte)).splitlines(keepends = True)]\n",
    "\n",
    "# get_nullrep(abyte)+b'1'\n",
    "# # int.from_bytes(b'\\\\x0', 'little'), int.from_bytes(b'\\\\x0', sys.byteorder), int.from_bytes(b'\\\\x0', 'big')\n",
    "\n",
    "# # newsheet = '\\n'.join(['\\t'.join([str(itm).replace(\"'\", \"\").replace(\"'\", '\"') for itm in\n",
    "# #                                  repr(line).replace('ÿ', '\\s').replace('\\\\x0', '\\s').replace(\n",
    "# #                                      '\\\\x', '\\s').replace('\\s', '').split()])\n",
    "# #                       for line in list(test.values())[0].iloc[5].bsheets.decode(\n",
    "# #                encod, 'replace').splitlines()]).splitlines()\n",
    "# # newsheet\n",
    "# # strtest = abyte.splitlines()[0]\n",
    "\n",
    "# repr(abyte.replace(repr(chr(0)).encode(encod), ''.encode(encod)).decode(encod, 'replace').replace('�', '')).encode(\n",
    "#     'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', '').encode()\n",
    "# abyte.decode('ISO-8859-1').encode('ascii', 'ignore').decode('utf8', 'replace').replace('�', '')\n",
    "\n",
    "# unidecode(repr(abyte.decode(encod, 'replace').replace('�', '')))\n",
    "# is_printable(abyte.decode(encod, 'replace').replace('�', '').encode().replace(chr(0).encode(), ''.encode()).decode())\n",
    "\n",
    "# # new = [[unidecode(itm).encode('ascii', 'ignore').decode('ascii', 'replace').replace('�', '')\n",
    "# #         for itm in line.replace(repr(chr(0)),\n",
    "# #                                '').split()\n",
    "# #       if unidecode(is_printable(itm)) != '']\n",
    "# #      for line in abyte.decode(encod, 'replace').replace('�', '').splitlines()\n",
    "# #      if line != []]\n",
    "# # new[0], ''.join(new[0]).replace(repr(chr(0)).encode(encod), ' '.encode(encod))\n",
    "# new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in string.printable #list(string.printable)\n",
    "\n",
    "chr(int.from_bytes(b'z', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(int.from_bytes(b'\\\\x00', 'big'))\n",
    "[chr(itm).encode() for itm in list(string.printable)]\n",
    "# chr(int.from_bytes('\\\\x0'.encode('Windows-1252'), sys.byteorder))\n",
    "# chr(int.from_bytes(b'\\\\x00', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# abyte.decode('utf8', 'replace').replace('�', '').encode().decode()\n",
    "# unidecode(abyte.decode(snif.get_bencod(abyte), 'replace').replace('�', '')).encode()\n",
    "snif.force_utf8(abyte, enc).splitlines()\n",
    "clean1 = re.sub('\\\\x00', '', abyte.decode('ascii', 'replace').replace('�', '').encode('utf16').decode('utf8', 'replace'))\n",
    "# unidecode(clean1.encode().decode()).encode('utf16', 'replace')\n",
    "clean1\n",
    "[re.sub(bytes(str(np.nan), enc), bytes('', enc),\n",
    "        re.sub(bytes(\"\\x00\", enc),\n",
    "               bytes(str(np.nan), enc),\n",
    "               line)).decode('utf8', 'replace').replace('�', '').replace('\\\\', '')#unidecode(line.decode(enc, 'replace'))).encode('ascii', 'replace').decode('utf8', 'replace').replace('�', '')\n",
    "  for line in abyte.splitlines()][0].split()[0]\n",
    "# snif.force_utf8(abyte.decode(enc, 'replace'))\n",
    "#.decode(enc, 'replace')]\n",
    "# re.sub('\\x0', '', clean1)\n",
    "# unidecode(\n",
    "#           .replace('\\x00', '').replace(\n",
    "#     '\\\\x0', '').replace('\\\\x', ''))\n",
    "#.replace(b\"\\x\".decode(), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(0)\n",
    "# str(\"\\xff\")\n",
    "chr(1).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes('n\\a', 'Windows-1252')\n",
    "unidecode(b'\\x0b\\x0c'.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_tsvs(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths'])`.set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                        [loadfiles(loadimages(row[1].fpaths)).dropna(axis = 0).T\n",
    "#                         for row in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[2: 4].iterrows()]).T\n",
    "#     test = dict((grp, dict((sgrp, cimaq.groupby('subids').get_group(grp).groupby(\n",
    "#                'fname').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "#                    'subids').get_group(grp).groupby('fname').groups))\n",
    "#                 for grp in allscans.groupby('subids').groups)\n",
    "# #     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "# #                       [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "# #                 columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "# #                                   for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "# #                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "# #     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'stereonl', 4: 'behavioral',\n",
    "# #                                     5: 'confounds', 6: 'events', 7:'func'})\n",
    "#     return test\n",
    "# cimaq = load_tsvs(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq = fetch_cimaq(xpu(cimaq_dir))\n",
    "#cimaq.sort_values('dccid').set_index('dccid', drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_cimaq(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths']).set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + [loadfiles(loadimages(cimaqpath)).dropna(axis = 0)['fpaths']\n",
    "#                                   for cimaqpath in get_cimaq_dir_paths(cimaq_dir)[0].loc['fpaths'][2: 5]],\n",
    "#                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "#     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'behavioral', 4: 'confounds',\n",
    "#                                     5: 'events'})\n",
    "#     return cimaq.set_index('dccid').sort_index()#.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scans(cimaq_dir):\n",
    "    cimaq = fetch_cimaq(cimaq_dir)\n",
    "\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                   [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "#             columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "#                               for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "#                   axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "    \n",
    "    allscans = pd.concat([loadfiles(loadimages(pjoin(dname(xpu(cimaq_dir)),\n",
    "                                                     sub))).dropna(axis = 0).T\n",
    "                          for sub in [itm for itm in ls(dname(xpu(cimaq_dir)))\n",
    "                        if itm.startswith('sub-')]], axis = 1).T\n",
    "    allscans[['dccid', 'modality', 'general']] = [(bname(row[1].fpaths).split('_')[0].split('-')[1],\n",
    "                                                   bname(row[1].fpaths).split('_')[-1],\n",
    "                                                   bname(dname(row[1].fpaths)))\n",
    "                         for row in allscans.iterrows()]\n",
    "\n",
    "    test = dict((grp, dict((sgrp, allscans.groupby('dccid').get_group(grp).groupby(\n",
    "               'general').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "                   'dccid').get_group(grp).groupby('general').groups))\n",
    "                for grp in allscans.groupby('dccid').groups)\n",
    "    allscans = df.from_dict(test, orient = 'index').sort_index()\n",
    "    indexes = set.intersection(set(cimaq.index), set(allscans.index)) \n",
    "    return pd.concat([allscans.loc[indexes], cimaq.loc[indexes]], axis = 0)\n",
    "test = load_scans(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.iloc[0].fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [pd.concat([row[1][['fname']], df.from_dict(json_read(row[1].fpaths, 'r'), orient = 'index')])\n",
    "#           for row in test[0].iterrows()\n",
    "#           if row[1].ext == '.json']\n",
    "# # allparams = pd.concat(((itm.groupby('fname').get_group(grp)\n",
    "# #                  for grp in itm.groupby('fname').groups)\n",
    "# #              for itm in allparams), axis = 1)\n",
    "# # pd.concat(params, axis =1).T.sort_values('fname')\n",
    "# pd.concat(params, axis = 1).T.set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "    \"\"\"\n",
    "    Context manager to set the timestamp of the path to plus or\n",
    "    minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "    if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "    \"\"\"\n",
    "    stats = os.stat(path)\n",
    "    if set_new:\n",
    "        new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "    else:\n",
    "        new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
