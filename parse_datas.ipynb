{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import byte\n",
    "import chardet\n",
    "import csv\n",
    "import json\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex as re\n",
    "import string\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from chardet import UniversalDetector as udet\n",
    "from collections import Counter\n",
    "from io import StringIO\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from string import printable\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "\n",
    "from cimaq_utils import get_cimaq_dir_paths\n",
    "from sniffbytes import flatten\n",
    "from sniffbytes import loadfiles\n",
    "from sniffbytes import loadimages\n",
    "from cimaq_utils import repair_enc_task\n",
    "from cimaq_utils import fetch_cimaq\n",
    "\n",
    "from json_read import json_read\n",
    "\n",
    "import sniffbytes as snif\n",
    "\n",
    "\n",
    "from get_zip_contents import scanzip\n",
    "from get_zip_contents import getnametuple\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "from multiple_replace import multiple_replace\n",
    "\n",
    "cimaq_dir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/CIMAQ_fmri_memory/data'\n",
    "zeprimes = pjoin(cimaq_dir, 'task_files/zipped_eprime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function pack in module _struct:\n",
      "\n",
      "pack(...)\n",
      "    pack(format, v1, v2, ...) -> bytes\n",
      "    \n",
      "    Return a bytes object containing the values v1, v2, ... packed according\n",
      "    to the format string.  See help(struct) for more on format strings.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dir(True)\n",
    "bool(2+2==5).to_bytes(True.bit_length(), sys.byteorder)\n",
    "str(np.nan).encode()\n",
    "[str(np.nan).encode()]*4\n",
    "pd.Series([str(np.nan).encode()]*3)\n",
    "\n",
    "bytes(str(np.nan), 'utf8')\n",
    "b'True'.decode()\n",
    "# UnidecodeError\n",
    "# bytearray(struct.pack(\"f\", str(np.nan).encode()))  \n",
    "help(struct.pack)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chardet\n",
    "# import csv\n",
    "# import json\n",
    "# import nilearn\n",
    "# import numpy as np\n",
    "# import os\n",
    "# import pandas as pd\n",
    "# import random\n",
    "# import regex as re\n",
    "# import string\n",
    "# import struct\n",
    "# import sys\n",
    "\n",
    "# from chardet import UniversalDetector as udet\n",
    "# from io import StringIO\n",
    "# from os.path import basename as bname\n",
    "# from os.path import dirname as dname\n",
    "# from os.path import expanduser as xpu\n",
    "# from os import listdir as ls\n",
    "# from os.path import join as pjoin\n",
    "# from pandas import DataFrame as df\n",
    "# from string import printable\n",
    "# from typing import Union\n",
    "# from zipfile import ZipFile\n",
    "# from tqdm import tqdm\n",
    "# from unidecode import unidecode\n",
    "\n",
    "# import sniffbytes as snif\n",
    "\n",
    "# def scanzip(archv_path: Union[os.PathLike, str],\n",
    "#                ntpl: Union[str, list, tuple] = [],\n",
    "#                exclude: Union[str, list, tuple] = []) -> object:\n",
    "\n",
    "\n",
    "#     myzip = ZipFile(archv_path)\n",
    "#     ntpl = [ntpl if ntpl else getnametuple(myzip)][0]\n",
    "\n",
    "#     vals = df(\n",
    "#             tuple(\n",
    "#                 dict(zip(snif.evenodd(itm)[0], snif.evenodd(itm)[1]))\n",
    "#                 for itm in tuple(\n",
    "#                     tuple(\n",
    "#                         snif.is_printable(repr(itm.lower()))\n",
    "#                         .strip()\n",
    "#                         .replace(\"'\", \"\")\n",
    "#                         .replace(\"'\", \"\")\n",
    "#                         .replace(\"=\", \" \")[:-2]\n",
    "#                         .split()\n",
    "#                     )[1:]\n",
    "#                     for itm in tqdm(set(\n",
    "#                         repr(myzip.getinfo(itm))\n",
    "#                         .strip(\" \")\n",
    "#                         .replace(itm, itm.replace(\" \", \"_\"))\n",
    "#                         if \" \" in itm\n",
    "#                         else repr(myzip.getinfo(itm)).strip(\" \")\n",
    "#                         for itm in ntpl\n",
    "#                     ), desc = 'scanning archive')\n",
    "#                 )\n",
    "#             ),\n",
    "#             dtype=\"object\",\n",
    "#         ).sort_values(\"filename\").reset_index(drop=True)\n",
    "\n",
    "#     vals['src_names'] = sorted(ntpl)\n",
    "#     vals['bsheets'] = [myzip.open(row[1].src_names).read()\n",
    "#                        for row in vals.iterrows()]\n",
    "#     myzip.close()\n",
    "#     sniffed = df(snif.scan_bytes(row[1].bsheets)\n",
    "#                for row in vals.iterrows()).to_dict()\n",
    "#     return df.from_dict({**vals.to_dict(), **sniffed}, orient = 'index')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sniffing:   0%|          | 0/103 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 4847.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6162.66it/s]\n",
      "sniffing:   2%|▏         | 2/103 [00:00<00:06, 16.29it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6024.57it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5227.20it/s]\n",
      "sniffing:   4%|▍         | 4/103 [00:00<00:06, 15.37it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4425.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5360.82it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5452.81it/s]\n",
      "sniffing:   7%|▋         | 7/103 [00:00<00:05, 16.08it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5395.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5385.60it/s]\n",
      "sniffing:   9%|▊         | 9/103 [00:00<00:06, 15.25it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5403.64it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5313.28it/s]\n",
      "sniffing:  11%|█         | 11/103 [00:00<00:06, 14.77it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 2824.45it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6098.14it/s]\n",
      "sniffing:  13%|█▎        | 13/103 [00:00<00:06, 14.25it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5601.37it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5232.42it/s]\n",
      "sniffing:  15%|█▍        | 15/103 [00:01<00:06, 13.82it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6026.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6301.54it/s]\n",
      "sniffing:  17%|█▋        | 17/103 [00:01<00:06, 13.75it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6250.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3323.54it/s]\n",
      "sniffing:  18%|█▊        | 19/103 [00:01<00:06, 13.81it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5762.99it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5983.32it/s]\n",
      "sniffing:  20%|██        | 21/103 [00:01<00:05, 13.82it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6115.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5399.46it/s]\n",
      "sniffing:  22%|██▏       | 23/103 [00:01<00:05, 13.43it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5492.80it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5315.97it/s]\n",
      "sniffing:  24%|██▍       | 25/103 [00:01<00:05, 13.49it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6184.46it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5314.63it/s]\n",
      "sniffing:  26%|██▌       | 27/103 [00:01<00:05, 13.54it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3379.78it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5494.24it/s]\n",
      "sniffing:  28%|██▊       | 29/103 [00:02<00:05, 13.38it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4647.94it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6200.92it/s]\n",
      "sniffing:  30%|███       | 31/103 [00:02<00:05, 13.40it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6200.92it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5138.82it/s]\n",
      "sniffing:  32%|███▏      | 33/103 [00:02<00:05, 12.58it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5193.54it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6208.27it/s]\n",
      "sniffing:  34%|███▍      | 35/103 [00:02<00:05, 12.94it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5355.34it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5355.34it/s]\n",
      "sniffing:  36%|███▌      | 37/103 [00:02<00:05, 13.19it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4301.85it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4800.07it/s]\n",
      "sniffing:  38%|███▊      | 39/103 [00:02<00:04, 13.14it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6946.51it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5069.26it/s]\n",
      "sniffing:  40%|███▉      | 41/103 [00:02<00:04, 13.32it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 5106.70it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5344.42it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5133.79it/s]\n",
      "sniffing:  43%|████▎     | 44/103 [00:03<00:04, 14.48it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5216.80it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5322.72it/s]\n",
      "sniffing:  45%|████▍     | 46/103 [00:03<00:03, 14.99it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5867.80it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4988.47it/s]\n",
      "sniffing:  47%|████▋     | 48/103 [00:03<00:03, 14.60it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5775.69it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5173.04it/s]\n",
      "sniffing:  49%|████▊     | 50/103 [00:03<00:03, 14.34it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6193.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5447.15it/s]\n",
      "sniffing:  50%|█████     | 52/103 [00:03<00:03, 14.30it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4997.98it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4579.93it/s]\n",
      "sniffing:  52%|█████▏    | 54/103 [00:03<00:03, 14.18it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4343.73it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5018.31it/s]\n",
      "sniffing:  54%|█████▍    | 56/103 [00:04<00:03, 13.68it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5345.33it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5769.33it/s]\n",
      "sniffing:  56%|█████▋    | 58/103 [00:04<00:03, 13.38it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4854.52it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3298.96it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5064.36it/s]\n",
      "sniffing:  59%|█████▉    | 61/103 [00:04<00:02, 14.45it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5374.56it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4669.68it/s]\n",
      "sniffing:  61%|██████    | 63/103 [00:04<00:02, 14.12it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5303.87it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5346.47it/s]\n",
      "sniffing:  63%|██████▎   | 65/103 [00:04<00:02, 14.79it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5340.34it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5100.08it/s]\n",
      "sniffing:  65%|██████▌   | 67/103 [00:04<00:02, 14.35it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5285.16it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5183.27it/s]\n",
      "sniffing:  67%|██████▋   | 69/103 [00:04<00:02, 14.09it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5353.97it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3091.32it/s]\n",
      "sniffing:  69%|██████▉   | 71/103 [00:05<00:02, 13.84it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 3389.61it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5464.18it/s]\n",
      "sniffing:  71%|███████   | 73/103 [00:05<00:02, 13.75it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5224.59it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6098.14it/s]\n",
      "sniffing:  73%|███████▎  | 75/103 [00:05<00:02, 13.78it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6068.15it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5739.33it/s]\n",
      "sniffing:  75%|███████▍  | 77/103 [00:05<00:01, 13.67it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5165.40it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 4791.66it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5110.02it/s]\n",
      "sniffing:  78%|███████▊  | 80/103 [00:05<00:01, 15.16it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5341.70it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 5176.56it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5162.86it/s]\n",
      "sniffing:  81%|████████  | 83/103 [00:05<00:01, 15.82it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5179.22it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4511.94it/s]\n",
      "sniffing:  83%|████████▎ | 85/103 [00:05<00:01, 14.79it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5470.83it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5400.86it/s]\n",
      "sniffing:  84%|████████▍ | 87/103 [00:06<00:01, 14.28it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5128.76it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5437.26it/s]\n",
      "sniffing:  86%|████████▋ | 89/103 [00:06<00:00, 14.08it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5232.42it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5265.26it/s]\n",
      "sniffing:  88%|████████▊ | 91/103 [00:06<00:00, 14.08it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5381.45it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 6028.03it/s]\n",
      "sniffing:  90%|█████████ | 93/103 [00:06<00:00, 13.84it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5305.22it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5338.98it/s]\n",
      "sniffing:  92%|█████████▏| 95/103 [00:06<00:00, 14.09it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5399.46it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5238.95it/s]\n",
      "sniffing:  94%|█████████▍| 97/103 [00:06<00:00, 14.14it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 5178.15it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 4802.27it/s]\n",
      "sniffing:  96%|█████████▌| 99/103 [00:06<00:00, 14.00it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5403.64it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5203.85it/s]\n",
      "sniffing:  98%|█████████▊| 101/103 [00:07<00:00, 13.92it/s]\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5922.49it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 5468.45it/s]\n",
      "sniffing: 100%|██████████| 103/103 [00:07<00:00, 14.16it/s]\n",
      "/home/francois/.local/lib/python3.8/site-packages/pandas/core/internals/construction.py:309: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  values = np.array([convert(v) for v in values])\n",
      "creating Pandas DataFrames: 365it [00:10, 33.59it/s]\n"
     ]
    }
   ],
   "source": [
    "cimaq_infos = pd.concat(val.T for val in\n",
    "                        df(tuple(scanzip(apath,\n",
    "                                         exclude = ['Practice', 'Pratique', 'PRATIQUE', 'PRACTICE'],\n",
    "                                         to_xtrct = ['.pdf', '.edat2'],\n",
    "                                         dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                                 for apath in tqdm(snif.loadimages(xpu(zeprimes)),\n",
    "                                                   desc = 'sniffing')),\n",
    "                          dtype = object)[0].values.flatten()).dropna().reset_index(drop = True)\n",
    "\n",
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index'])\n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.mkfrombytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     hdr = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>CTL</td>\n",
       "      <td>CTL0</td>\n",
       "      <td>nan</td>\n",
       "      <td>CTL0</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>3.034</td>\n",
       "      <td>1.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc00</td>\n",
       "      <td>nan</td>\n",
       "      <td>Enc00</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>7.53</td>\n",
       "      <td>0.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc000</td>\n",
       "      <td>nan</td>\n",
       "      <td>Enc000</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>11.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>CTL</td>\n",
       "      <td>CTL01</td>\n",
       "      <td>nan</td>\n",
       "      <td>CTL01</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>14.551</td>\n",
       "      <td>5.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc01</td>\n",
       "      <td>nan</td>\n",
       "      <td>Enc01</td>\n",
       "      <td>Old60</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>23.06</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>116</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc75</td>\n",
       "      <td>nan</td>\n",
       "      <td>Enc75</td>\n",
       "      <td>Old45</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>697.419</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>117</td>\n",
       "      <td>CTL</td>\n",
       "      <td>CTL39</td>\n",
       "      <td>nan</td>\n",
       "      <td>CTL39</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>706.429</td>\n",
       "      <td>10.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>118</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc76</td>\n",
       "      <td>nan</td>\n",
       "      <td>Enc76</td>\n",
       "      <td>Old18</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>719.936</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc77</td>\n",
       "      <td>nan</td>\n",
       "      <td>Enc77</td>\n",
       "      <td>Old33</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>727.943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>120</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc78</td>\n",
       "      <td>nan</td>\n",
       "      <td>Enc78</td>\n",
       "      <td>Old14</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>731.938</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       0    1       2    3       4         5         6        7     8\n",
       "0      1  CTL    CTL0  nan    CTL0  Fixation     3.034      1.5   nan\n",
       "1      2  Enc   Enc00  nan   Enc00  Fixation      7.53      0.5   nan\n",
       "2      3  Enc  Enc000  nan  Enc000  Fixation    11.041      0.5   nan\n",
       "3      4  CTL   CTL01  nan   CTL01  Fixation    14.551      5.5   nan\n",
       "4      5  Enc   Enc01  nan   Enc01     Old60  Fixation    23.06   0.5\n",
       "..   ...  ...     ...  ...     ...       ...       ...      ...   ...\n",
       "115  116  Enc   Enc75  nan   Enc75     Old45  Fixation  697.419     6\n",
       "116  117  CTL   CTL39  nan   CTL39  Fixation   706.429     10.5   nan\n",
       "117  118  Enc   Enc76  nan   Enc76     Old18  Fixation  719.936     5\n",
       "118  119  Enc   Enc77  nan   Enc77     Old33  Fixation  727.943     1\n",
       "119  120  Enc   Enc78  nan   Enc78     Old14  Fixation  731.938  18.5\n",
       "\n",
       "[120 rows x 9 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq_infos['as_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>nan</td>\n",
       "      <td>3</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc000</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>11.041</td>\n",
       "      <td>0.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nan</td>\n",
       "      <td>4</td>\n",
       "      <td>CTL</td>\n",
       "      <td>CTL01</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>14.551</td>\n",
       "      <td>5.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nan</td>\n",
       "      <td>5</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc01</td>\n",
       "      <td>Old60</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>23.06</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>nan</td>\n",
       "      <td>116</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc75</td>\n",
       "      <td>Old45</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>697.419</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>nan</td>\n",
       "      <td>117</td>\n",
       "      <td>CTL</td>\n",
       "      <td>CTL39</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>706.429</td>\n",
       "      <td>10.5</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>nan</td>\n",
       "      <td>118</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc76</td>\n",
       "      <td>Old18</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>719.936</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>nan</td>\n",
       "      <td>119</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc77</td>\n",
       "      <td>Old33</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>727.943</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>nan</td>\n",
       "      <td>120</td>\n",
       "      <td>Enc</td>\n",
       "      <td>Enc78</td>\n",
       "      <td>Old14</td>\n",
       "      <td>Fixation</td>\n",
       "      <td>731.938</td>\n",
       "      <td>18.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      3    4    5       6         7         8        9     10\n",
       "0    nan  nan  nan     nan       nan       nan      nan   nan\n",
       "1    nan  nan  nan     nan       nan       nan      nan   nan\n",
       "2    nan    3  Enc  Enc000  Fixation    11.041      0.5   nan\n",
       "3    nan    4  CTL   CTL01  Fixation    14.551      5.5   nan\n",
       "4    nan    5  Enc   Enc01     Old60  Fixation    23.06   0.5\n",
       "..   ...  ...  ...     ...       ...       ...      ...   ...\n",
       "115  nan  116  Enc   Enc75     Old45  Fixation  697.419     6\n",
       "116  nan  117  CTL   CTL39  Fixation   706.429     10.5   nan\n",
       "117  nan  118  Enc   Enc76     Old18  Fixation  719.936     5\n",
       "118  nan  119  Enc   Enc77     Old33  Fixation  727.943     1\n",
       "119  nan  120  Enc   Enc78     Old14  Fixation  731.938  18.5\n",
       "\n",
       "[120 rows x 8 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([itm[1] for itm in\n",
    "           list(cimaq_infos['as_df'].iloc[0].iteritems())[[0, 1, 2, 3][-1]:]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "creating Pandas DataFrames: 365it [00:10, 35.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# cimaq_infos = df(tuple(scanzip(apath) for apath in\n",
    "#                        tqdm(loadimages(xpu(zeprimes)),\n",
    "#                             desc = 'sniffing')))\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.mkfrombytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     hdr = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n",
    "\n",
    "# cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                           dtype = object).T.set_index(0, drop = True).T\n",
    "#                          if row[1].has_header else\n",
    "#                          df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                            dtype = object)\n",
    "#                          for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                          desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module collections.abc in collections:\n",
      "\n",
      "NAME\n",
      "    collections.abc\n",
      "\n",
      "MODULE REFERENCE\n",
      "    https://docs.python.org/3.8/library/collections.abc\n",
      "    \n",
      "    The following documentation is automatically generated from the Python\n",
      "    source files.  It may be incomplete, incorrect or include features that\n",
      "    are considered implementation detail and may vary between Python\n",
      "    implementations.  When in doubt, consult the module reference at the\n",
      "    location listed above.\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        AsyncIterable\n",
      "            AsyncIterator\n",
      "                AsyncGenerator\n",
      "        Awaitable\n",
      "            Coroutine\n",
      "        Callable\n",
      "        Container\n",
      "        Hashable\n",
      "        Iterable\n",
      "            Iterator\n",
      "                Generator\n",
      "            Reversible\n",
      "                Sequence(Reversible, Collection)\n",
      "                    ByteString\n",
      "                    MutableSequence\n",
      "        Sized\n",
      "            Collection(Sized, Iterable, Container)\n",
      "                Mapping\n",
      "                    MutableMapping\n",
      "                Set\n",
      "                    MutableSet\n",
      "            MappingView\n",
      "                ItemsView(MappingView, Set)\n",
      "                KeysView(MappingView, Set)\n",
      "                ValuesView(MappingView, Collection)\n",
      "    \n",
      "    class AsyncGenerator(AsyncIterator)\n",
      "     |  Method resolution order:\n",
      "     |      AsyncGenerator\n",
      "     |      AsyncIterator\n",
      "     |      AsyncIterable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  async __anext__(self)\n",
      "     |      Return the next item from the asynchronous generator.\n",
      "     |      When exhausted, raise StopAsyncIteration.\n",
      "     |  \n",
      "     |  async aclose(self)\n",
      "     |      Raise GeneratorExit inside coroutine.\n",
      "     |  \n",
      "     |  async asend(self, value)\n",
      "     |      Send a value into the asynchronous generator.\n",
      "     |      Return next yielded value or raise StopAsyncIteration.\n",
      "     |  \n",
      "     |  async athrow(self, typ, val=None, tb=None)\n",
      "     |      Raise an exception in the asynchronous generator.\n",
      "     |      Return next yielded value or raise StopAsyncIteration.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'asend', 'athrow'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from AsyncIterator:\n",
      "     |  \n",
      "     |  __aiter__(self)\n",
      "    \n",
      "    class AsyncIterable(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __aiter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__aiter__'})\n",
      "    \n",
      "    class AsyncIterator(AsyncIterable)\n",
      "     |  Method resolution order:\n",
      "     |      AsyncIterator\n",
      "     |      AsyncIterable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __aiter__(self)\n",
      "     |  \n",
      "     |  async __anext__(self)\n",
      "     |      Return the next item or raise StopAsyncIteration when exhausted.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__anext__'})\n",
      "    \n",
      "    class Awaitable(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __await__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__await__'})\n",
      "    \n",
      "    class ByteString(Sequence)\n",
      "     |  This unifies bytes and bytearray.\n",
      "     |  \n",
      "     |  XXX Should add all their methods.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ByteString\n",
      "     |      Sequence\n",
      "     |      Reversible\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__getitem__', '__len__'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sequence:\n",
      "     |  \n",
      "     |  __contains__(self, value)\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __reversed__(self)\n",
      "     |  \n",
      "     |  count(self, value)\n",
      "     |      S.count(value) -> integer -- return number of occurrences of value\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=None)\n",
      "     |      S.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |      \n",
      "     |      Supporting start and stop arguments is optional, but\n",
      "     |      recommended.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Reversible:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "    \n",
      "    class Callable(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __call__(self, *args, **kwds)\n",
      "     |      Call self as a function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__call__'})\n",
      "    \n",
      "    class Collection(Sized, Iterable, Container)\n",
      "     |  Method resolution order:\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__contains__', '__iter__', '__len__'...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Iterable:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Container:\n",
      "     |  \n",
      "     |  __contains__(self, x)\n",
      "    \n",
      "    class Container(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, x)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__contains__'})\n",
      "    \n",
      "    class Coroutine(Awaitable)\n",
      "     |  Method resolution order:\n",
      "     |      Coroutine\n",
      "     |      Awaitable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Raise GeneratorExit inside coroutine.\n",
      "     |  \n",
      "     |  send(self, value)\n",
      "     |      Send a value into the coroutine.\n",
      "     |      Return next yielded value or raise StopIteration.\n",
      "     |  \n",
      "     |  throw(self, typ, val=None, tb=None)\n",
      "     |      Raise an exception in the coroutine.\n",
      "     |      Return next yielded value or raise StopIteration.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__await__', 'send', 'throw'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Awaitable:\n",
      "     |  \n",
      "     |  __await__(self)\n",
      "    \n",
      "    class Generator(Iterator)\n",
      "     |  Method resolution order:\n",
      "     |      Generator\n",
      "     |      Iterator\n",
      "     |      Iterable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __next__(self)\n",
      "     |      Return the next item from the generator.\n",
      "     |      When exhausted, raise StopIteration.\n",
      "     |  \n",
      "     |  close(self)\n",
      "     |      Raise GeneratorExit inside generator.\n",
      "     |  \n",
      "     |  send(self, value)\n",
      "     |      Send a value into the generator.\n",
      "     |      Return next yielded value or raise StopIteration.\n",
      "     |  \n",
      "     |  throw(self, typ, val=None, tb=None)\n",
      "     |      Raise an exception in the generator.\n",
      "     |      Return next yielded value or raise StopIteration.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'send', 'throw'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Iterator:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "    \n",
      "    class Hashable(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__hash__'})\n",
      "    \n",
      "    class ItemsView(MappingView, Set)\n",
      "     |  ItemsView(mapping)\n",
      "     |  \n",
      "     |  A set is a finite, iterable container.\n",
      "     |  \n",
      "     |  This class provides concrete generic implementations of all\n",
      "     |  methods except for __contains__, __iter__ and __len__.\n",
      "     |  \n",
      "     |  To override the comparisons (presumably for speed, as the\n",
      "     |  semantics are fixed), redefine __le__ and __ge__,\n",
      "     |  then the other operations will automatically follow suit.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ItemsView\n",
      "     |      MappingView\n",
      "     |      Set\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, item)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MappingView:\n",
      "     |  \n",
      "     |  __init__(self, mapping)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Set:\n",
      "     |  \n",
      "     |  __and__(self, other)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __or__(self, other)\n",
      "     |  \n",
      "     |  __rand__ = __and__(self, other)\n",
      "     |  \n",
      "     |  __ror__ = __or__(self, other)\n",
      "     |  \n",
      "     |  __rsub__(self, other)\n",
      "     |  \n",
      "     |  __rxor__ = __xor__(self, other)\n",
      "     |  \n",
      "     |  __sub__(self, other)\n",
      "     |  \n",
      "     |  __xor__(self, other)\n",
      "     |  \n",
      "     |  isdisjoint(self, other)\n",
      "     |      Return True if two sets have a null intersection.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Set:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "    \n",
      "    class Iterable(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__iter__'})\n",
      "    \n",
      "    class Iterator(Iterable)\n",
      "     |  Method resolution order:\n",
      "     |      Iterator\n",
      "     |      Iterable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __next__(self)\n",
      "     |      Return the next item from the iterator. When exhausted, raise StopIteration\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__next__'})\n",
      "    \n",
      "    class KeysView(MappingView, Set)\n",
      "     |  KeysView(mapping)\n",
      "     |  \n",
      "     |  A set is a finite, iterable container.\n",
      "     |  \n",
      "     |  This class provides concrete generic implementations of all\n",
      "     |  methods except for __contains__, __iter__ and __len__.\n",
      "     |  \n",
      "     |  To override the comparisons (presumably for speed, as the\n",
      "     |  semantics are fixed), redefine __le__ and __ge__,\n",
      "     |  then the other operations will automatically follow suit.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      KeysView\n",
      "     |      MappingView\n",
      "     |      Set\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, key)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MappingView:\n",
      "     |  \n",
      "     |  __init__(self, mapping)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Set:\n",
      "     |  \n",
      "     |  __and__(self, other)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __or__(self, other)\n",
      "     |  \n",
      "     |  __rand__ = __and__(self, other)\n",
      "     |  \n",
      "     |  __ror__ = __or__(self, other)\n",
      "     |  \n",
      "     |  __rsub__(self, other)\n",
      "     |  \n",
      "     |  __rxor__ = __xor__(self, other)\n",
      "     |  \n",
      "     |  __sub__(self, other)\n",
      "     |  \n",
      "     |  __xor__(self, other)\n",
      "     |  \n",
      "     |  isdisjoint(self, other)\n",
      "     |      Return True if two sets have a null intersection.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Set:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "    \n",
      "    class Mapping(Collection)\n",
      "     |  Method resolution order:\n",
      "     |      Mapping\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, key)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getitem__(self, key)\n",
      "     |  \n",
      "     |  get(self, key, default=None)\n",
      "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      "     |  \n",
      "     |  items(self)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(self)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  values(self)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__getitem__', '__iter__', '__len__'}...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __reversed__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Iterable:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "    \n",
      "    class MappingView(Sized)\n",
      "     |  MappingView(mapping)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MappingView\n",
      "     |      Sized\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, mapping)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Sized:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "    \n",
      "    class MutableMapping(Mapping)\n",
      "     |  Method resolution order:\n",
      "     |      MutableMapping\n",
      "     |      Mapping\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delitem__(self, key)\n",
      "     |  \n",
      "     |  __setitem__(self, key, value)\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      D.clear() -> None.  Remove all items from D.\n",
      "     |  \n",
      "     |  pop(self, key, default=<object object at 0x7f0c75eb9140>)\n",
      "     |      D.pop(k[,d]) -> v, remove specified key and return the corresponding value.\n",
      "     |      If key is not found, d is returned if given, otherwise KeyError is raised.\n",
      "     |  \n",
      "     |  popitem(self)\n",
      "     |      D.popitem() -> (k, v), remove and return some (key, value) pair\n",
      "     |      as a 2-tuple; but raise KeyError if D is empty.\n",
      "     |  \n",
      "     |  setdefault(self, key, default=None)\n",
      "     |      D.setdefault(k[,d]) -> D.get(k,d), also set D[k]=d if k not in D\n",
      "     |  \n",
      "     |  update(self, other=(), /, **kwds)\n",
      "     |      D.update([E, ]**F) -> None.  Update D from mapping/iterable E and F.\n",
      "     |      If E present and has a .keys() method, does:     for k in E: D[k] = E[k]\n",
      "     |      If E present and lacks .keys() method, does:     for (k, v) in E: D[k] = v\n",
      "     |      In either case, this is followed by: for k, v in F.items(): D[k] = v\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__delitem__', '__getitem__', '__iter...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Mapping:\n",
      "     |  \n",
      "     |  __contains__(self, key)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __getitem__(self, key)\n",
      "     |  \n",
      "     |  get(self, key, default=None)\n",
      "     |      D.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\n",
      "     |  \n",
      "     |  items(self)\n",
      "     |      D.items() -> a set-like object providing a view on D's items\n",
      "     |  \n",
      "     |  keys(self)\n",
      "     |      D.keys() -> a set-like object providing a view on D's keys\n",
      "     |  \n",
      "     |  values(self)\n",
      "     |      D.values() -> an object providing a view on D's values\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Mapping:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  __reversed__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Iterable:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "    \n",
      "    class MutableSequence(Sequence)\n",
      "     |  All the operations on a read-only sequence.\n",
      "     |  \n",
      "     |  Concrete subclasses must override __new__ or __init__,\n",
      "     |  __getitem__, and __len__.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MutableSequence\n",
      "     |      Sequence\n",
      "     |      Reversible\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __delitem__(self, index)\n",
      "     |  \n",
      "     |  __iadd__(self, values)\n",
      "     |  \n",
      "     |  __setitem__(self, index, value)\n",
      "     |  \n",
      "     |  append(self, value)\n",
      "     |      S.append(value) -- append value to the end of the sequence\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      S.clear() -> None -- remove all items from S\n",
      "     |  \n",
      "     |  extend(self, values)\n",
      "     |      S.extend(iterable) -- extend sequence by appending elements from the iterable\n",
      "     |  \n",
      "     |  insert(self, index, value)\n",
      "     |      S.insert(index, value) -- insert value before index\n",
      "     |  \n",
      "     |  pop(self, index=-1)\n",
      "     |      S.pop([index]) -> item -- remove and return item at index (default last).\n",
      "     |      Raise IndexError if list is empty or index is out of range.\n",
      "     |  \n",
      "     |  remove(self, value)\n",
      "     |      S.remove(value) -- remove first occurrence of value.\n",
      "     |      Raise ValueError if the value is not present.\n",
      "     |  \n",
      "     |  reverse(self)\n",
      "     |      S.reverse() -- reverse *IN PLACE*\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__delitem__', '__getitem__', '__len_...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sequence:\n",
      "     |  \n",
      "     |  __contains__(self, value)\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __reversed__(self)\n",
      "     |  \n",
      "     |  count(self, value)\n",
      "     |      S.count(value) -> integer -- return number of occurrences of value\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=None)\n",
      "     |      S.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |      \n",
      "     |      Supporting start and stop arguments is optional, but\n",
      "     |      recommended.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Reversible:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "    \n",
      "    class MutableSet(Set)\n",
      "     |  A mutable set is a finite, iterable container.\n",
      "     |  \n",
      "     |  This class provides concrete generic implementations of all\n",
      "     |  methods except for __contains__, __iter__, __len__,\n",
      "     |  add(), and discard().\n",
      "     |  \n",
      "     |  To override the comparisons (presumably for speed, as the\n",
      "     |  semantics are fixed), all you have to do is redefine __le__ and\n",
      "     |  then the other operations will automatically follow suit.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MutableSet\n",
      "     |      Set\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __iand__(self, it)\n",
      "     |  \n",
      "     |  __ior__(self, it)\n",
      "     |  \n",
      "     |  __isub__(self, it)\n",
      "     |  \n",
      "     |  __ixor__(self, it)\n",
      "     |  \n",
      "     |  add(self, value)\n",
      "     |      Add an element.\n",
      "     |  \n",
      "     |  clear(self)\n",
      "     |      This is slow (creates N new iterators!) but effective.\n",
      "     |  \n",
      "     |  discard(self, value)\n",
      "     |      Remove an element.  Do not raise an exception if absent.\n",
      "     |  \n",
      "     |  pop(self)\n",
      "     |      Return the popped value.  Raise KeyError if empty.\n",
      "     |  \n",
      "     |  remove(self, value)\n",
      "     |      Remove an element. If not a member, raise a KeyError.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__contains__', '__iter__', '__len__'...\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Set:\n",
      "     |  \n",
      "     |  __and__(self, other)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __or__(self, other)\n",
      "     |  \n",
      "     |  __rand__ = __and__(self, other)\n",
      "     |  \n",
      "     |  __ror__ = __or__(self, other)\n",
      "     |  \n",
      "     |  __rsub__(self, other)\n",
      "     |  \n",
      "     |  __rxor__ = __xor__(self, other)\n",
      "     |  \n",
      "     |  __sub__(self, other)\n",
      "     |  \n",
      "     |  __xor__(self, other)\n",
      "     |  \n",
      "     |  isdisjoint(self, other)\n",
      "     |      Return True if two sets have a null intersection.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from Set:\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Iterable:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Container:\n",
      "     |  \n",
      "     |  __contains__(self, x)\n",
      "    \n",
      "    class Reversible(Iterable)\n",
      "     |  Method resolution order:\n",
      "     |      Reversible\n",
      "     |      Iterable\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __reversed__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__iter__', '__reversed__'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Iterable:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "    \n",
      "    class Sequence(Reversible, Collection)\n",
      "     |  All the operations on a read-only sequence.\n",
      "     |  \n",
      "     |  Concrete subclasses must override __new__ or __init__,\n",
      "     |  __getitem__, and __len__.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Sequence\n",
      "     |      Reversible\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, value)\n",
      "     |  \n",
      "     |  __getitem__(self, index)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  __reversed__(self)\n",
      "     |  \n",
      "     |  count(self, value)\n",
      "     |      S.count(value) -> integer -- return number of occurrences of value\n",
      "     |  \n",
      "     |  index(self, value, start=0, stop=None)\n",
      "     |      S.index(value, [start, [stop]]) -> integer -- return first index of value.\n",
      "     |      Raises ValueError if the value is not present.\n",
      "     |      \n",
      "     |      Supporting start and stop arguments is optional, but\n",
      "     |      recommended.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__getitem__', '__len__'})\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Reversible:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "    \n",
      "    class Set(Collection)\n",
      "     |  A set is a finite, iterable container.\n",
      "     |  \n",
      "     |  This class provides concrete generic implementations of all\n",
      "     |  methods except for __contains__, __iter__ and __len__.\n",
      "     |  \n",
      "     |  To override the comparisons (presumably for speed, as the\n",
      "     |  semantics are fixed), redefine __le__ and __ge__,\n",
      "     |  then the other operations will automatically follow suit.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      Set\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __and__(self, other)\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __ge__(self, other)\n",
      "     |      Return self>=value.\n",
      "     |  \n",
      "     |  __gt__(self, other)\n",
      "     |      Return self>value.\n",
      "     |  \n",
      "     |  __le__(self, other)\n",
      "     |      Return self<=value.\n",
      "     |  \n",
      "     |  __lt__(self, other)\n",
      "     |      Return self<value.\n",
      "     |  \n",
      "     |  __or__(self, other)\n",
      "     |  \n",
      "     |  __rand__ = __and__(self, other)\n",
      "     |  \n",
      "     |  __ror__ = __or__(self, other)\n",
      "     |  \n",
      "     |  __rsub__(self, other)\n",
      "     |  \n",
      "     |  __rxor__ = __xor__(self, other)\n",
      "     |  \n",
      "     |  __sub__(self, other)\n",
      "     |  \n",
      "     |  __xor__(self, other)\n",
      "     |  \n",
      "     |  isdisjoint(self, other)\n",
      "     |      Return True if two sets have a null intersection.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__contains__', '__iter__', '__len__'...\n",
      "     |  \n",
      "     |  __hash__ = None\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Sized:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Iterable:\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from Container:\n",
      "     |  \n",
      "     |  __contains__(self, x)\n",
      "    \n",
      "    class Sized(builtins.object)\n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset({'__len__'})\n",
      "    \n",
      "    class ValuesView(MappingView, Collection)\n",
      "     |  ValuesView(mapping)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ValuesView\n",
      "     |      MappingView\n",
      "     |      Collection\n",
      "     |      Sized\n",
      "     |      Iterable\n",
      "     |      Container\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __contains__(self, value)\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __abstractmethods__ = frozenset()\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from MappingView:\n",
      "     |  \n",
      "     |  __init__(self, mapping)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from Collection:\n",
      "     |  \n",
      "     |  __subclasshook__(C) from abc.ABCMeta\n",
      "     |      Abstract classes can override this to customize issubclass().\n",
      "     |      \n",
      "     |      This is invoked early on by abc.ABCMeta.__subclasscheck__().\n",
      "     |      It should return True, False or NotImplemented.  If it returns\n",
      "     |      NotImplemented, the normal algorithm is used.  Otherwise, it\n",
      "     |      overrides the normal algorithm (and the outcome is cached).\n",
      "\n",
      "DATA\n",
      "    __all__ = ['Awaitable', 'Coroutine', 'AsyncIterable', 'AsyncIterator',...\n",
      "\n",
      "FILE\n",
      "    /usr/lib/python3.8/collections/abc.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import typing\n",
    "import collections\n",
    "help(collections.abc)\n",
    "# help(typing.Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'1\\tctl\\tctl0\\tnan\\tnan\\tnan\\tfixation\\t3.034\\t1.5',\n",
       " b'2\\tenc\\tenc00\\tnan\\tnan\\tnan\\tfixation\\t7.53\\t0.5',\n",
       " b'3\\tenc\\tenc000\\tnan\\tnan\\tnan\\tfixation\\t11.041\\t0.5',\n",
       " b'4\\tctl\\tctl01\\tnan\\tnan\\tnan\\tfixation\\t14.551\\t5.5',\n",
       " b'5\\tenc\\tenc01\\told60\\tnan\\told60\\tfixation\\t23.06\\t0.5',\n",
       " b'6\\tenc\\tenc02\\told01\\tnan\\told01\\tfixation\\t26.57\\t1',\n",
       " b'7\\tenc\\tenc03\\told24\\tnan\\told24\\tfixation\\t30.566\\t1',\n",
       " b'8\\tenc\\tenc04\\told68\\tnan\\told68\\tfixation\\t34.578\\t1.5',\n",
       " b'9\\tenc\\tenc05\\told55\\tnan\\told55\\tfixation\\t39.074\\t2',\n",
       " b'10\\tenc\\tenc06\\told46\\tnan\\told46\\tfixation\\t44.073\\t3',\n",
       " b'11\\tctl\\tctl02\\tnan\\tnan\\tnan\\tfixation\\t50.074\\t0.5',\n",
       " b'12\\tctl\\tctl03\\tnan\\tnan\\tnan\\tfixation\\t53.584\\t4',\n",
       " b'13\\tenc\\tenc07\\told38\\tnan\\told38\\tfixation\\t60.588\\t2.5',\n",
       " b'14\\tctl\\tctl04\\tnan\\tnan\\tnan\\tfixation\\t66.088\\t1.5',\n",
       " b'15\\tenc\\tenc08\\told47\\tnan\\told47\\tfixation\\t70.585\\t2',\n",
       " b'16\\tenc\\tenc09\\told20\\tnan\\told20\\tfixation\\t75.583\\t3.5',\n",
       " b'17\\tctl\\tctl05\\tnan\\tnan\\tnan\\tfixation\\t82.086\\t0.5',\n",
       " b'18\\tenc\\tenc10\\told30\\tnan\\told30\\tfixation\\t85.596\\t2.5',\n",
       " b'19\\tenc\\tenc11\\told05\\tnan\\told05\\tfixation\\t91.096\\t0.5',\n",
       " b'20\\tenc\\tenc12\\told25\\tnan\\told25\\tfixation\\t94.606\\t1',\n",
       " b'21\\tenc\\tenc13\\told77\\tnan\\told77\\tfixation\\t98.602\\t1.5',\n",
       " b'22\\tenc\\tenc14\\told17\\tnan\\told17\\tfixation\\t103.098\\t2.5',\n",
       " b'23\\tenc\\tenc15\\told35\\tnan\\told35\\tfixation\\t108.598\\t3',\n",
       " b'24\\tenc\\tenc16\\told32\\tnan\\told32\\tfixation\\t114.599\\t1',\n",
       " b'25\\tenc\\tenc17\\told26\\tnan\\told26\\tfixation\\t118.595\\t1.5',\n",
       " b'26\\tctl\\tctl06\\tnan\\tnan\\tnan\\tfixation\\t123.091\\t1.5',\n",
       " b'27\\tenc\\tenc18\\told03\\tnan\\told03\\tfixation\\t127.588\\t1',\n",
       " b'28\\tenc\\tenc19\\told41\\tnan\\told41\\tfixation\\t131.6\\t2.5',\n",
       " b'29\\tctl\\tctl07\\tnan\\tnan\\tnan\\tfixation\\t137.1\\t2.5',\n",
       " b'30\\tenc\\tenc20\\told66\\tnan\\told66\\tfixation\\t142.599\\t0.5',\n",
       " b'31\\tenc\\tenc21\\told56\\tnan\\told56\\tfixation\\t146.11\\t5.5',\n",
       " b'32\\tenc\\tenc22\\told74\\tnan\\told74\\tfixation\\t154.618\\t5.5',\n",
       " b'33\\tenc\\tenc23\\told10\\tnan\\told10\\tfixation\\t163.127\\t2',\n",
       " b'34\\tenc\\tenc24\\told07\\tnan\\told07\\tfixation\\t168.125\\t1',\n",
       " b'35\\tenc\\tenc25\\told52\\tnan\\told52\\tfixation\\t172.121\\t3',\n",
       " b'36\\tenc\\tenc26\\told64\\tnan\\told64\\tfixation\\t178.122\\t3.5',\n",
       " b'37\\tctl\\tctl08\\tnan\\tnan\\tnan\\tfixation\\t184.625\\t0.5',\n",
       " b'38\\tctl\\tctl09\\tnan\\tnan\\tnan\\tfixation\\t188.135\\t1.5',\n",
       " b'39\\tctl\\tctl10\\tnan\\tnan\\tnan\\tfixation\\t192.632\\t9.5',\n",
       " b'40\\tctl\\tctl11\\tnan\\tnan\\tnan\\tfixation\\t205.136\\t2',\n",
       " b'41\\tenc\\tenc27\\told51\\tnan\\told51\\tfixation\\t210.134\\t2',\n",
       " b'42\\tctl\\tctl12\\tnan\\tnan\\tnan\\tfixation\\t215.132\\t1',\n",
       " b'43\\tenc\\tenc28\\told16\\tnan\\told16\\tfixation\\t219.127\\t0.5',\n",
       " b'44\\tctl\\tctl13\\tnan\\tnan\\tnan\\tfixation\\t222.638\\t0.5',\n",
       " b'45\\tenc\\tenc29\\told28\\tnan\\told28\\tfixation\\t226.148\\t2',\n",
       " b'46\\tenc\\tenc30\\told58\\tnan\\told58\\tfixation\\t231.146\\t4.5',\n",
       " b'47\\tenc\\tenc31\\told04\\tnan\\told04\\tfixation\\t238.652\\t3.5',\n",
       " b'48\\tenc\\tenc32\\told71\\tnan\\told71\\tfixation\\t245.155\\t8',\n",
       " b'49\\tenc\\tenc33\\told15\\tnan\\told15\\tfixation\\t256.154\\t9',\n",
       " b'50\\tenc\\tenc34\\told19\\tnan\\told19\\tfixation\\t268.157\\t0.5',\n",
       " b'51\\tctl\\tctl14\\tnan\\tnan\\tnan\\tfixation\\t271.667\\t2.5',\n",
       " b'52\\tenc\\tenc35\\told78\\tnan\\told78\\tfixation\\t277.167\\t8.5',\n",
       " b'53\\tenc\\tenc36\\told12\\tnan\\told12\\tfixation\\t288.668\\t0.5',\n",
       " b'54\\tenc\\tenc37\\told02\\tnan\\told02\\tfixation\\t292.178\\t5.5',\n",
       " b'55\\tenc\\tenc38\\told43\\tnan\\told43\\tfixation\\t300.687\\t3',\n",
       " b'56\\tctl\\tctl15\\tnan\\tnan\\tnan\\tfixation\\t306.688\\t4',\n",
       " b'57\\tctl\\tctl16\\tnan\\tnan\\tnan\\tfixation\\t313.692\\t6.5',\n",
       " b'58\\tenc\\tenc39\\told62\\tnan\\told62\\tfixation\\t323.187\\t0.5',\n",
       " b'59\\tenc\\tenc40\\told63\\tnan\\told63\\tfixation\\t326.698\\t11.5',\n",
       " b'60\\tenc\\tenc41\\told67\\tnan\\told67\\tfixation\\t341.208\\t9.5',\n",
       " b'61\\tctl\\tctl17\\tnan\\tnan\\tnan\\tfixation\\t353.712\\t1',\n",
       " b'62\\tenc\\tenc42\\told53\\tnan\\told53\\tfixation\\t357.724\\t5.5',\n",
       " b'63\\tenc\\tenc43\\told23\\tnan\\told23\\tfixation\\t366.232\\t1.5',\n",
       " b'64\\tenc\\tenc44\\told39\\tnan\\told39\\tfixation\\t370.729\\t1',\n",
       " b'65\\tctl\\tctl18\\tnan\\tnan\\tnan\\tfixation\\t374.741\\t2',\n",
       " b'66\\tenc\\tenc45\\told22\\tnan\\told22\\tfixation\\t379.739\\t3.5',\n",
       " b'67\\tenc\\tenc46\\told37\\tnan\\told37\\tfixation\\t386.242\\t1.5',\n",
       " b'68\\tctl\\tctl19\\tnan\\tnan\\tnan\\tfixation\\t390.739\\t0.5',\n",
       " b'69\\tctl\\tctl20\\tnan\\tnan\\tnan\\tfixation\\t394.249\\t2.5',\n",
       " b'70\\tctl\\tctl21\\tnan\\tnan\\tnan\\tfixation\\t399.749\\t0.5',\n",
       " b'71\\tctl\\tctl22\\tnan\\tnan\\tnan\\tfixation\\t403.259\\t2.5',\n",
       " b'72\\tctl\\tctl23\\tnan\\tnan\\tnan\\tfixation\\t408.759\\t2',\n",
       " b'73\\tctl\\tctl24\\tnan\\tnan\\tnan\\tfixation\\t413.757\\t11',\n",
       " b'74\\tenc\\tenc47\\told34\\tnan\\told34\\tfixation\\t427.766\\t1',\n",
       " b'75\\tenc\\tenc48\\told09\\tnan\\told09\\tfixation\\t431.778\\t1.5',\n",
       " b'76\\tenc\\tenc49\\told42\\tnan\\told42\\tfixation\\t436.274\\t0.5',\n",
       " b'77\\tenc\\tenc50\\told50\\tnan\\told50\\tfixation\\t439.785\\t0.5',\n",
       " b'78\\tenc\\tenc51\\told11\\tnan\\told11\\tfixation\\t443.295\\t2',\n",
       " b'79\\tctl\\tctl25\\tnan\\tnan\\tnan\\tfixation\\t448.293\\t1',\n",
       " b'80\\tctl\\tctl26\\tnan\\tnan\\tnan\\tfixation\\t452.289\\t2',\n",
       " b'81\\tenc\\tenc52\\told40\\tnan\\told40\\tfixation\\t457.287\\t4',\n",
       " b'82\\tenc\\tenc53\\told59\\tnan\\told59\\tfixation\\t464.291\\t4.5',\n",
       " b'83\\tenc\\tenc54\\told48\\tnan\\told48\\tfixation\\t471.797\\t8.5',\n",
       " b'84\\tenc\\tenc55\\told65\\tnan\\told65\\tfixation\\t483.298\\t1',\n",
       " b'85\\tctl\\tctl27\\tnan\\tnan\\tnan\\tfixation\\t487.31\\t6',\n",
       " b'86\\tenc\\tenc56\\told29\\tnan\\told29\\tfixation\\t496.32\\t1.5',\n",
       " b'87\\tctl\\tctl28\\tnan\\tnan\\tnan\\tfixation\\t500.817\\t13',\n",
       " b'88\\tenc\\tenc57\\told57\\tnan\\told57\\tfixation\\t516.814\\t1.5',\n",
       " b'89\\tenc\\tenc58\\told75\\tnan\\told75\\tfixation\\t521.311\\t1.5',\n",
       " b'90\\tenc\\tenc59\\told54\\tnan\\told54\\tfixation\\t525.808\\t7.5',\n",
       " b'91\\tctl\\tctl29\\tnan\\tnan\\tnan\\tfixation\\t536.306\\t1.5',\n",
       " b'92\\tenc\\tenc60\\told61\\tnan\\told61\\tfixation\\t540.802\\t3.5',\n",
       " b'93\\tctl\\tctl30\\tnan\\tnan\\tnan\\tfixation\\t547.305\\t1',\n",
       " b'94\\tenc\\tenc61\\told13\\tnan\\told13\\tfixation\\t551.317\\t1.5',\n",
       " b'95\\tctl\\tctl31\\tnan\\tnan\\tnan\\tfixation\\t555.814\\t3.5',\n",
       " b'96\\tenc\\tenc62\\told31\\tnan\\told31\\tfixation\\t562.316\\t4.5',\n",
       " b'97\\tctl\\tctl32\\tnan\\tnan\\tnan\\tfixation\\t569.822\\t3',\n",
       " b'98\\tenc\\tenc63\\told69\\tnan\\told69\\tfixation\\t575.823\\t0.5',\n",
       " b'99\\tenc\\tenc64\\told76\\tnan\\told76\\tfixation\\t579.334\\t8',\n",
       " b'100\\tctl\\tctl33\\tnan\\tnan\\tnan\\tfixation\\t590.333\\t5',\n",
       " b'101\\tctl\\tctl34\\tnan\\tnan\\tnan\\tfixation\\t598.34\\t0.5',\n",
       " b'102\\tenc\\tenc65\\told72\\tnan\\told72\\tfixation\\t601.851\\t1',\n",
       " b'103\\tenc\\tenc66\\told73\\tnan\\told73\\tfixation\\t605.863\\t11.5',\n",
       " b'104\\tctl\\tctl35\\tnan\\tnan\\tnan\\tfixation\\t620.373\\t2.5',\n",
       " b'105\\tenc\\tenc67\\told08\\tnan\\told08\\tfixation\\t625.872\\t2',\n",
       " b'106\\tenc\\tenc68\\told06\\tnan\\told06\\tfixation\\t630.871\\t0.5',\n",
       " b'107\\tenc\\tenc69\\told36\\tnan\\told36\\tfixation\\t634.381\\t1',\n",
       " b'108\\tenc\\tenc70\\told49\\tnan\\told49\\tfixation\\t638.376\\t1',\n",
       " b'109\\tenc\\tenc71\\told27\\tnan\\told27\\tfixation\\t642.372\\t0.5',\n",
       " b'110\\tctl\\tctl36\\tnan\\tnan\\tnan\\tfixation\\t645.882\\t7.5',\n",
       " b'111\\tenc\\tenc72\\told21\\tnan\\told21\\tfixation\\t656.38\\t15',\n",
       " b'112\\tenc\\tenc73\\told70\\tnan\\told70\\tfixation\\t674.384\\t0.5',\n",
       " b'113\\tctl\\tctl37\\tnan\\tnan\\tnan\\tfixation\\t677.894\\t1',\n",
       " b'114\\tctl\\tctl38\\tnan\\tnan\\tnan\\tfixation\\t681.906\\t0.5',\n",
       " b'115\\tenc\\tenc74\\told44\\tnan\\told44\\tfixation\\t685.416\\t9',\n",
       " b'116\\tenc\\tenc75\\told45\\tnan\\told45\\tfixation\\t697.419\\t6',\n",
       " b'117\\tctl\\tctl39\\tnan\\tnan\\tnan\\tfixation\\t706.429\\t10.5',\n",
       " b'118\\tenc\\tenc76\\told18\\tnan\\told18\\tfixation\\t719.936\\t5',\n",
       " b'119\\tenc\\tenc77\\told33\\tnan\\told33\\tfixation\\t727.943\\t1',\n",
       " b'120\\tenc\\tenc78\\told14\\tnan\\told14\\tfixation\\t731.938\\t18.5']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index\n",
    "\n",
    "tsheets = cimaq_infos.iloc[:3, :]\n",
    "# bsheets\n",
    "def clean_bytes(inpt, encoding, hdr, delimiter, lineterminator, dup_index, nfields):\n",
    "    newsheet = b'\\n'.join([b'\\t'.join(itm.strip(b'\\\\s') for itm in re.sub(b'\\\\s{2,}',\n",
    "                                         b'\\\\s'+delimiter+b'\\\\s',\n",
    "                                         line).split(delimiter))\n",
    "                       for line in fix_na_reps(inpt.lower(), encoding,\n",
    "                                               delimiter,\n",
    "                                               lineterminator).decode(\n",
    "                           \"utf8\", \"replace\").replace(\"�\", \"\").strip().encode(\n",
    "                           \"utf8\").splitlines()])\n",
    "    return [fix_dup_index(newsheet, encoding, hdr, delimiter, nfields)\n",
    "            if dup_index else newsheet][0]\n",
    "\n",
    "newsheets = [clean_bytes(row[1].bsheets, row[1].encoding, row[1].has_header,\n",
    "                                row[1].delimiter, row[1].lineterminator,\n",
    "                                row[1].dup_index, row[1].nfields)\n",
    "                    for row in tsheets.iterrows()]\n",
    "\n",
    "newsheets[0].splitlines()\n",
    "\n",
    "\n",
    "# udec = pd.read_csv(StringIO(unidecode(newsheet.decode())), sep = '\\t')\n",
    "# no_udec = pd.read_csv(StringIO(newsheet.decode()), sep = '\\t')\n",
    "# all(no_udec == udec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0].values == itm[1][1].values)]\n",
    "    return b'\\n'.join(b'\\t'.join(row[1].values.tolist()) for row in\n",
    "                      pd.concat((evdf[booltest],\n",
    "                       pd.Series([str(np.nan).encode()]*evdf.shape[1]),\n",
    "#                       pd.Series((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1)\n",
    "#                                 for row in evdf.iterrows()),\n",
    "                       oddf[booltest[-1]:]), axis = 1).iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # help(os.PathLike)\n",
    "\n",
    "# def fix_dup_index(\n",
    "#     inpt: Union[bytes, str, os.PathLike],\n",
    "#     encoding: str = None,\n",
    "#     hdr: bool = False,\n",
    "#     delimiter: bytes = None,\n",
    "#     nfields: int = None\n",
    "# ) -> bytes:\n",
    "#     inpt = get_bytes(inpt)\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "#     evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "#                   in evenodd(inpt.splitlines()))\n",
    "#     booltest = [itm[0] for itm in enumerate(\n",
    "#                    tuple(zip([itm[1] for itm in\n",
    "#                               evdf.iteritems()],\n",
    "#                               [itm[1] for itm in\n",
    "#                                oddf.iteritems()])))\n",
    "#                 if all(itm[1][0].values == itm[1][1].values)]\n",
    "#     datas = pd.concat((evdf[booltest],\n",
    "#                       pd.Series(bytes((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1), 'utf8')\n",
    "#                                 for row in evdf.iterrows()),\n",
    "#                       oddf[booltest[-1]:]), axis = 1)\n",
    "#     return b'\\n'.join(b'\\t'.join(itm for\n",
    "#                                  itm in row[1].values.tolist())\n",
    "#                       for row in datas.iterrows())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bytes(inpt: Union[bytes, str, os.PathLike, object]) -> bytes:\n",
    "#     \"\"\" Returns bytes from file either from memory or from reading \"\"\"\n",
    "#     if type(input) == object:\n",
    "#         return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                      itm in row[1].values.tolist())\n",
    "#                           for row in inpt.iterrows())    \n",
    "#     elif type(inpt) == bytes:\n",
    "#         return [inpt if bool(len(inpt.splitlines()) > \\\n",
    "#                   0 and inpt != None) else b\"1\"][0]\n",
    "#     elif type(inpt) == str:\n",
    "# #         try:\n",
    "#         with open(inpt, \"rb\", buffering=0) as myfile:\n",
    "#             outpt = myfile.read()\n",
    "#             if bool(len(myfile.read().splitlines()) > \\\n",
    "#                       0 and outpt != None):\n",
    "#                 return (outpt, myfile.close())[0]\n",
    "#             else:\n",
    "#                 return (b\"1\"[0], myfile.close())[0]\n",
    "# inpt = loadfiles(loadimages(xpu(zeprimes))).iloc[:, :-1]\n",
    "# tsheet = b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                itm in row[1].values.tolist())\n",
    "#                       for row in inpt.iterrows())\n",
    "# inpt.shape, tsheet.splitlines().__len__(), tsheet.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     return '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#             row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#             ['non'.encode(encoding)] + \\\n",
    "#             row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#             oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                         for row in evdf.fillna(\n",
    "#                                             'non'.encode(encoding)).iterrows())).replace(\n",
    "#             'non'.encode(encoding), str(np.nan).encode(encoding))\n",
    "\n",
    "# str(np.nan).encode(encoding)\n",
    "\n",
    "\n",
    "#     booltest = [itm[0] for itm in evdf.dropna().iteritems()\n",
    "#                 if all(itm[1].values == oddf.dropna()[itm[0]].values)]\n",
    "#     booltest = [itm[0][0] for itm in\n",
    "#                 tuple(zip(list(itm for itm in evdf.dropna().iteritems()),\n",
    "#                           list(itm for itm in oddf.dropna().iteritems())))\n",
    "#                if bool(all(itm[0][1].values) == all(itm[1][1].values))]\n",
    "#     tmp = [pd.Series(itm[0] + [str(np.nan).encode(encoding)] \\\n",
    "#                      + itm[1]).unique()\n",
    "#            if len(pd.Series(itm[0] + itm[1]).unique().tolist()) < nfields\n",
    "#            else pd.Series(itm[0] + itm[1]).unique()\n",
    "#            for itm in\n",
    "#            tuple(zip([line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[0]],\n",
    "#                     [line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[1]]))]\n",
    "#     return tmp\n",
    "#     evdf = df([line.decode().split()\n",
    "#                for line in tmp[0]])\n",
    "#     oddf = df([line.decode().split() for line in tmp[1]])\n",
    "\n",
    "#     return evdf.replace('None', np.nan)\n",
    "#     return pd.concat([evdf, df(oddf[[col[0] for col in\n",
    "#                                      enumerate(oddf.columns)\n",
    "#                                      if col[0] not in\n",
    "#                                      booltest]].iteritems())[1:]],\n",
    "#                      axis = 1)\n",
    "#pd.merge(evdf, oddf[list(oddf.columns)], on = booltest)\n",
    "# .drop(columns = booltest)\n",
    "#     booltest = [col for col in evdf.columns if\n",
    "#                 evdf[col].values.all() == oddf[col].values.all()]\n",
    "#     dup_test = [col for col in oddf.columns if evdf[col].values.all() == oddf[col].values.all()]\n",
    "\n",
    "#     return \"\\\\n\".join(\n",
    "#         [\n",
    "#             \"\\t\".join([itm for itm in line.split(=)])\n",
    "#             for line in newsheet.rename(\n",
    "#                 dict(enumerate(newsheet.columns))\n",
    "#             ).values.tolist()\n",
    "#         ]\n",
    "#     ).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(pd.Series)\n",
    "\n",
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes = cimaq_infos.loc[[row[0] for row in cimaq_infos.iterrows()\n",
    "                                   if 'onset' in row[1].filename]].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-495d19122c51>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m newsheets = [fix_dup_index(fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                row[1].encoding, row[1].has_header, row[1].delimiter, row[1].nfields)\n\u001b[1;32m      3\u001b[0m                              for row in cimaq_infos.iloc[:3, :].iterrows()]\n\u001b[1;32m      4\u001b[0m \u001b[0mtestdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewsheets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtestdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-495d19122c51>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m newsheets = [fix_dup_index(fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter),\n\u001b[0m\u001b[1;32m      2\u001b[0m                                row[1].encoding, row[1].has_header, row[1].delimiter, row[1].nfields)\n\u001b[1;32m      3\u001b[0m                              for row in cimaq_infos.iloc[:3, :].iterrows()]\n\u001b[1;32m      4\u001b[0m \u001b[0mtestdfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minpt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minpt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewsheets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtestdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cimaq_memory/sniffbytes.py\u001b[0m in \u001b[0;36mfix_dup_index\u001b[0;34m(inpt, encoding, hdr, delimiter, nfields)\u001b[0m\n\u001b[1;32m    519\u001b[0m     evdf, oddf = (df(line.split() for line in lines) for lines\n\u001b[1;32m    520\u001b[0m                   in evenodd(inpt.splitlines()))\n\u001b[0;32m--> 521\u001b[0;31m     booltest = [itm[0] for itm in enumerate(\n\u001b[0m\u001b[1;32m    522\u001b[0m                    tuple(zip([itm[1] for itm in\n\u001b[1;32m    523\u001b[0m                               evdf.iteritems()],\n",
      "\u001b[0;32m~/cimaq_memory/sniffbytes.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m                               [itm[1] for itm in\n\u001b[1;32m    525\u001b[0m                                oddf.iteritems()])))\n\u001b[0;32m--> 526\u001b[0;31m                 if all(itm[1][0].values == itm[1][1].values)]\n\u001b[0m\u001b[1;32m    527\u001b[0m     return pd.concat((evdf[booltest],\n\u001b[1;32m    528\u001b[0m                       pd.Series((int(len(row[1].values)) \\\n",
      "\u001b[0;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "newsheets = [fix_dup_index(fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter),\n",
    "                               row[1].encoding, row[1].has_header, row[1].delimiter, row[1].nfields)\n",
    "                             for row in cimaq_infos.iloc[:3, :].iterrows()]\n",
    "testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsheets = cimaq_infos.iloc[:3, :]\n",
    "# def fix_na_reps(inpt: bytes, encoding: str = None, delimiter: bytes = None) - > bytes:\n",
    "#     return '\\\\n'.encode(encoding).join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "#                       delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "#                       line) for line in inpt.splitlines())\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "\n",
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = snif.get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else snif.get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in snif.evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0] == itm[1][1])]\n",
    "    datas = pd.concat((evdf[booltest], pd.Series((int(len(row[1].values)) == nfields[0] -1)\n",
    "                                         for row in evdf.iterrows()),\n",
    "                     oddf[booltest[-1]:]), axis = 1)\n",
    "    return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "                               itm in row[1].values.tolist())\n",
    "                      for row in datas.iterrows())\n",
    "\n",
    "def fix_na_reps(inpt: bytes,\n",
    "                encoding: str = None,\n",
    "                delimiter: bytes = None,\n",
    "                lterm: bytes = None) -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    delimiter = [delimiter if delimiter else get_delimiter(inpt, encoding)][0]\n",
    "    lterm = [lterm if lterm else get_lineterminator(inpt)][0]\n",
    "    return lterm.join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "                      delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "                      line) for line in inpt.split(lterm))\n",
    "# newsheets = [fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter)\n",
    "#                              for row in testsheets.iterrows()]\n",
    "# testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "# testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'1'</td>\n",
       "      <td>b'CTL'</td>\n",
       "      <td>b'CTL0'</td>\n",
       "      <td>b'Control'</td>\n",
       "      <td>b'0.025'</td>\n",
       "      <td>b'3'</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'2'</td>\n",
       "      <td>b'Enc'</td>\n",
       "      <td>b'Enc00'</td>\n",
       "      <td>b'Encoding'</td>\n",
       "      <td>b'5.023'</td>\n",
       "      <td>b'3'</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'3'</td>\n",
       "      <td>b'Enc'</td>\n",
       "      <td>b'Enc000'</td>\n",
       "      <td>b'Encoding'</td>\n",
       "      <td>b'11.025'</td>\n",
       "      <td>b'3'</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'4'</td>\n",
       "      <td>b'CTL'</td>\n",
       "      <td>b'CTL01'</td>\n",
       "      <td>b'Control'</td>\n",
       "      <td>b'16.023'</td>\n",
       "      <td>b'3'</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'5'</td>\n",
       "      <td>b'Enc'</td>\n",
       "      <td>b'Enc01'</td>\n",
       "      <td>b'Old63'</td>\n",
       "      <td>b'Encoding'</td>\n",
       "      <td>b'20.035'</td>\n",
       "      <td>b'3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>b'116'</td>\n",
       "      <td>b'CTL'</td>\n",
       "      <td>b'CTL38'</td>\n",
       "      <td>b'Control'</td>\n",
       "      <td>b'713.858'</td>\n",
       "      <td>b'3'</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>b'117'</td>\n",
       "      <td>b'CTL'</td>\n",
       "      <td>b'CTL39'</td>\n",
       "      <td>b'Control'</td>\n",
       "      <td>b'721.364'</td>\n",
       "      <td>b'3'</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>b'118'</td>\n",
       "      <td>b'Enc'</td>\n",
       "      <td>b'Enc76'</td>\n",
       "      <td>b'Old58'</td>\n",
       "      <td>b'Encoding'</td>\n",
       "      <td>b'724.874'</td>\n",
       "      <td>b'3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>b'119'</td>\n",
       "      <td>b'Enc'</td>\n",
       "      <td>b'Enc77'</td>\n",
       "      <td>b'Old27'</td>\n",
       "      <td>b'Encoding'</td>\n",
       "      <td>b'731.879'</td>\n",
       "      <td>b'3'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>b'120'</td>\n",
       "      <td>b'Enc'</td>\n",
       "      <td>b'Enc78'</td>\n",
       "      <td>b'Old19'</td>\n",
       "      <td>b'Encoding'</td>\n",
       "      <td>b'735.891'</td>\n",
       "      <td>b'3'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0       1          2            3            4           5     6\n",
       "0      b'1'  b'CTL'    b'CTL0'   b'Control'     b'0.025'        b'3'  None\n",
       "1      b'2'  b'Enc'   b'Enc00'  b'Encoding'     b'5.023'        b'3'  None\n",
       "2      b'3'  b'Enc'  b'Enc000'  b'Encoding'    b'11.025'        b'3'  None\n",
       "3      b'4'  b'CTL'   b'CTL01'   b'Control'    b'16.023'        b'3'  None\n",
       "4      b'5'  b'Enc'   b'Enc01'     b'Old63'  b'Encoding'   b'20.035'  b'3'\n",
       "..      ...     ...        ...          ...          ...         ...   ...\n",
       "115  b'116'  b'CTL'   b'CTL38'   b'Control'   b'713.858'        b'3'  None\n",
       "116  b'117'  b'CTL'   b'CTL39'   b'Control'   b'721.364'        b'3'  None\n",
       "117  b'118'  b'Enc'   b'Enc76'     b'Old58'  b'Encoding'  b'724.874'  b'3'\n",
       "118  b'119'  b'Enc'   b'Enc77'     b'Old27'  b'Encoding'  b'731.879'  b'3'\n",
       "119  b'120'  b'Enc'   b'Enc78'     b'Old19'  b'Encoding'  b'735.891'  b'3'\n",
       "\n",
       "[120 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fid = fix_dup_index(all_dup_indexes.bsheets[55],\n",
    "              all_dup_indexes.encoding[55],\n",
    "              all_dup_indexes.has_header[55],\n",
    "              all_dup_indexes.delimiter[55])\n",
    "\n",
    "fid\n",
    "#     '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#         row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#         [str(np.nan).encode(encoding)] + \\\n",
    "#         row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#         oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                  for row in evdf.iterrows()))\n",
    "#  for line in evdf.iterrows()fid[0][0].values.tolist()]\n",
    "# all(fid[0][0][3].values == fid[0][1][3].values)\n",
    "df((snif.evenodd((line.split() for line in\n",
    "                  all_dup_indexes.iloc[55].bsheets.splitlines()))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fdi[0][3] == fdi[1][3]\n",
    "# def get_nfields(inpt: bytes, hdr: bool = None) -> bytes:\n",
    "#     inpt = [get_bytes(inpt).splitlines()[1:] if hdr else\n",
    "#             get_bytes(inpt).splitlines()][0]\n",
    "#     return pd.Series(len(line.split())\n",
    "#                       for line in inpt).max()\n",
    "#             if not hdr\n",
    "#             else pd.Series(len(line.split()) for line in\n",
    "#                            inpt.splitlines()[1:]).max()][0]\n",
    "# fix_dup_index(all_dup_indexes.bsheets[55])\n",
    "\n",
    "# [chr(itm) for itm in list(all_dup_indexes.delimiter[0])].__len__()\n",
    "# fxd = fix_dup_index()\n",
    "# fxd\n",
    "# all_dup_indexes['na_reps'] = [chr(list(row[1].bsheets)[-1])\n",
    "#                               for row in all_dup_indexes.iterrows()]\n",
    "\n",
    "\n",
    "# all_dup_indexes\n",
    "# all_dup_indexes['na_reps']\n",
    "# all_dup_indexes.bsheets['na_rep'] = [row[1].bsheets]\n",
    "#              encoding: str = None,\n",
    "#              hdr: bool = False,\n",
    "#              delimiter: bytes = None)\n",
    "# chardet.detect('�'.encode())\n",
    "# '�'.encode('utf32')\n",
    "# [line.split().__len__() for line in\n",
    "#  all_dup_indexes.bsheets[55].splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets'] = ['\\\\n'.encode(row[1].encoding).join(['\\t'.encode(row[1].encoding).join(\n",
    "                                   re.sub(b'\\\\s' + b'{2,}',\n",
    "                                          b'\\\\s',\n",
    "                                          re.sub(row[1].delimiter + b'{2,}',\n",
    "                                          row[1].delimiter + \\\n",
    "                                          str(np.nan).encode(row[1].encoding) + \\\n",
    "                                          row[1].delimiter,\n",
    "                                          line)) \\\n",
    "                                for line in row[1].bsheets.splitlines())])\n",
    "    for row in cimaq_infos.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes['incr_delim'] = [Counter([snif.get_delimiter(line, 'ascii')\n",
    "                                          for line in row[1]['bsheets'].splitlines()]).most_common(1)[0][0]\n",
    "                                 for row in all_dup_indexes.iterrows()]\n",
    "all_dup_indexes['incr_delim'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['force_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "                             for row in cimaq_infos.iterrows()]\n",
    "# with open(pjoin(os.getcwd(), 'test.txt'), 'wb') as newfile:\n",
    "#     newfile.write(cimaq_infos.iloc[124]['bsheets'])\n",
    "#     newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded'] = [row[1].force_utf8.decode()\n",
    "                          for row in cimaq_infos.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index']).decode(\n",
    "#                                     'utf8', 'replace').replace(\n",
    "#                                         '�', '').encode().decode().strip()\n",
    "                            \n",
    "                            \n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n'\n",
    "                                            ))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.iloc[667]['as_df']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2 = pd.concat(val.T for val in cimaq_infos[0].values.flatten())\n",
    "cimaq_infos2 = cimaq_infos2.dropna()\n",
    "cimaq_infos2['newsheets'] = [snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode()\n",
    "                             for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                             desc = 'repairing')]\n",
    "\n",
    "cimaq_infos2['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n'))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['as_df'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[99].as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['to_csv'] = [pd.read_csv(StringIO(row[1].unidecoded),\n",
    "                                      sep = '\\t',\n",
    "                                      header = [0 if row[1].has_header\n",
    "                                                else None][0],\n",
    "                                      engine = 'c')\n",
    "#                                       quoting = 1)\n",
    "                              for row in cimaq_infos2.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[667].to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['cleaned'] = ['\\n'.join(['\\t'.join(line.split()) for line in\n",
    "                                      snif.is_printable(row[1].newsheets.decode(\n",
    "                                          row[1].encoding).encode(\n",
    "                                          'utf8', 'replace').decode()).splitlines()]).encode().decode()\n",
    "                           for row in cimaq_infos2.iterrows()]\n",
    "cimaq_infos2['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pd.read_csv(StringIO(row[1].newsheets.decode(row[1].encoding)), sep = '\\t')\n",
    " for row in cimaq_infos2.iterrows()][666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(scanzip(loadimages(xpu(zeprimes))[0]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chardet.detect(b'0xf8')#0xf\n",
    "# chr(int.from_bytes(b'0xf', sys.byteorder))\n",
    "# bytes(int.from_bytes(b'0xf8', sys.byteorder))\n",
    "# import codecs\n",
    "# help(codecs)\n",
    "[chr(item) for item in list(b'0xf')]\n",
    "[itm for itm in list]\n",
    "int.from_bytes(b'0xf', sys.byteorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniffzip(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    scan_bytes(myzip.open(row[1].))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "        return pd.concat([df(snif.evenodd(repr(itm)[8:-1].strip().replace('=', ' ').split())).T[1]\n",
    "                          for itm in\n",
    "                (ZipFile(archv_path).__dict__['filelist'][1:],\n",
    "                ZipFile(archv_path).close())[0]\n",
    "                if '__MACOSX' not in repr(itm)], axis = 1).T\n",
    "\n",
    "# atest = list(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'][0].values())\n",
    "# btest = get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['filelist'][0]\n",
    "# atest# json.dumps(repr(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'].values.tolist()).split())\n",
    "# #.replace('=', '\":').replace('\\\\', '')\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])\n",
    "df(repr(itm[1])[8:-1].replace('=', ' ').split() for itm in\n",
    "      ZipFile(loadimages(xpu(zeprimes))[0]).__dict__['NameToInfo'].items()\n",
    "      if '__MACOSX' not in itm)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GOOD ONE ######################333\n",
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    return ((df(zip(tuple(itm.strip(\"'\") for itm in repr(\n",
    "               ZipFile(archv_path).getinfo(nm))[8:-1].strip().replace(\n",
    "                   b'='.decode(), ' ').split()#.replace(chr(34), '').split())\n",
    "                   for nm in snif.filter_lst_exc(\n",
    "                       exclude, [ntpl if ntpl\n",
    "                       else getnametuple(ZipFile(archv_path))][0])))),\n",
    "            ZipFile(archv_path).close())[0])\n",
    "\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])[0]\n",
    "#.replace('=', '\":').replace('\\\\', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_archv(apath: Union[str, os.PathLike], ntpl: Union[str, list, tuple] = []) -> object:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_contents(\n",
    "    archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = [],\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    withbytes: bool = False,\n",
    "    to_sniff: bool = False,\n",
    "    to_close: bool = True,\n",
    ") -> object:\n",
    "    myzip = ZipFile(archv_path)\n",
    "    ntpl = snif.filter_list_exc(exclude,\n",
    "                                [ntpl if ntpl else snif.getnametuple(myzip)][0])\n",
    "\n",
    "    vals = (\n",
    "        df(\n",
    "            tuple(\n",
    "                dict(zip(evenodd(itm)[0], evenodd(itm)[1]))\n",
    "                for itm in tuple(\n",
    "                    tuple(\n",
    "                        force_ascii(repr(itm.lower()))\n",
    "                        .strip()\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"=\", \" \")[:-2]\n",
    "                        .split()\n",
    "                    )[1:]\n",
    "                    for itm in set(\n",
    "                        repr(myzip.getinfo(itm))\n",
    "                        .strip(\" \")\n",
    "                        .replace(itm, itm.replace(\" \", \"_\"))\n",
    "                        if \" \" in itm\n",
    "                        else repr(myzip.getinfo(itm)).strip(\" \")\n",
    "                        for itm in ntpl\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            dtype=\"object\",\n",
    "        )\n",
    "        .sort_values(\"filename\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    vals[[\"src_name\", \"ext\"]] = [(nm, os.path.splitext(nm)[1]) for nm in ntpl]\n",
    "    vals[\"filename\"] = [\n",
    "        \"_\".join(\n",
    "            pd.Series(\n",
    "                row[1].filename.lower().replace(\"/\",\n",
    "                                                \"_\").replace(\"-\",\n",
    "                                                             \"_\").split(\"_\")\n",
    "            ).unique()\n",
    "            .__iter__()\n",
    "        )\n",
    "        for row in vals.iterrows()\n",
    "    ]\n",
    "    if exclude:\n",
    "        vals = vals.drop(\n",
    "            [\n",
    "                row[0]\n",
    "                for row in vals.iterrows()\n",
    "                if row[1].filename\n",
    "                not in filter_lst_exc(exclude, [itm.lower() for itm in vals.filename])\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    if withbytes:\n",
    "        vals[\"bsheets\"] = [\n",
    "#             snif.strip_null() \n",
    "            myzip.open(row[1].src_name).read().lower() for row in vals.iterrows()\n",
    "        ]\n",
    "        \n",
    "    if to_sniff:\n",
    "        vals[[\"encoding\", \"delimiter\", \"has_header\", \"width\", \"dup_index\", \"nrows\"]] = \\\n",
    "            [tuple(snif.scan_bytes(row[1].bsheets).values()) for row in vals.iterrows()]\n",
    "    if to_close:\n",
    "        myzip.close()\n",
    "        return vals\n",
    "    else:\n",
    "        return (myzip, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azpath = loadfiles(loadimages(xpu(zeprimes))).fpaths[0]\n",
    "# with zipfile.Zipfile(azpath) as myzip:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  dict((bname(apath.replace('-', '_').strip()),\n",
    "                    get_zip_contents(apath, withbytes = True,\n",
    "                         to_sniff = True, to_close = True))\n",
    "                    for apath in tqdm(loadimages(zeprimes),\n",
    "                          desc = 'scan sniff'))\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sniffbytes import no_ascii\n",
    "# # chardet.detect(no_ascii('bon matin tin�tin!').encode())\n",
    "# # Union[str, bytes]\n",
    "# ''.join([chr(int.from_bytes(int.from_bytes(itm.encode(), sys.byteorder)))#.encode(encoding)\n",
    "#  for itm in ])\n",
    "# # ==  dict(json.dumps(set(string.printable)))\n",
    "set([chr(int.from_bytes(itm, sys.byteorder)).encode('utf32')\n",
    " for itm in [itm.encode() for itm in list(string.printable)]])\n",
    "[int.from_bytes(itm.encode(), sys.byteorder) for itm in list(string.printable)[-2:]]\n",
    "# '\\x00'.encode('ISO-8859-1')\n",
    "encodings = ['Windows-1252', 'utf32', 'utf8', 'ISO-8859-1']\n",
    "[int.from_bytes(itm.encode(encoding), sys.byteorder) for itm in list(string.printable)]\n",
    "int.from_bytes(b'', sys.byteorder)\n",
    "chardet.detect(b'\\x00'), chardet.detect(b'None')\n",
    "is_printable = [itm.encode('utf32') for itm in list(string.printable)], '\\x00'.encode('ascii')\n",
    "\n",
    "def force_encoding(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "    \"\"\"\n",
    "    notnull_printable = set(itm.encode(encoding) for itm in list(string.printable)\n",
    "                            if int.from_bytes(itm, sys.byteorder) != 0)\n",
    "#                             if chr(int.from_bytes(itm, sys.byteorder)) != \"\\x00\")\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    return \"\".encode(encoding).join(filter(lambda x: x in notnull_printable, inpt))\n",
    "\n",
    "\n",
    "# is_printable = , '\\x00'.encode('ascii')\n",
    "# '0xff'.encode('utf32') in is_printable\n",
    "utf32p = ''.encode('utf32').join(set(itm.encode('utf32') for itm in list(string.printable)))\n",
    "windows1252p = ''.encode('Windows-1252').join(set(itm.encode('Windows-1252') for itm in list(string.printable)))\n",
    "utf8p = list(itm.encode('utf8') for itm in list(string.printable))\n",
    "utf32p = list(itm.encode('utf32') for itm in list(string.printable))\n",
    "test = set(itm.encode('utf32') for itm in list(string.printable)\n",
    "           if int.from_bytes(itm.encode('utf32'),\n",
    "                                 'little') != '\\x00'.encode('utf32'))\n",
    "# test == utf32set\n",
    "# force_encoding(utf8p, 'Windows-1252')\n",
    "# chr(0)\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in utf32set]\n",
    "\n",
    "astring = \"    Salut\\tbébé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\"\n",
    "chardet.detect((\"Salut bébé, mon nom c'est François!\").encode())\n",
    "# weird = list(chr(itm) for itm in tuple(''.join(utf32set).encode()))[-5]\n",
    "# astring.encode('ascii')\n",
    "\n",
    "# clean_astring = ''.join(itm for itm in list(astring) if\n",
    "#                         int.from_bytes(itm.encode('utf32'), sys.byteorder) != 0)\n",
    "\n",
    "def clean_bytes(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    return ''.join(itm for itm in list(inpt) if\n",
    "                        chr(int.from_bytes(itm)) != chr(0).encode(encoding) and itm in\n",
    "                       set(itm.encode(encoding) for itm in list(string.printable)))\n",
    "# chr(int.from_bytes(weird.encode('ascii'), sys.byteorder))\n",
    "# list(astring)\n",
    "# int.from_bytes('\\\\x00'.encode(), 'big')\n",
    "# int.from_bytes('\\\\x00'.encode(), sys.byteorder) == int.from_bytes('\\\\x00'.encode(), 'little')\n",
    "maybe_null == notnull_printable\n",
    "(astring, clean_astring)\n",
    "# '\\x00'.encode('utf32')\n",
    "nareps = [chr(0).encode('ascii'), chr(0).encode('utf32'), chr(0).encode('ISO-8859-1')]\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in nareps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astring = \"    \\\\\\tSalut\\tbé bé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\\S\"\n",
    "bstring = astring.encode('utf16')\n",
    "narep = chr(0).encode(snif.get_bencod(bstring))\n",
    "chr(0).encode('utf16') == chr(0).encode(snif.get_bencod(bstring))\n",
    "[chr(itm) for itm in list(bstring)]# chardet.detect('ç'.encode())\n",
    "# [chr(itm) for itm in list(bstring) if chr(itm) != ]\n",
    "narep, bstring, bstring.replace(narep, ''.encode(snif.get_bencod(bstring)))\n",
    "tuple(zip(list(astring), list(bstring)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(int.from_bytes(''.encode(), sys.byteorder)).encode('utf32'), chr(int.from_bytes('0'.encode(), sys.byteorder)).encode('utf32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import bytes\n",
    "# # import format as fmt\n",
    "\n",
    "# def strip_null(inpt: bytes, nullrep: bytes = None, encoding: str = None):\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     nullrep = [nullrep if nullrep else snif.get_nullrep(inpt, encoding)][0]\n",
    "#     return {\n",
    "#         format(print(\"[%q]\",\n",
    "#                    bytes.Trim(byte(\" !!! Achtung! Achtung! !!! \"), \"! \")))}\n",
    "\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "#         ''.join(get_nullrep(inpt, encoding)), '').encode()\n",
    "    try:\n",
    "        return inpt.replace(chr(int.from_bytes(\n",
    "                   b\"\\x00\", sys.byteorder)).encode(encoding), ''.encode(encoding))\n",
    "    except UnidecodeError:\n",
    "        return inpt.replace(get_nullrep(inpt, encoding), ''.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbytes = b'bonjour\\xff\\xfe\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "chardet.detect(snif.strip_null(testbytes))#.encode('ISO-8859-1')\n",
    "chardet.detect(testbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nullrep(inpt, encoding: str = None) -> bytes:\n",
    "#     return [itm for itm in list(inpt) if\n",
    "#             chr(int.from_bytes(itm, sys.byteorder)).encode(encoding) == \"\\x00\".encode(encoding)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "help(codecs.lookup('utf8').streamreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest = [dict((subject[0], \n",
    "                df.from_dict(dict((itm for itm in\n",
    "                                   list(snif.scan_bytes(snif.force_utf8(\n",
    "                  row[1].bsheets, row[1].encoding)).items()) + \\\n",
    "                      [('filename', row[1].filename),\n",
    "                       (('pscid', 'dccid'), row[1].filename.split('_')[:2])])),\n",
    "                             orient = 'index'))\n",
    "               for row in tqdm(subject[1].iterrows(), desc = 'new sniff'))\n",
    "           for subject in list(test.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtest\n",
    "# # both = {**test, **newtest}\n",
    "# both = tuple({**itm[0], **itm[1]} for itm\n",
    "#              in )\n",
    "# both\n",
    "# both = tuple(zip([itm.values() for itm in test], [itm.values() for itm in newtest]))\n",
    "# both\n",
    "tuple(zip(test.values(), newtest.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_xtrct(archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = None,\n",
    "    to_xtrct: Union[str, list, tuple] = 'all',\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    to_close: bool = True,\n",
    "    withbytes: bool = False,\n",
    "    dst_path: Union[os.PathLike, str] = None) -> object:\n",
    "    dst_path = [dst_path if dst_path\n",
    "                else pjoin(os.get_cwd(),\n",
    "                           os.path.splitext(bname(archv_path))[0])][0]\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "    myzip = zipfile.ZipFile(archv_path)\n",
    "    ntpl = filter_lst_exc([ntpl if ntpl else getnametuple(myzip)][0])\n",
    "    contents = get_zip_contents(archv_path, ntpl, excllude, to_xtrct,\n",
    "                                to_close = True, withbytes = True, to_sniff = True)\n",
    "    xtrct_lst = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename\n",
    "            in filter_lst_inc(to_xtrct, list(vals.filename), sort=True)\n",
    "        ]\n",
    "    ]\n",
    "    [\n",
    "        shutil.move(\n",
    "            myzip.extract(member=row[1].src_name, path=dst_path),\n",
    "            pjoin(\n",
    "                dst_path,\n",
    "                \"_\".join(\n",
    "                    pd.Series(\n",
    "                        row[1].filename.lower().replace(\"-\", \"_\").split(\"_\")\n",
    "                    ).unique()\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        for row in tqdm(xtrct_lst.iterrows(), desc=\"extracting\")\n",
    "    ]\n",
    "    vals = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename not in xtrct_lst.values\n",
    "        ]\n",
    "    ]\n",
    "    removeEmptyFolders(dst_path, False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_printables = [chr(int.from_bytes(itm.encode(), sys.byteorder)).encode() for itm in string.printable]\n",
    "str(b_printables[-1]) == repr(b_printables[-1])\n",
    "\n",
    "def force_to(inpt, src_enc: str = None, dst_enc: str = 'utf8'):\n",
    "    inpt = get_bytes(inpt)\n",
    "    as_ints = [[line.split()]]\n",
    "    \n",
    "    \n",
    "chr(list(asheet)[0])\n",
    "list(asheet).__len__()\n",
    "# chr(int.from_bytes(b_printables[-1], sys.byteorder))\n",
    "\n",
    "# [chr(itm) for itm in list(asheet)]\n",
    "# help(chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_topdir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/'\n",
    "# big = loadfiles([apath for apath in loadimages(xpu(cimaq_topdir))\n",
    "#                  if os.path.isfile(apath)])\n",
    "# big[['dccid', 'pscid']] = [([re.compile('\\d{6}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{6}').search(row[1].fname) != None\n",
    "#                              else None][0],\n",
    "#                             [re.compile('\\d{7}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{7}').search(row[1].fname) != None\n",
    "#                              else None][0])\n",
    "#                            for row in big.iterrows()]\n",
    "'\\\\x0'.encode('utf16')\n",
    "chr(int.from_bytes(b\"\\0xff\", sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(asheet)\n",
    "\n",
    "# test2 = pd.concat([itm[1] for itm in tqdm(sorted(list(test)), desc = 'concatenate')])\n",
    "\n",
    "# test2['forced_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "#                         for row in test2.iterrows()]\n",
    "\n",
    "#.filename,\n",
    "#                    snif.scan_bytes(row[1].forced_utf8, encoding='utf'))\n",
    "#                         for row in itm[1].iterrows())\n",
    "#              for itm in tqdm(list(test.items()),  desc = 'scan & sniff utf8')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert newtest.loc['encoding'].all() == 'ascii' or 'UTF-8'\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder))\n",
    "\n",
    "b'n\\a' == 'n\\a'.encode()\n",
    "\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder)).encode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_item(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Returns null byte representation as bytes in native file encoding'''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "#     rep = chr(list(inpt)[-1]).encode(encoding)\n",
    "    last1 = inpt.splitlines()[-1].split()[-1]\n",
    "    last2 = [chr(itm).encode(encoding) for itm in\n",
    "             list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]\n",
    "    return (last1, last2)\n",
    "#     return [chr(itm) for itm in list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(chr(int.from_bytes(b'\\x00', sys.byteorder)))\n",
    "# snif.get_bencod(chr(0))\n",
    "int.from_bytes(b'\\x00', sys.byteorder)\n",
    "aguy = random.sample(list(test.values()), 1)\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aguy = list(test.values())[0]\n",
    "aguy['size_check'] = [int(row[1].file_size) == len(list(row[1].bsheets))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy['na_reps'] = [snif.get_nullrep(row[1].bsheets) for row in aguy.iterrows()]\n",
    "aguy['last_item'] = [last_item(row[1].bsheets, row[1].encoding) for row in aguy.iterrows()]\n",
    "\n",
    "aguy[['n_zbytes', 'n_scanbytes', 'chkup']] = [(int(len(list(row[1].bsheets))), int(row[1].file_size),\n",
    "                                      (int(len(list(row[1].bsheets))) == int(row[1].file_size)))\n",
    "                                     for row in aguy.iterrows()]\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aguy['utf8len'] = [len(row[1].forced_utf8.splitlines()) for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_tophalf'] = [row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))] == \\\n",
    "                                 row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]\n",
    "                              for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_bothalf'] = [[line.strip() for line in\n",
    "                               row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))]] == \\\n",
    "                                 [line.strip() for line in\n",
    "                                  row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]]\n",
    "                              for row in aguy.iterrows()]\n",
    "\n",
    "aguy['nrows_test'] = [(len(row[1].bsheets.splitlines()) == len(row[1].forced_utf8.splitlines()))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy[['missing_line_index', 'missing_line_values']] = [(row[1].nrows_test)*2 if row[1].nrows_test else\n",
    "                        sorted([(line[0], line[1]) for line in enumerate(row[1].bsheets.splitlines()) if\n",
    "                                snif.force_utf8(line[1], 'utf8') not in row[1].forced_utf8.splitlines()])\n",
    "                       for row in aguy.iterrows()]\n",
    "# aguy['eq_lines'] = [len([line[0] for line in enumerate(tuple(zip(row[1].bsheets.splitlines(),\n",
    "#                       row[1].forced_utf8.splitlines()))) if line[1][0] == line[1][1]])\n",
    "#                     == len\n",
    "#                     for row in aguy.iterrows()]\n",
    "\n",
    "aguy.last_item.iloc[6][1][0].decode('utf16').encode('utf8').decode('utf8')\n",
    "aguy.forced_utf8, aguy.bsheets\n",
    "aguy['missing_line'].iloc[5]\n",
    "# aguy.iloc[5].bsheets.splitlines(), aguy.iloc[5].forced_utf8.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asheet = aguy.iloc[4].bsheets[0]\n",
    "utfsheet = aguy.iloc[4].forced_utf8[0]\n",
    "asheet == utfsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asheet = aguy.sample(1).bsheets.values[0]\n",
    "# last_item(asheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = snif.get_bencod(abyte)\n",
    "nonull = strip_null(abyte, enc)\n",
    "nonull.splitllines()[:4], abyte.splitlines()[:4]\n",
    "# aguy['no_null'] = [strip_null(row[1].bsheets,\n",
    "#                               row[1].encoding)#.encode(row[1].encoding)\n",
    "#                    for row in aguy.iterrows()]\n",
    "# aguy['bprints'] = [snif.bytes_printable(row[1].bsheets).encode(snif.get_bencod(row[1].bsheets))\n",
    "#                  for row in aguy.iterrows()]\n",
    "# checkfx = []\n",
    "\n",
    "\n",
    "# list(abyte.splitlines()[-1])\n",
    "# [chr(itm).encode() for itm in\n",
    "#  [list(row[1].bsheets.splitlines()[-1])[-1]\n",
    "# for row in aguy.iterrows()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = list(test.values())[0].iloc[0].bsheets\n",
    "encoding = list(test.values())[0].iloc[0].encoding\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    return '\\n'.join(inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "        ''.join(snif.get_nullrep(inpt, encoding)), '').splitlines())\n",
    "\n",
    "pd.read_csv(StringIO(snif.mkfrombytes(inpt.strip()).decode()), sep = '\\t')\n",
    "#     narep =''.encode(encoding).join([itm.encode(encoding) for itm in\n",
    "#                                            snif.get_nullrep(inpt, encoding)])\n",
    "#     return inpt.replace(narep, '|'.encode(encoding))\n",
    "#         inpt = inpt.replace(narep.encode(encoding),\n",
    "#                             '|'.encode(encoding)).replace('|'.encode(encoding),\n",
    "#                                                           ''.encode(encoding))\n",
    "#         return inpt\n",
    "#     return snif.bytes_printable(inpt.replace(,\n",
    "#                                              repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "# snif.mkfrombytes(strip_null(inpt, 'utf8').encode())\n",
    "# pd.read_csv(StringIO(snif.mkfrombytes(inpt).decode(encoding)),\n",
    "#             sep='\\t', header = [0 if snif.get_has_header(strip_null(inpt).encode(encoding)) else None][0])\n",
    "# inpt.decode(snif.get_bencod(strip_null(inpt)), 'ignore').replace('�', '')\n",
    "# pd.read_csv(StringIO(), sep='\\t')\n",
    "# [[list(itm for itm in line.split() if itm not in nareps)]\n",
    "#  for line in snif.bytes_printable(inpt)]\n",
    "# def get_na_reps(inpt: bytes, encoding: str = None) -> Union[list, int, bytes]:\n",
    "#     inpt = snif.bytes_printable(inpt)\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)]\n",
    "#     return [chr(itm).encode(encoding) for itm\n",
    "#             in [chr(nulb) for nulb in\n",
    "#                 list(snif.get_nullrep(snif.bytes_printable(inpt)))][0]]\n",
    "\n",
    "# get_na_reps(inpt)\n",
    "# # [list(''.encode(encoding).join((line.split())))\n",
    "# #  for line in inpt.splitlines()]\n",
    "# chr(int.from_bytes('0xf8', sys.byteorder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 3+5 ==9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_replace import multiple_replace\n",
    "\n",
    "def clear_nullbytes(inpt: bytes, encoding: str = None):\n",
    "    inpt = snif.bytes_printable(inpt)\n",
    "    encoding = snif.get_bencod(inpt)\n",
    "    toclear = dict((itm.encode(encoding), '|'.encode(encoding))\n",
    "                   for itm in\n",
    "                   )\n",
    "\n",
    "\n",
    "    for narep in nareps:\n",
    "        inpt = inpt.replace(narep, '|'.encode(encoding))\n",
    "    return inpt\n",
    "#     return inpt.replace('|'.encode(encoding), 'nan'.encode(encoding))\n",
    "# multiple_replace(toclear, snif.bytes_printable(abyte), encoding)\n",
    "# help(str.replace)\n",
    "# b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "# b_printable\n",
    "\n",
    "inpt = list(test.values())[0].iloc[8].bsheets\n",
    "encoding = list(test.values())[0].iloc[8].encoding\n",
    "# new = snif.bytes_printable(abyte).replace(tostrip, ''.encode(encoding))\n",
    "clear_nullbytes(inpt, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def bytes_prntble(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Same as is_printable, but for bytes in native file encoding '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "\n",
    "#     b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "#     return ''.encode(encoding).join([str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding) for ch in \n",
    "#                                     list(inpt) if str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding)\n",
    "#                                     in b_printable])\n",
    "    \n",
    "# def get_nullrep(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Returns null byte representation as bytes in native file encoding'''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "\n",
    "# def strip_null(inpt: bytes, encoding: str = None, replace_val: str = None) -> bytes:\n",
    "#     ''' Remove null bytes from byte stream with proper representation\n",
    "#         Adapted from:\n",
    "#         https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "#         All files end by a null byte, so the last byte in a file shows\n",
    "#         how null bytes are represented within this file '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     repval = ['' if not replace_val else replace_val][0].encode(encoding)\n",
    "#     return bytes_prntble(inpt.replace(get_nullrep(inpt, encoding), repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "\n",
    "# snif.get_nullrep(abyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abyte = list(test.values())[0].iloc[1].bsheets\n",
    "enc = snif.get_bencod(abyte)\n",
    "# (abyte.decode(enc, 'replace').splitlines()[0], repr(abyte.decode(enc, 'replace').splitlines()[0]),\n",
    "#  repr(snif.force_utf8(abyte).splitlines()[0].decode())\n",
    "# )\n",
    "repr(abyte.decode(enc, 'replace'))\n",
    "def to_utf8(astring):\n",
    "    return unidecode(repr(astring.decode(enc, 'replace').replace('�', '').encode(\n",
    "              'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', ''))).encode()\n",
    "# len(to_utf8(abyte).splitlines()\n",
    "# )\n",
    "len(abyte.splitlines())\n",
    "new = b'\\n'.join([b'\\t'.join([to_utf8(itm) for itm in to_utf8(line).split()])\n",
    "            for line in abyte.splitlines()]).decode()\n",
    "\n",
    "df([' '.join(line.split(\"\\\\x00\")).split() for line in new.splitlines()])\n",
    "# clean_utf8 = '\\n'.join([['\\t'.join([to_utf8(itm) for itm in line.split()])]\n",
    "#               for line in abyte.splitlines()])\n",
    "# clean_utf8\n",
    "# newsheet = repr(abyte.replace(bytes(str(chr(0)), enc),\n",
    "#               str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x0'\", enc), str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x'\", enc), ''.encode(enc)).replace(\n",
    "#                       bytes(str(np.nan), enc), str(\"\").encode(enc)).decode(\n",
    "#                           'ascii', 'replace').replace('�', '').replace('ÿ', ''))\n",
    "\n",
    "# cleaned = '\\n'.join(['\\t'.join([itm.replace('\\\\x0', ' ').strip().replace('\\\\x', ' ').strip() for itm in line.split()])\n",
    "#            for line in repr(newsheet).replace('\\\\x00', ' ').splitlines()]).strip()[1:].encode().decode()\n",
    "# pd.read_csv(StringIO(unidecode(cleaned)), sep='\\t')\n",
    "# [line.replace('\\\\x00', str(np.nan)).replace('\\\\x0', str(np.nan)) for line in newsheet.splitlines() if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abyte = list(test.values())[0].iloc[2].bsheets\n",
    "# encod = list(test.values())[0].iloc[2].encoding\n",
    "\n",
    "\n",
    "    \n",
    "enc = snif.get_bencod(snif.strip_null(abyte))\n",
    "# snif.strip_null(abyte)\n",
    "# [strip_null(line) for line in strip_null(bytes_prntble(abyte)).splitlines(keepends = True)]\n",
    "\n",
    "# get_nullrep(abyte)+b'1'\n",
    "# # int.from_bytes(b'\\\\x0', 'little'), int.from_bytes(b'\\\\x0', sys.byteorder), int.from_bytes(b'\\\\x0', 'big')\n",
    "\n",
    "# # newsheet = '\\n'.join(['\\t'.join([str(itm).replace(\"'\", \"\").replace(\"'\", '\"') for itm in\n",
    "# #                                  repr(line).replace('ÿ', '\\s').replace('\\\\x0', '\\s').replace(\n",
    "# #                                      '\\\\x', '\\s').replace('\\s', '').split()])\n",
    "# #                       for line in list(test.values())[0].iloc[5].bsheets.decode(\n",
    "# #                encod, 'replace').splitlines()]).splitlines()\n",
    "# # newsheet\n",
    "# # strtest = abyte.splitlines()[0]\n",
    "\n",
    "# repr(abyte.replace(repr(chr(0)).encode(encod), ''.encode(encod)).decode(encod, 'replace').replace('�', '')).encode(\n",
    "#     'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', '').encode()\n",
    "# abyte.decode('ISO-8859-1').encode('ascii', 'ignore').decode('utf8', 'replace').replace('�', '')\n",
    "\n",
    "# unidecode(repr(abyte.decode(encod, 'replace').replace('�', '')))\n",
    "# is_printable(abyte.decode(encod, 'replace').replace('�', '').encode().replace(chr(0).encode(), ''.encode()).decode())\n",
    "\n",
    "# # new = [[unidecode(itm).encode('ascii', 'ignore').decode('ascii', 'replace').replace('�', '')\n",
    "# #         for itm in line.replace(repr(chr(0)),\n",
    "# #                                '').split()\n",
    "# #       if unidecode(is_printable(itm)) != '']\n",
    "# #      for line in abyte.decode(encod, 'replace').replace('�', '').splitlines()\n",
    "# #      if line != []]\n",
    "# # new[0], ''.join(new[0]).replace(repr(chr(0)).encode(encod), ' '.encode(encod))\n",
    "# new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in string.printable #list(string.printable)\n",
    "\n",
    "chr(int.from_bytes(b'z', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(int.from_bytes(b'\\\\x00', 'big'))\n",
    "[chr(itm).encode() for itm in list(string.printable)]\n",
    "# chr(int.from_bytes('\\\\x0'.encode('Windows-1252'), sys.byteorder))\n",
    "# chr(int.from_bytes(b'\\\\x00', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# abyte.decode('utf8', 'replace').replace('�', '').encode().decode()\n",
    "# unidecode(abyte.decode(snif.get_bencod(abyte), 'replace').replace('�', '')).encode()\n",
    "snif.force_utf8(abyte, enc).splitlines()\n",
    "clean1 = re.sub('\\\\x00', '', abyte.decode('ascii', 'replace').replace('�', '').encode('utf16').decode('utf8', 'replace'))\n",
    "# unidecode(clean1.encode().decode()).encode('utf16', 'replace')\n",
    "clean1\n",
    "[re.sub(bytes(str(np.nan), enc), bytes('', enc),\n",
    "        re.sub(bytes(\"\\x00\", enc),\n",
    "               bytes(str(np.nan), enc),\n",
    "               line)).decode('utf8', 'replace').replace('�', '').replace('\\\\', '')#unidecode(line.decode(enc, 'replace'))).encode('ascii', 'replace').decode('utf8', 'replace').replace('�', '')\n",
    "  for line in abyte.splitlines()][0].split()[0]\n",
    "# snif.force_utf8(abyte.decode(enc, 'replace'))\n",
    "#.decode(enc, 'replace')]\n",
    "# re.sub('\\x0', '', clean1)\n",
    "# unidecode(\n",
    "#           .replace('\\x00', '').replace(\n",
    "#     '\\\\x0', '').replace('\\\\x', ''))\n",
    "#.replace(b\"\\x\".decode(), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(0)\n",
    "# str(\"\\xff\")\n",
    "chr(1).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes('n\\a', 'Windows-1252')\n",
    "unidecode(b'\\x0b\\x0c'.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_tsvs(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths'])`.set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                        [loadfiles(loadimages(row[1].fpaths)).dropna(axis = 0).T\n",
    "#                         for row in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[2: 4].iterrows()]).T\n",
    "#     test = dict((grp, dict((sgrp, cimaq.groupby('subids').get_group(grp).groupby(\n",
    "#                'fname').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "#                    'subids').get_group(grp).groupby('fname').groups))\n",
    "#                 for grp in allscans.groupby('subids').groups)\n",
    "# #     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "# #                       [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "# #                 columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "# #                                   for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "# #                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "# #     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'stereonl', 4: 'behavioral',\n",
    "# #                                     5: 'confounds', 6: 'events', 7:'func'})\n",
    "#     return test\n",
    "# cimaq = load_tsvs(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq = fetch_cimaq(xpu(cimaq_dir))\n",
    "#cimaq.sort_values('dccid').set_index('dccid', drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_cimaq(cimaq_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_cimaq_dir_paths(\n",
    "#                     cimaq_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_cimaq_dir_paths(\n",
    "#                             cimaq_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths']).set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_cimaq_dir_paths(cimaq_dir)[0].zeprimes.fpaths,\n",
    "#                    get_cimaq_dir_paths(cimaq_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(cimaq_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + [loadfiles(loadimages(cimaqpath)).dropna(axis = 0)['fpaths']\n",
    "#                                   for cimaqpath in get_cimaq_dir_paths(cimaq_dir)[0].loc['fpaths'][2: 5]],\n",
    "#                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "#     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'behavioral', 4: 'confounds',\n",
    "#                                     5: 'events'})\n",
    "#     return cimaq.set_index('dccid').sort_index()#.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scans(cimaq_dir):\n",
    "    cimaq = fetch_cimaq(cimaq_dir)\n",
    "\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                   [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "#             columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "#                               for cimaqrow in get_cimaq_dir_paths(cimaq_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "#                   axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "    \n",
    "    allscans = pd.concat([loadfiles(loadimages(pjoin(dname(xpu(cimaq_dir)),\n",
    "                                                     sub))).dropna(axis = 0).T\n",
    "                          for sub in [itm for itm in ls(dname(xpu(cimaq_dir)))\n",
    "                        if itm.startswith('sub-')]], axis = 1).T\n",
    "    allscans[['dccid', 'modality', 'general']] = [(bname(row[1].fpaths).split('_')[0].split('-')[1],\n",
    "                                                   bname(row[1].fpaths).split('_')[-1],\n",
    "                                                   bname(dname(row[1].fpaths)))\n",
    "                         for row in allscans.iterrows()]\n",
    "\n",
    "    test = dict((grp, dict((sgrp, allscans.groupby('dccid').get_group(grp).groupby(\n",
    "               'general').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "                   'dccid').get_group(grp).groupby('general').groups))\n",
    "                for grp in allscans.groupby('dccid').groups)\n",
    "    allscans = df.from_dict(test, orient = 'index').sort_index()\n",
    "    indexes = set.intersection(set(cimaq.index), set(allscans.index)) \n",
    "    return pd.concat([allscans.loc[indexes], cimaq.loc[indexes]], axis = 0)\n",
    "test = load_scans(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.iloc[0].fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [pd.concat([row[1][['fname']], df.from_dict(json_read(row[1].fpaths, 'r'), orient = 'index')])\n",
    "#           for row in test[0].iterrows()\n",
    "#           if row[1].ext == '.json']\n",
    "# # allparams = pd.concat(((itm.groupby('fname').get_group(grp)\n",
    "# #                  for grp in itm.groupby('fname').groups)\n",
    "# #              for itm in allparams), axis = 1)\n",
    "# # pd.concat(params, axis =1).T.sort_values('fname')\n",
    "# pd.concat(params, axis = 1).T.set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "    \"\"\"\n",
    "    Context manager to set the timestamp of the path to plus or\n",
    "    minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "    if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "    \"\"\"\n",
    "    stats = os.stat(path)\n",
    "    if set_new:\n",
    "        new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "    else:\n",
    "        new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
