{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import chardet\n",
    "import csv\n",
    "import json\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex as re\n",
    "import string\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from chardet import UniversalDetector as udet\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from string import printable\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from unidecode import unidecode\n",
    "from operator import itemgetter\n",
    "\n",
    "from sniffbytes import flatten\n",
    "from sniffbytes import loadfiles\n",
    "from sniffbytes import loadimages\n",
    "\n",
    "from bidsify_utils import bidsify_load_scans\n",
    "\n",
    "from json_read import json_read\n",
    "\n",
    "import sniffbytes as snif\n",
    "\n",
    "from scanzip import scanzip\n",
    "\n",
    "\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "from multiple_replace import multiple_replace\n",
    "\n",
    "cimaq_dir = xpu('~/../../data/simexp/datasets/cimaq_03-19/')\n",
    "drv_dir = pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/94 [00:00<?, ?it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 951.14it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 833.73it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 782.52it/s]\n",
      "  3%|▎         | 3/94 [00:00<00:03, 28.71it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1204.80it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1502.66it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1209.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1068.88it/s]\n",
      "  7%|▋         | 7/94 [00:00<00:02, 29.98it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1501.99it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1187.18it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 859.43it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1518.44it/s]\n",
      " 12%|█▏        | 11/94 [00:00<00:02, 30.40it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 887.50it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1247.44it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1448.14it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 2019.08it/s]\n",
      " 16%|█▌        | 15/94 [00:00<00:02, 31.95it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1264.11it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1047.99it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1253.65it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1431.67it/s]\n",
      " 20%|██        | 19/94 [00:00<00:02, 32.50it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1340.03it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 5/5 [00:00<00:00, 1784.35it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1325.11it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 937.07it/s]\n",
      " 24%|██▍       | 23/94 [00:00<00:02, 32.61it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2025.74it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 760.53it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1631.18it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1123.12it/s]\n",
      " 29%|██▊       | 27/94 [00:00<00:02, 33.24it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1652.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1399.03it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1472.03it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 871.45it/s]\n",
      " 33%|███▎      | 31/94 [00:00<00:01, 31.55it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1286.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 2384.48it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1533.99it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1138.67it/s]\n",
      " 37%|███▋      | 35/94 [00:01<00:01, 30.16it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1227.60it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1533.85it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 983.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1350.97it/s]\n",
      " 41%|████▏     | 39/94 [00:01<00:01, 31.07it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2354.04it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 859.62it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 903.31it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1055.17it/s]\n",
      " 46%|████▌     | 43/94 [00:01<00:01, 29.79it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 961.83it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 969.93it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1253.72it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 931.91it/s]\n",
      " 50%|█████     | 47/94 [00:01<00:01, 30.29it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1313.49it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 986.20it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1456.86it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1311.54it/s]\n",
      " 54%|█████▍    | 51/94 [00:01<00:01, 28.97it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1301.27it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1259.80it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1471.17it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 2028.85it/s]\n",
      " 59%|█████▊    | 55/94 [00:01<00:01, 31.39it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1318.96it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 749.99it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 958.31it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1255.65it/s]\n",
      " 63%|██████▎   | 59/94 [00:01<00:01, 31.51it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1986.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1173.78it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1592.11it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1338.43it/s]\n",
      " 67%|██████▋   | 63/94 [00:02<00:01, 30.29it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1164.84it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1079.06it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1252.65it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 2080.16it/s]\n",
      " 71%|███████▏  | 67/94 [00:02<00:00, 31.52it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1767.88it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1270.74it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1059.70it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 782.03it/s]\n",
      " 76%|███████▌  | 71/94 [00:02<00:00, 31.88it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2281.06it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1230.84it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1277.84it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1500.78it/s]\n",
      " 80%|███████▉  | 75/94 [00:02<00:00, 32.52it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 921.49it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1633.30it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1354.31it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1423.57it/s]\n",
      " 84%|████████▍ | 79/94 [00:02<00:00, 33.95it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1510.24it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1271.13it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 2097.94it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1307.45it/s]\n",
      " 88%|████████▊ | 83/94 [00:02<00:00, 32.87it/s]\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1346.27it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1328.29it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1706.91it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1262.39it/s]\n",
      " 93%|█████████▎| 87/94 [00:02<00:00, 32.54it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1028.52it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 4/4 [00:00<00:00, 1484.05it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1266.40it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 2251.77it/s]\n",
      " 97%|█████████▋| 91/94 [00:02<00:00, 33.79it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1863.31it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1939.41it/s]\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1952.35it/s]\n",
      "100%|██████████| 94/94 [00:02<00:00, 32.18it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>compress_type</th>\n",
       "      <th>filemode</th>\n",
       "      <th>external_attr</th>\n",
       "      <th>file_size</th>\n",
       "      <th>compress_size</th>\n",
       "      <th>src_names</th>\n",
       "      <th>bsheets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3025432_658178_tache_irm_onset-event-encoding_...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>21511</td>\n",
       "      <td>3011</td>\n",
       "      <td>3025432_658178_Tache_IRM/Onset-Event-Encoding_...</td>\n",
       "      <td>b'1             ctl           ctl0            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3025432_658178_tache_irm_output-responses-enco...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>3557</td>\n",
       "      <td>1206</td>\n",
       "      <td>3025432_658178_Tache_IRM/Output-Responses-Enco...</td>\n",
       "      <td>b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3025432_658178_tache_irm_output_retrieval_cima...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>5816</td>\n",
       "      <td>2174</td>\n",
       "      <td>3025432_658178_Tache_IRM/Output_Retrieval_CIMA...</td>\n",
       "      <td>b'category\\tstim\\toldnumber\\trecognition_acc\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3291977_748676_tache_irm_onset-event-encoding_...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>21501</td>\n",
       "      <td>2789</td>\n",
       "      <td>3291977_748676_Tache_IRM/Onset-Event-Encoding_...</td>\n",
       "      <td>b'1             enc           enc00           ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3291977_748676_tache_irm_output-responses-enco...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>3125</td>\n",
       "      <td>866</td>\n",
       "      <td>3291977_748676_Tache_IRM/Output-Responses-Enco...</td>\n",
       "      <td>b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>9296157_955548_tache_irm_output-responses-enco...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>3546</td>\n",
       "      <td>1208</td>\n",
       "      <td>9296157_955548_Tache_IRM/Output-Responses-Enco...</td>\n",
       "      <td>b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>9296157_955548_tache_irm_output_retrieval_cima...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>5823</td>\n",
       "      <td>2159</td>\n",
       "      <td>9296157_955548_Tache_IRM/Output_Retrieval_CIMA...</td>\n",
       "      <td>b'category\\tstim\\toldnumber\\trecognition_acc\\t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>9929164_197192_tache_irm_onset-event-encoding_...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>21511</td>\n",
       "      <td>2780</td>\n",
       "      <td>9929164_197192_Tache_IRM/Onset-Event-Encoding_...</td>\n",
       "      <td>b'1             ctl           ctl0            ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>9929164_197192_tache_irm_output-responses-enco...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>3125</td>\n",
       "      <td>873</td>\n",
       "      <td>9929164_197192_Tache_IRM/Output-Responses-Enco...</td>\n",
       "      <td>b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>9929164_197192_tache_irm_output_retrieval_cima...</td>\n",
       "      <td>deflate</td>\n",
       "      <td>-rwxrwxrwx</td>\n",
       "      <td>0x4000</td>\n",
       "      <td>5876</td>\n",
       "      <td>2216</td>\n",
       "      <td>9929164_197192_Tache_IRM/Output_Retrieval_CIMA...</td>\n",
       "      <td>b'category\\tstim\\toldnumber\\trecognition_acc\\t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>198 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              filename compress_type  \\\n",
       "0    3025432_658178_tache_irm_onset-event-encoding_...       deflate   \n",
       "1    3025432_658178_tache_irm_output-responses-enco...       deflate   \n",
       "2    3025432_658178_tache_irm_output_retrieval_cima...       deflate   \n",
       "3    3291977_748676_tache_irm_onset-event-encoding_...       deflate   \n",
       "4    3291977_748676_tache_irm_output-responses-enco...       deflate   \n",
       "..                                                 ...           ...   \n",
       "193  9296157_955548_tache_irm_output-responses-enco...       deflate   \n",
       "194  9296157_955548_tache_irm_output_retrieval_cima...       deflate   \n",
       "195  9929164_197192_tache_irm_onset-event-encoding_...       deflate   \n",
       "196  9929164_197192_tache_irm_output-responses-enco...       deflate   \n",
       "197  9929164_197192_tache_irm_output_retrieval_cima...       deflate   \n",
       "\n",
       "       filemode external_attr file_size compress_size  \\\n",
       "0    -rwxrwxrwx        0x4000     21511          3011   \n",
       "1    -rwxrwxrwx        0x4000      3557          1206   \n",
       "2    -rwxrwxrwx        0x4000      5816          2174   \n",
       "3    -rwxrwxrwx        0x4000     21501          2789   \n",
       "4    -rwxrwxrwx        0x4000      3125           866   \n",
       "..          ...           ...       ...           ...   \n",
       "193  -rwxrwxrwx        0x4000      3546          1208   \n",
       "194  -rwxrwxrwx        0x4000      5823          2159   \n",
       "195  -rwxrwxrwx        0x4000     21511          2780   \n",
       "196  -rwxrwxrwx        0x4000      3125           873   \n",
       "197  -rwxrwxrwx        0x4000      5876          2216   \n",
       "\n",
       "                                             src_names  \\\n",
       "0    3025432_658178_Tache_IRM/Onset-Event-Encoding_...   \n",
       "1    3025432_658178_Tache_IRM/Output-Responses-Enco...   \n",
       "2    3025432_658178_Tache_IRM/Output_Retrieval_CIMA...   \n",
       "3    3291977_748676_Tache_IRM/Onset-Event-Encoding_...   \n",
       "4    3291977_748676_Tache_IRM/Output-Responses-Enco...   \n",
       "..                                                 ...   \n",
       "193  9296157_955548_Tache_IRM/Output-Responses-Enco...   \n",
       "194  9296157_955548_Tache_IRM/Output_Retrieval_CIMA...   \n",
       "195  9929164_197192_Tache_IRM/Onset-Event-Encoding_...   \n",
       "196  9929164_197192_Tache_IRM/Output-Responses-Enco...   \n",
       "197  9929164_197192_Tache_IRM/Output_Retrieval_CIMA...   \n",
       "\n",
       "                                               bsheets  \n",
       "0    b'1             ctl           ctl0            ...  \n",
       "1    b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...  \n",
       "2    b'category\\tstim\\toldnumber\\trecognition_acc\\t...  \n",
       "3    b'1             enc           enc00           ...  \n",
       "4    b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...  \n",
       "..                                                 ...  \n",
       "193  b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...  \n",
       "194  b'category\\tstim\\toldnumber\\trecognition_acc\\t...  \n",
       "195  b'1             ctl           ctl0            ...  \n",
       "196  b'trialnumber\\tcategory\\ttrialcode\\toldnumber\\...  \n",
       "197  b'category\\tstim\\toldnumber\\trecognition_acc\\t...  \n",
       "\n",
       "[198 rows x 8 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ext</th>\n",
       "      <th>parent</th>\n",
       "      <th>fpaths</th>\n",
       "      <th>subid</th>\n",
       "      <th>ses</th>\n",
       "      <th>run</th>\n",
       "      <th>task</th>\n",
       "      <th>modality</th>\n",
       "      <th>dccid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-108391_ses-4_run-01_task-idle_FLAIR</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-108391</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-01</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>FLAIR</td>\n",
       "      <td>108391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-108391_ses-4_run-01_task-idle_FLAIR</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-108391</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-01</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>FLAIR</td>\n",
       "      <td>108391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-108391_ses-4_run-01_task-idle_PD</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-108391</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-01</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>PD</td>\n",
       "      <td>108391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-108391_ses-4_run-01_task-idle_PD</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-108391</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-01</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>PD</td>\n",
       "      <td>108391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-108391_ses-4_run-01_task-idle_T1w</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-108391</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-01</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>T1w</td>\n",
       "      <td>108391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>sub-998166_ses-4_run-02_task-idle_T1w</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-998166</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-02</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>T1w</td>\n",
       "      <td>998166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>sub-998166_ses-4_run-02_task-idle_fieldmap</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>fmap</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-998166</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-02</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>fieldmap</td>\n",
       "      <td>998166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3201</th>\n",
       "      <td>sub-998166_ses-4_run-02_task-idle_fieldmap</td>\n",
       "      <td>.json</td>\n",
       "      <td>fmap</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-998166</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-02</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>fieldmap</td>\n",
       "      <td>998166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3202</th>\n",
       "      <td>sub-998166_ses-4_run-02_task-idle_magnitude</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>fmap</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-998166</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-02</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>magnitude</td>\n",
       "      <td>998166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>sub-998166_ses-4_run-02_task-idle_magnitude</td>\n",
       "      <td>.json</td>\n",
       "      <td>fmap</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>sub-998166</td>\n",
       "      <td>ses-4</td>\n",
       "      <td>run-02</td>\n",
       "      <td>task-idle</td>\n",
       "      <td>magnitude</td>\n",
       "      <td>998166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3204 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         filename      ext parent  \\\n",
       "0         sub-108391_ses-4_run-01_task-idle_FLAIR  .nii.gz   anat   \n",
       "1         sub-108391_ses-4_run-01_task-idle_FLAIR    .json   anat   \n",
       "2            sub-108391_ses-4_run-01_task-idle_PD  .nii.gz   anat   \n",
       "3            sub-108391_ses-4_run-01_task-idle_PD    .json   anat   \n",
       "4           sub-108391_ses-4_run-01_task-idle_T1w  .nii.gz   anat   \n",
       "...                                           ...      ...    ...   \n",
       "3199        sub-998166_ses-4_run-02_task-idle_T1w    .json   anat   \n",
       "3200   sub-998166_ses-4_run-02_task-idle_fieldmap  .nii.gz   fmap   \n",
       "3201   sub-998166_ses-4_run-02_task-idle_fieldmap    .json   fmap   \n",
       "3202  sub-998166_ses-4_run-02_task-idle_magnitude  .nii.gz   fmap   \n",
       "3203  sub-998166_ses-4_run-02_task-idle_magnitude    .json   fmap   \n",
       "\n",
       "                                                 fpaths       subid    ses  \\\n",
       "0     /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-108391  ses-4   \n",
       "1     /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-108391  ses-4   \n",
       "2     /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-108391  ses-4   \n",
       "3     /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-108391  ses-4   \n",
       "4     /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-108391  ses-4   \n",
       "...                                                 ...         ...    ...   \n",
       "3199  /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-998166  ses-4   \n",
       "3200  /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-998166  ses-4   \n",
       "3201  /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-998166  ses-4   \n",
       "3202  /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-998166  ses-4   \n",
       "3203  /home/fnadeau/../../data/simexp/datasets/cimaq...  sub-998166  ses-4   \n",
       "\n",
       "         run       task   modality   dccid  \n",
       "0     run-01  task-idle      FLAIR  108391  \n",
       "1     run-01  task-idle      FLAIR  108391  \n",
       "2     run-01  task-idle         PD  108391  \n",
       "3     run-01  task-idle         PD  108391  \n",
       "4     run-01  task-idle        T1w  108391  \n",
       "...      ...        ...        ...     ...  \n",
       "3199  run-02  task-idle        T1w  998166  \n",
       "3200  run-02  task-idle   fieldmap  998166  \n",
       "3201  run-02  task-idle   fieldmap  998166  \n",
       "3202  run-02  task-idle  magnitude  998166  \n",
       "3203  run-02  task-idle  magnitude  998166  \n",
       "\n",
       "[3204 rows x 10 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "broken_sheets = pd.concat((val for val in\n",
    "                         df(tuple(scanzip(apath,\n",
    "                                          exclude = ['Practice', 'Pratique',\n",
    "                                                    'PRATIQUE', 'PRACTICE', 'READ',\n",
    "                                                     'Encoding-scan', 'Retrieval-'],\n",
    "                                          to_xtrct = ['.pdf', '.edat2'],\n",
    "                                          dst_path = pjoin(os.getcwd(), 'newdevs',\n",
    "                                                           'cimaq_uzeprimes'))\n",
    "                                 for apath in\n",
    "                                 tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:], snif.loadimages(xpu(pjoin(\n",
    "                                         drv_dir, 'task_files/zipped_eprime')))))))\\\n",
    "                        [0].values.flatten()), ignore_index = True).dropna().sort_values('filename').reset_index(drop = True)\n",
    "\n",
    "scan_infos = bidsify_load_scans(cimaq_dir, snif.clean_bytes(xpu(pjoin(\n",
    "            drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:])\n",
    "\n",
    "scan_infos['dccid'] = sorted([(filename, filename.split('-')[1].split('_')[0])[1]\n",
    "                            for filename in cimaq.filename])\n",
    "display(broken_sheets, scan_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/94 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1260.26it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 1410.38it/s]\n",
      "\n",
      "\n",
      "  2%|▏         | 2/94 [00:00<00:05, 18.00it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1131.46it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1399.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1382.89it/s]\n",
      "\n",
      "\n",
      "  5%|▌         | 5/94 [00:00<00:04, 18.89it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1297.29it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1282.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1485.17it/s]\n",
      "\n",
      "\n",
      "  9%|▊         | 8/94 [00:00<00:04, 19.98it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1567.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1396.04it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 1253.03it/s]\n",
      "\n",
      "\n",
      " 12%|█▏        | 11/94 [00:00<00:04, 20.08it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1211.79it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1398.03it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1602.26it/s]\n",
      "\n",
      "\n",
      " 15%|█▍        | 14/94 [00:00<00:03, 20.55it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1568.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1618.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1835.08it/s]\n",
      "\n",
      "\n",
      " 18%|█▊        | 17/94 [00:00<00:03, 20.50it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1121.60it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1113.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1106.51it/s]\n",
      "\n",
      "\n",
      " 21%|██▏       | 20/94 [00:00<00:03, 21.12it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 1305.24it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1585.52it/s]\n",
      "\n",
      "\n",
      " 23%|██▎       | 22/94 [00:01<00:03, 18.94it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1471.24it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 1275.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1141.48it/s]\n",
      "\n",
      "\n",
      " 27%|██▋       | 25/94 [00:01<00:03, 20.34it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1334.55it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1552.44it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 1690.45it/s]\n",
      "\n",
      "\n",
      " 30%|██▉       | 28/94 [00:01<00:03, 19.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 1474.32it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1963.36it/s]\n",
      "\n",
      "\n",
      " 32%|███▏      | 30/94 [00:01<00:03, 19.68it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1371.71it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1389.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 1615.57it/s]\n",
      "\n",
      "\n",
      " 35%|███▌      | 33/94 [00:01<00:03, 19.91it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1170.52it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1228.79it/s]\n",
      "\n",
      "\n",
      " 37%|███▋      | 35/94 [00:01<00:03, 19.25it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1064.20it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1588.83it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1239.68it/s]\n",
      "\n",
      "\n",
      " 40%|████      | 38/94 [00:01<00:02, 19.61it/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1567.38it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1596.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1422.97it/s]\n",
      "  0%|          | 0/94 [00:29<?, ?it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1369.85it/s]\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 1428.31it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1385.43it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1543.24it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1297.74it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1314.11it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1503.60it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1546.50it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1280.83it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1375.50it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1598.06it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1352.07it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1382.95it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1359.14it/s]\n",
      "scanning archive: 100%|██████████| 9/9 [00:00<00:00, 1348.41it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1603.79it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1276.66it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1378.80it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1326.79it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1683.49it/s]\n",
      "scanning archive: 100%|██████████| 11/11 [00:00<00:00, 1479.28it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1382.15it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1553.30it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1578.73it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1380.48it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1558.64it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1414.01it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1374.99it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1590.48it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1352.19it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1345.14it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1224.67it/s]\n",
      "scanning archive: 100%|██████████| 6/6 [00:00<00:00, 1472.98it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1383.86it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1239.09it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1569.56it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1384.49it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1569.89it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1624.83it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1359.96it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1346.92it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1543.16it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1412.82it/s]\n",
      "scanning archive: 100%|██████████| 3/3 [00:00<00:00, 1272.16it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1813.56it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1573.55it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1483.51it/s]\n",
      "scanning archive: 100%|██████████| 8/8 [00:00<00:00, 1373.72it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1361.22it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1537.10it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1352.63it/s]\n",
      "\n",
      "\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1365.78it/s]\n",
      "scanning archive: 100%|██████████| 7/7 [00:00<00:00, 1297.46it/s]\n",
      "100%|██████████| 94/94 [00:04<00:00, 20.98it/s]\n",
      "creating Pandas DataFrames: 322it [01:39,  3.23it/s]\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(pjoin(os.getcwd(), 'newdevs', 'cimaq_dialects'), exist_ok = True)\n",
    "os.makedirs(pjoin(os.getcwd(), 'newdevs', 'cimaq_temp_events'), exist_ok = True)\n",
    "newsh = [((snif.clean_bytes(row[1].bsheets, **row[1]), \n",
    "          df.from_dict(snif.sniff_bytes(snif.clean_bytes(row[1].bsheets)),\n",
    "                       orient = 'index').to_csv(pjoin(os.getcwd(), 'newdevs', 'cimaq_dialects',\n",
    "                                                      'old_'+row[1].filename), sep = '\\t')),\n",
    "          df.from_dict(snif.sniff_bytes(row[1].bsheets),\n",
    "                       orient = 'index').to_csv(pjoin(os.getcwd(), 'newdevs', 'cimaq_dialects',\n",
    "                                                      'new_'+row[1].filename), sep = '\\t'),\n",
    "          snif.bytes2df(row[1].bsheets).to_csv(\n",
    "              pjoin(os.getcwd(), 'newdevs', 'cimaq_temp_events', os.path.splitext(row[1].filename)[0]+'.tsv'),\n",
    "                                               sep = '\\t', index = None))[0]\n",
    "         for row in tqdm(pd.concat(\n",
    "                 val for val in\n",
    "                 df(tuple(scanzip(apath,\n",
    "                                  exclude = ['Practice', 'Pratique',\n",
    "                                             'PRATIQUE', 'PRACTICE'],\n",
    "                                  to_xtrct = ['.pdf', '.edat2'],\n",
    "                                  dst_path = pjoin(os.getcwd(),  'newdevs', 'cimaq_uzeprimes'))\n",
    "                          for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "                                         drv_dir, 'task_files/zipped_eprime')))))),\n",
    "                    dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "                             desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "argument of type 'NoneType' is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-0d4f175b9100>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbidsify_load_scans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'events'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'filename'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# [filename.split(\"_\") for filename in events.filename]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# events[['pscid', 'dccid']] = [filename.split('_')[1:3] for filename in events.filename]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/cimaq_memory/bidsify_utils.py\u001b[0m in \u001b[0;36mbidsify_load_scans\u001b[0;34m(maindir, id_list)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     subjects = pd.concat([bidsify_df(bidsify_lim(pjoin(maindir, itm)))\n\u001b[0;32m---> 85\u001b[0;31m                       \u001b[0;32mfor\u001b[0m \u001b[0mitm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaindir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'sub'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m                       \u001b[0;32mand\u001b[0m \u001b[0mitm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                       id_list], ignore_index = True).sort_values(\n",
      "\u001b[0;32m~/cimaq_memory/bidsify_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     85\u001b[0m                       \u001b[0;32mfor\u001b[0m \u001b[0mitm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaindir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'sub'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                       \u001b[0;32mand\u001b[0m \u001b[0mitm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'-'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                       id_list], ignore_index = True).sort_values(\n\u001b[0m\u001b[1;32m     88\u001b[0m                               'filename').reset_index(drop = True)\n\u001b[1;32m     89\u001b[0m     scaninfos = df((bidsify_sort(name)\n",
      "\u001b[0;31mTypeError\u001b[0m: argument of type 'NoneType' is not iterable"
     ]
    }
   ],
   "source": [
    "events = bidsify_load_scans(pjoin(os.getcwd(), 'events'),\n",
    "                           ).sort_values('filename')\n",
    "# [filename.split(\"_\") for filename in events.filename]\n",
    "# events[['pscid', 'dccid']] = [filename.split('_')[1:3] for filename in events.filename]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ext</th>\n",
       "      <th>parent</th>\n",
       "      <th>fpaths</th>\n",
       "      <th>pscid</th>\n",
       "      <th>dccid</th>\n",
       "      <th>modality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sub-108391_ses-4_FLAIR</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>FLAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sub-108391_ses-4_PD</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>sub-108391_ses-4_T1w</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>T1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sub-108391_ses-4_T2star</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>T2star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>sub-108391_ses-4_T2w</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>T2w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3195</th>\n",
       "      <td>sub-998166_ses-4_run-02_T1w</td>\n",
       "      <td>.json</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998166</td>\n",
       "      <td>T1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3197</th>\n",
       "      <td>sub-998166_ses-4_run-02_fieldmap</td>\n",
       "      <td>.json</td>\n",
       "      <td>fmap</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998166</td>\n",
       "      <td>fieldmap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>sub-998166_ses-4_run-02_magnitude</td>\n",
       "      <td>.json</td>\n",
       "      <td>fmap</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998166</td>\n",
       "      <td>magnitude</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3200</th>\n",
       "      <td>sub-998166_ses-4_task-memory_bold</td>\n",
       "      <td>.json</td>\n",
       "      <td>func</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998166</td>\n",
       "      <td>bold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3203</th>\n",
       "      <td>sub-998166_ses-4_task-rest_bold</td>\n",
       "      <td>.json</td>\n",
       "      <td>func</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>998166</td>\n",
       "      <td>bold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1508 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               filename    ext parent  \\\n",
       "1                sub-108391_ses-4_FLAIR  .json   anat   \n",
       "3                   sub-108391_ses-4_PD  .json   anat   \n",
       "5                  sub-108391_ses-4_T1w  .json   anat   \n",
       "7               sub-108391_ses-4_T2star  .json   anat   \n",
       "9                  sub-108391_ses-4_T2w  .json   anat   \n",
       "...                                 ...    ...    ...   \n",
       "3195        sub-998166_ses-4_run-02_T1w  .json   anat   \n",
       "3197   sub-998166_ses-4_run-02_fieldmap  .json   fmap   \n",
       "3199  sub-998166_ses-4_run-02_magnitude  .json   fmap   \n",
       "3200  sub-998166_ses-4_task-memory_bold  .json   func   \n",
       "3203    sub-998166_ses-4_task-rest_bold  .json   func   \n",
       "\n",
       "                                                 fpaths pscid   dccid  \\\n",
       "1     /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  108391   \n",
       "3     /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  108391   \n",
       "5     /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  108391   \n",
       "7     /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  108391   \n",
       "9     /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  108391   \n",
       "...                                                 ...   ...     ...   \n",
       "3195  /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  998166   \n",
       "3197  /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  998166   \n",
       "3199  /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  998166   \n",
       "3200  /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  998166   \n",
       "3203  /home/fnadeau/../../data/simexp/datasets/cimaq...   NaN  998166   \n",
       "\n",
       "       modality  \n",
       "1         FLAIR  \n",
       "3            PD  \n",
       "5           T1w  \n",
       "7        T2star  \n",
       "9           T2w  \n",
       "...         ...  \n",
       "3195        T1w  \n",
       "3197   fieldmap  \n",
       "3199  magnitude  \n",
       "3200       bold  \n",
       "3203       bold  \n",
       "\n",
       "[1508 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>ext</th>\n",
       "      <th>parent</th>\n",
       "      <th>fpaths</th>\n",
       "      <th>pscid</th>\n",
       "      <th>dccid</th>\n",
       "      <th>modality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sub-108391_ses-4_FLAIR</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>FLAIR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sub-108391_ses-4_PD</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>PD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sub-108391_ses-4_T1w</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>T1w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>sub-108391_ses-4_T2star</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>T2star</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>sub-108391_ses-4_T2w</td>\n",
       "      <td>.nii.gz</td>\n",
       "      <td>anat</td>\n",
       "      <td>/home/fnadeau/../../data/simexp/datasets/cimaq...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108391</td>\n",
       "      <td>T2w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3331</th>\n",
       "      <td>sub-_9249304_778749_task-encoding_events</td>\n",
       "      <td>.tsv</td>\n",
       "      <td>events</td>\n",
       "      <td>/home/fnadeau/cimaq_memory/events/sub-_9249304...</td>\n",
       "      <td>9249304</td>\n",
       "      <td>778749</td>\n",
       "      <td>events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3332</th>\n",
       "      <td>sub-_9296157_955548_task-encoding_behavioural</td>\n",
       "      <td>.tsv</td>\n",
       "      <td>behavioural</td>\n",
       "      <td>/home/fnadeau/cimaq_memory/behavioural/sub-_92...</td>\n",
       "      <td>9296157</td>\n",
       "      <td>955548</td>\n",
       "      <td>behavioural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3333</th>\n",
       "      <td>sub-_9296157_955548_task-encoding_events</td>\n",
       "      <td>.tsv</td>\n",
       "      <td>events</td>\n",
       "      <td>/home/fnadeau/cimaq_memory/events/sub-_9296157...</td>\n",
       "      <td>9296157</td>\n",
       "      <td>955548</td>\n",
       "      <td>events</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3334</th>\n",
       "      <td>sub-_9929164_197192_task-encoding_behavioural</td>\n",
       "      <td>.tsv</td>\n",
       "      <td>behavioural</td>\n",
       "      <td>/home/fnadeau/cimaq_memory/behavioural/sub-_99...</td>\n",
       "      <td>9929164</td>\n",
       "      <td>197192</td>\n",
       "      <td>behavioural</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3335</th>\n",
       "      <td>sub-_9929164_197192_task-encoding_events</td>\n",
       "      <td>.tsv</td>\n",
       "      <td>events</td>\n",
       "      <td>/home/fnadeau/cimaq_memory/events/sub-_9929164...</td>\n",
       "      <td>9929164</td>\n",
       "      <td>197192</td>\n",
       "      <td>events</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1828 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           filename      ext       parent  \\\n",
       "0                            sub-108391_ses-4_FLAIR  .nii.gz         anat   \n",
       "2                               sub-108391_ses-4_PD  .nii.gz         anat   \n",
       "4                              sub-108391_ses-4_T1w  .nii.gz         anat   \n",
       "6                           sub-108391_ses-4_T2star  .nii.gz         anat   \n",
       "8                              sub-108391_ses-4_T2w  .nii.gz         anat   \n",
       "...                                             ...      ...          ...   \n",
       "3331       sub-_9249304_778749_task-encoding_events     .tsv       events   \n",
       "3332  sub-_9296157_955548_task-encoding_behavioural     .tsv  behavioural   \n",
       "3333       sub-_9296157_955548_task-encoding_events     .tsv       events   \n",
       "3334  sub-_9929164_197192_task-encoding_behavioural     .tsv  behavioural   \n",
       "3335       sub-_9929164_197192_task-encoding_events     .tsv       events   \n",
       "\n",
       "                                                 fpaths    pscid   dccid  \\\n",
       "0     /home/fnadeau/../../data/simexp/datasets/cimaq...      NaN  108391   \n",
       "2     /home/fnadeau/../../data/simexp/datasets/cimaq...      NaN  108391   \n",
       "4     /home/fnadeau/../../data/simexp/datasets/cimaq...      NaN  108391   \n",
       "6     /home/fnadeau/../../data/simexp/datasets/cimaq...      NaN  108391   \n",
       "8     /home/fnadeau/../../data/simexp/datasets/cimaq...      NaN  108391   \n",
       "...                                                 ...      ...     ...   \n",
       "3331  /home/fnadeau/cimaq_memory/events/sub-_9249304...  9249304  778749   \n",
       "3332  /home/fnadeau/cimaq_memory/behavioural/sub-_92...  9296157  955548   \n",
       "3333  /home/fnadeau/cimaq_memory/events/sub-_9296157...  9296157  955548   \n",
       "3334  /home/fnadeau/cimaq_memory/behavioural/sub-_99...  9929164  197192   \n",
       "3335  /home/fnadeau/cimaq_memory/events/sub-_9929164...  9929164  197192   \n",
       "\n",
       "         modality  \n",
       "0           FLAIR  \n",
       "2              PD  \n",
       "4             T1w  \n",
       "6          T2star  \n",
       "8             T2w  \n",
       "...           ...  \n",
       "3331       events  \n",
       "3332  behavioural  \n",
       "3333       events  \n",
       "3334  behavioural  \n",
       "3335       events  \n",
       "\n",
       "[1828 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# subs = [subjects.groupby('dccid').get_group(grp) for grp in subjects.groupby('dccid').groups]\n",
    "\n",
    "\n",
    "behav = loadfiles(loadimages(pjoin(os.getcwd(), 'behavioural')))\n",
    "behav[['pscid', 'dccid']] = [filename.split('_')[1:3] for filename in behav.filename]\n",
    "cimaq_dataset = pd.concat([events, behav, subjects]).sort_values('filename', ignore_index = True).reset_index(drop = True)\n",
    "cimaq_dataset['modality'] = [filename.split('_')[-1] for filename in cimaq_dataset.filename]\n",
    "cimaq_ds = [cimaq_dataset.groupby('dccid').get_group(grp) for grp in\n",
    "       cimaq_dataset.groupby('dccid').groups]\n",
    "jsonfiles, cimaq_dataset= (cimaq_dataset.drop([[row[0]\n",
    "                                for row in cimaq_dataset.iterrows()\n",
    "                                 if row[1].ext != '.json']][0], axis=0),\n",
    "            cimaq_dataset.drop([[row[0]\n",
    "                                for row in cimaq_dataset.iterrows()\n",
    "                                 if row[1].ext == '.json']][0], axis=0))\n",
    "display(jsonfiles,\n",
    "cimaq_dataset)\n",
    "# test_ds = pd.concat([cimaq_dataset, jsonfiles], axis = 1)\n",
    "# cimaq_dataset = cimaq_dataset.drop(jsonfiles.index)\n",
    "# jsonfiles\n",
    "# behav\n",
    "# cimaq_dataset = pd.concat([events, behav, subjects]).sort_values('filename', ignore_index = True).reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ TODO: Group by modality ################3333\n",
    "\n",
    "subs_json = [jsonfiles.groupby('dccid').get_group(grp)\n",
    "             for grp in jsonfiles.groupby('dccid').groups]\n",
    "modgrps = [[sjson.groupby('modality').get_group(grp) for grp in sjson.groupby('modality').groups]\n",
    "           for sjson in subs_json]\n",
    "modgrps[4][0]\n",
    "# loaded_json = [pd.read_json(row[1].fpaths) for row in itm.iterrows()]\n",
    "#                for itm in subs_json]\n",
    "\n",
    "# loaded_json2 = [row[1].unique() for row in loaded_json.iterrows()]\n",
    "# loaded_json2 = [Counter(row[1]).most_common(1) for row in loaded_json.iterrows()]\n",
    "# loaded_json2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newsh2 = [(bname(apath),\n",
    "#            pd.read_csv(apath, sep='\\t')) for apath in\n",
    "cimaq = loadfiles(snif.filter_lst_inc(['onset', 'output'],\n",
    "                                                 loadimages(pjoin(os.getcwd(), 'cimaq_temp_events'))))\n",
    "\n",
    "cimaq[['pscid', 'dccid', 'as_df']] = [(bname(apath).split('_')[0],\n",
    "                                       bname(apath).split('_')[1],\n",
    "                                       pd.read_csv(apath, sep='\\t')) for apath in\n",
    "                   cimaq.fpaths]\n",
    "cimaq = tuple((cimaq.groupby('pscid').get_group(grp).reset_index(drop = True).sort_values('fname') for\n",
    "                   grp in cimaq.groupby('pscid').groups))\n",
    "all([len(itm) == 3 for itm in cimaq])\n",
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cimaq_utils import get_drv_dir_paths\n",
    "get_drv_dir_paths(drv_dir)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(pjoin(os.getcwd(), 'behavioural'), exist_ok = True)\n",
    "os.makedirs(pjoin(os.getcwd(), 'events'), exist_ok = True)\n",
    "[(pd.concat([itm['as_df'].iloc[0],\n",
    "            itm['as_df'].loc[1].iloc[:,-3:]],\n",
    "           axis = 1).to_csv(pjoin(os.getcwd(), 'events',\n",
    "                                  'sub-_'+next((itm.pscid.__iter__())) + '_' + \\\n",
    "                                  (next(itm.dccid.__iter__()))+'_task-encoding_events.tsv'),\n",
    "                            sep = '\\t', index = None),\n",
    "  itm['as_df'].iloc[2].to_csv(pjoin(os.getcwd(), 'behavioural',\n",
    "                                   'sub-_'+next((itm.pscid.__iter__())) + '_' + \\\n",
    "                                  (next(itm.dccid.__iter__()))+'_task-encoding_behavioural.tsv'),\n",
    "                             sep = '\\t', index = None)) for itm in cimaq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict((bname(apath), pd.read_csv(apath, sep = '\\t')) for apath\n",
    " in snif.loadimages(pjoin(os.getcwd(), 'behavioural')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16s vs 23s (6 seconds faster)\n",
    "newsh = [df((line.split('\\t') for line in unidecode(\n",
    "                  snif.clean_bytes(row[1].bsheets).decode()).split('\\n')),\n",
    "                 dtype = object).T.set_index(0, drop = True).T\n",
    "             if row[1].has_header else\n",
    "        df((line.split('\\t') for line in unidecode(\n",
    "                 snif.clean_bytes(row[1].bsheets).decode()).split('\\n')),\n",
    "                                   dtype = object)\n",
    "        for row in tqdm(pd.concat(\n",
    "                 val.T for val in\n",
    "                 df(tuple(sz1(apath,\n",
    "                                  exclude = ['Practice', 'Pratique',\n",
    "                                             'PRATIQUE', 'PRACTICE'],\n",
    "                                  to_xtrct = ['.pdf', '.edat2'],\n",
    "                                  dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                          for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "                                         drv_dir, 'task_files/zipped_eprime')))))),\n",
    "                    dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "                             desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsh[0]\n",
    "newsh2 = [(bname(fpath), pd.read_csv(fpath, sep='\\t')) for fpath in\n",
    "         snif.filter_lst_inc(['onset', 'output'], loadimages(pjoin(drv_dir, 'cimaq_events')))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsh2[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# snif.filter_lst_inc(['.pdf', '.edat2'], cimaq_infos.filename)\n",
    "# [itm for itm in cimaq_infos.filename if '.pdf' in itm]\n",
    "# 322/94\n",
    "len(cimaq_infos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsh[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsh[0:5]\n",
    "\n",
    "newdir = pjoin(os.getcwd(), 'cimaq_clean_eprime_datas3')\n",
    "os.makedirs(newdir, exist_ok = True)\n",
    "[itm[1].to_csv(pjoin(newdir, itm[0]['filename']), sep = '\\t',\n",
    "               header = [0 if itm[0]['has_header'] else None][0],\n",
    "               index = None)\n",
    " for itm in newsh]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qcshet = snif.clean_bytes(xpu(pjoin(\n",
    "#     drv_dir, 'participants/sub_list_TaskQC.tsv'\n",
    "# ))).decode().split()[1:]\n",
    "\n",
    "# snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#     drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:],\n",
    "#                     snif.loadimages(xpu(pjoin(drv_dir, 'task_files/zipped_eprime'))))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.delimiter.unique()#[-1].decode('utf16')\n",
    "snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "    drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:],\n",
    "                cimaq_infos.filename).__len__()\n",
    "type(cimaq_infos.width.unique()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bytes2df(inpt, **kwargs):\n",
    "#     return [df((line.split('\\t') for line in unidecode(\n",
    "#                   clean_bytes(inpt, **kwargs).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T\n",
    "#              if get_has_header(inpt) else\n",
    "#         df((line.split('\\t') for line in unidecode(\n",
    "#                  clean_bytes(inpt, **kwargs).decode()).split('\\n')),\n",
    "#                                    dtype = object)][0]\n",
    "#         [for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          drv_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newsh = [df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T\n",
    "#              if row[1].has_header else\n",
    "#         df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object)\n",
    "#         for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          drv_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsh2 = [df((line.split('\\t') for line in unidecode(\n",
    "                  snif.clean_bytes(row[1].bsheets,\n",
    "                                   [snif.sniff_bytes(row[1].bsheets).values()]\\\n",
    "                                   [1:4]).decode()).split('\\n')),\n",
    "                 dtype = object).T.set_index(0, drop = True).T\n",
    "             if row[1].has_header else\n",
    "        df((line.split('\\t') for line in unidecode(\n",
    "                 snif.clean_bytes(row[1].bsheets,\n",
    "                                  [snif.sniff_bytes(row[1].bsheets).values()]\\\n",
    "                                  [1:4]).decode()).split('\\n')),\n",
    "                                   dtype = object)\n",
    "        for row in tqdm(pd.concat(\n",
    "                 val.T for val in\n",
    "                 df(tuple(scanzip(apath,\n",
    "                                  exclude = ['Practice', 'Pratique',\n",
    "                                             'PRATIQUE', 'PRACTICE'],\n",
    "                                  to_xtrct = ['.pdf', '.edat2'],\n",
    "                                  dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "                          for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "                                     drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                                         ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "                                         drv_dir, 'task_files/zipped_eprime')))))),\n",
    "                    dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "                             desc = 'creating Pandas DataFrames')]\n",
    "# newsheets = [(row[1].to_dict(),\n",
    "#               df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T)\n",
    "#              if row[1].has_header else\n",
    "#              (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object))\n",
    "#              for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          drv_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### MODEL ##################################\n",
    "# newsheets = [(row[1].to_dict(),\n",
    "#               df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T)\n",
    "#              if row[1].has_header else\n",
    "#              (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object))\n",
    "#              for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.filter_lst_inc(snif.clean_bytes(xpu(pjoin(\n",
    "#                                      drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "#                                          ).split()[1:],snif.loadimages(xpu(pjoin(\n",
    "#                                          drv_dir, 'task_files/zipped_eprime')))))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #             (\"encoding\", \"delimiter\", \"has_header\", \"dup_index\",\n",
    "# #              \"lineterminator\", \"nfields\", \"width\", \"nrows\").\n",
    "######### MODEL #############################################33\n",
    "# newsheets = [(row[1].to_dict(),\n",
    "#               df((line.split('\\t') for line in unidecode(\n",
    "#                   snif.clean_bytes(row[1].bsheets,\n",
    "#                                    [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                    [1:4]).decode()).split('\\n')),\n",
    "#                  dtype = object).T.set_index(0, drop = True).T)\n",
    "#              if row[1].has_header else\n",
    "#              (row[1].to_dict(), df((line.split('\\t') for line in unidecode(\n",
    "#                  snif.clean_bytes(row[1].bsheets,\n",
    "#                                   [snif.scan_bytes(row[1].bsheets).values()]\\\n",
    "#                                   [1:4]).decode()).split('\\n')),\n",
    "#                                    dtype = object))\n",
    "#              for row in tqdm(pd.concat(\n",
    "#                  val.T for val in\n",
    "#                  df(tuple(scanzip(apath,\n",
    "#                                   exclude = ['Practice', 'Pratique',\n",
    "#                                              'PRATIQUE', 'PRACTICE'],\n",
    "#                                   to_xtrct = ['.pdf', '.edat2'],\n",
    "#                                   dst_path = pjoin(os.getcwd(), 'cimaq_uzeprimes'))\n",
    "#                           for apath in tqdm(snif.loadimages(xpu(zeprimes)))),\n",
    "#                     dtype = object)[0].values.flatten()).dropna().reset_index(drop = True).iterrows(),\n",
    "#                              desc = 'creating Pandas DataFrames')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdir = pjoin(os.getcwd(), 'cimaq_clean_eprime_datas3')\n",
    "os.makedirs(newdir, exist_ok = True)\n",
    "[itm[1].to_csv(pjoin(newdir, itm[0]['filename']), sep = '\\t',\n",
    "               header = [0 if itm[0]['has_header'] else None][0],\n",
    "               index = None)\n",
    " for itm in newsheets]\n",
    "\n",
    "# json.dumps(newsheets)\n",
    "# newsheets[0][0]\n",
    "# def megamerge_dicts(dict_list: list) -> dict:\n",
    "#     return reduce(\n",
    "#         lambda x, y: {**x, **y}, dict_list\n",
    "#     )\n",
    "# allinfos = megamerge_dicts([itm[0] for itm in newsheets])\n",
    "# allinfos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadfiles(sorted(loadimages(newdir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index'])\n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.clean_bytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     has_header = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.clean_bytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                has_header = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['as_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[row[1].as_df.shape for row in cimaq_infos.iterrows()]\n",
    "cimaq_infos.as_df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([itm[1] for itm in\n",
    "           list(cimaq_infos['as_df'].iloc[0].iteritems())[[0, 1, 2, 3][-1]:]], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_infos = df(tuple(scanzip(apath) for apath in\n",
    "#                        tqdm(loadimages(xpu(zeprimes)),\n",
    "#                             desc = 'sniffing')))\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             unidecode(\n",
    "                                 snif.mkfrombytes(\n",
    "                                     row[1].bsheets,\n",
    "                                     encoding = row[1]['encoding'],\n",
    "                                     delimiter = row[1]['delimiter'],\n",
    "                                     hdr = row[1]['has_header'],\n",
    "                                     dup_index = row[1]['dup_index']).decode()\n",
    "                                      ).split('\\n')),\n",
    "                          dtype = object).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             unidecode(snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode()).split('\\n')),\n",
    "                           dtype = object)\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]\n",
    "\n",
    "# cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                           dtype = object).T.set_index(0, drop = True).T\n",
    "#                          if row[1].has_header else\n",
    "#                          df((line.split('\\t') for line in\n",
    "#                              unidecode(row[1].newsheets).split('\\n')),\n",
    "#                            dtype = object)\n",
    "#                          for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                          desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['as_df'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import collections\n",
    "help(collections.abc)\n",
    "# help(typing.Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index\n",
    "\n",
    "tsheets = cimaq_infos.iloc[:3, :]\n",
    "# bsheets\n",
    "def clean_bytes(inpt, encoding, hdr, delimiter, lineterminator, dup_index, nfields):\n",
    "    newsheet = b'\\n'.join([b'\\t'.join(itm.strip(b'\\\\s') for itm in re.sub(b'\\\\s{2,}',\n",
    "                                         b'\\\\s'+delimiter+b'\\\\s',\n",
    "                                         line).split(delimiter))\n",
    "                       for line in fix_na_reps(inpt.lower(), encoding,\n",
    "                                               delimiter,\n",
    "                                               lineterminator).decode(\n",
    "                           \"utf8\", \"replace\").replace(\"�\", \"\").strip().encode(\n",
    "                           \"utf8\").splitlines()])\n",
    "    return [fix_dup_index(newsheet, encoding, hdr, delimiter, nfields)\n",
    "            if dup_index else newsheet][0]\n",
    "\n",
    "newsheets = [clean_bytes(row[1].bsheets, row[1].encoding, row[1].has_header,\n",
    "                                row[1].delimiter, row[1].lineterminator,\n",
    "                                row[1].dup_index, row[1].nfields)\n",
    "                    for row in tsheets.iterrows()]\n",
    "\n",
    "newsheets[0].splitlines()\n",
    "\n",
    "\n",
    "# udec = pd.read_csv(StringIO(unidecode(newsheet.decode())), sep = '\\t')\n",
    "# no_udec = pd.read_csv(StringIO(newsheet.decode()), sep = '\\t')\n",
    "# all(no_udec == udec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0].values == itm[1][1].values)]\n",
    "    return b'\\n'.join(b'\\t'.join(row[1].values.tolist()) for row in\n",
    "                      pd.concat((evdf[booltest],\n",
    "                       pd.Series([str(np.nan).encode()]*evdf.shape[1]),\n",
    "#                       pd.Series((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1)\n",
    "#                                 for row in evdf.iterrows()),\n",
    "                       oddf[booltest[-1]:]), axis = 1).iterrows())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # help(os.PathLike)\n",
    "\n",
    "# def fix_dup_index(\n",
    "#     inpt: Union[bytes, str, os.PathLike],\n",
    "#     encoding: str = None,\n",
    "#     hdr: bool = False,\n",
    "#     delimiter: bytes = None,\n",
    "#     nfields: int = None\n",
    "# ) -> bytes:\n",
    "#     inpt = get_bytes(inpt)\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     nfields = [nfields if nfields else get_nfields(inpt, hdr)]\n",
    "#     evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "#                   in evenodd(inpt.splitlines()))\n",
    "#     booltest = [itm[0] for itm in enumerate(\n",
    "#                    tuple(zip([itm[1] for itm in\n",
    "#                               evdf.iteritems()],\n",
    "#                               [itm[1] for itm in\n",
    "#                                oddf.iteritems()])))\n",
    "#                 if all(itm[1][0].values == itm[1][1].values)]\n",
    "#     datas = pd.concat((evdf[booltest],\n",
    "#                       pd.Series(bytes((int(len(row[1].values)) \\\n",
    "#                                  == nfields[0] -1), 'utf8')\n",
    "#                                 for row in evdf.iterrows()),\n",
    "#                       oddf[booltest[-1]:]), axis = 1)\n",
    "#     return b'\\n'.join(b'\\t'.join(itm for\n",
    "#                                  itm in row[1].values.tolist())\n",
    "#                       for row in datas.iterrows())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_bytes(inpt: Union[bytes, str, os.PathLike, object]) -> bytes:\n",
    "#     \"\"\" Returns bytes from file either from memory or from reading \"\"\"\n",
    "#     if type(input) == object:\n",
    "#         return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                      itm in row[1].values.tolist())\n",
    "#                           for row in inpt.iterrows())    \n",
    "#     elif type(inpt) == bytes:\n",
    "#         return [inpt if bool(len(inpt.splitlines()) > \\\n",
    "#                   0 and inpt != None) else b\"1\"][0]\n",
    "#     elif type(inpt) == str:\n",
    "# #         try:\n",
    "#         with open(inpt, \"rb\", buffering=0) as myfile:\n",
    "#             outpt = myfile.read()\n",
    "#             if bool(len(myfile.read().splitlines()) > \\\n",
    "#                       0 and outpt != None):\n",
    "#                 return (outpt, myfile.close())[0]\n",
    "#             else:\n",
    "#                 return (b\"1\"[0], myfile.close())[0]\n",
    "# inpt = loadfiles(loadimages(xpu(zeprimes))).iloc[:, :-1]\n",
    "# tsheet = b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "#                                itm in row[1].values.tolist())\n",
    "#                       for row in inpt.iterrows())\n",
    "# inpt.shape, tsheet.splitlines().__len__(), tsheet.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#     return '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#             row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#             ['non'.encode(encoding)] + \\\n",
    "#             row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#             oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                         for row in evdf.fillna(\n",
    "#                                             'non'.encode(encoding)).iterrows())).replace(\n",
    "#             'non'.encode(encoding), str(np.nan).encode(encoding))\n",
    "\n",
    "# str(np.nan).encode(encoding)\n",
    "\n",
    "\n",
    "#     booltest = [itm[0] for itm in evdf.dropna().iteritems()\n",
    "#                 if all(itm[1].values == oddf.dropna()[itm[0]].values)]\n",
    "#     booltest = [itm[0][0] for itm in\n",
    "#                 tuple(zip(list(itm for itm in evdf.dropna().iteritems()),\n",
    "#                           list(itm for itm in oddf.dropna().iteritems())))\n",
    "#                if bool(all(itm[0][1].values) == all(itm[1][1].values))]\n",
    "#     tmp = [pd.Series(itm[0] + [str(np.nan).encode(encoding)] \\\n",
    "#                      + itm[1]).unique()\n",
    "#            if len(pd.Series(itm[0] + itm[1]).unique().tolist()) < nfields\n",
    "#            else pd.Series(itm[0] + itm[1]).unique()\n",
    "#            for itm in\n",
    "#            tuple(zip([line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[0]],\n",
    "#                     [line.split(delimiter) for line in\n",
    "#                      snif.evenodd(inpt.splitlines())[1]]))]\n",
    "#     return tmp\n",
    "#     evdf = df([line.decode().split()\n",
    "#                for line in tmp[0]])\n",
    "#     oddf = df([line.decode().split() for line in tmp[1]])\n",
    "\n",
    "#     return evdf.replace('None', np.nan)\n",
    "#     return pd.concat([evdf, df(oddf[[col[0] for col in\n",
    "#                                      enumerate(oddf.columns)\n",
    "#                                      if col[0] not in\n",
    "#                                      booltest]].iteritems())[1:]],\n",
    "#                      axis = 1)\n",
    "#pd.merge(evdf, oddf[list(oddf.columns)], on = booltest)\n",
    "# .drop(columns = booltest)\n",
    "#     booltest = [col for col in evdf.columns if\n",
    "#                 evdf[col].values.all() == oddf[col].values.all()]\n",
    "#     dup_test = [col for col in oddf.columns if evdf[col].values.all() == oddf[col].values.all()]\n",
    "\n",
    "#     return \"\\\\n\".join(\n",
    "#         [\n",
    "#             \"\\t\".join([itm for itm in line.split(=)])\n",
    "#             for line in newsheet.rename(\n",
    "#                 dict(enumerate(newsheet.columns))\n",
    "#             ).values.tolist()\n",
    "#         ]\n",
    "#     ).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(pd.Series)\n",
    "\n",
    "import sniffbytes as snif\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "from sniffbytes import fix_dup_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes = cimaq_infos.loc[[row[0] for row in cimaq_infos.iterrows()\n",
    "                                   if 'onset' in row[1].filename]].reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsheets = [fix_dup_index(fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter),\n",
    "                               row[1].encoding, row[1].has_header, row[1].delimiter, row[1].nfields)\n",
    "                             for row in cimaq_infos.iloc[:3, :].iterrows()]\n",
    "testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testsheets = cimaq_infos.iloc[:3, :]\n",
    "# def fix_na_reps(inpt: bytes, encoding: str = None, delimiter: bytes = None) - > bytes:\n",
    "#     return '\\\\n'.encode(encoding).join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "#                       delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "#                       line) for line in inpt.splitlines())\n",
    "from sniffbytes import get_bencod\n",
    "from sniffbytes import get_delimiter\n",
    "from sniffbytes import get_bytes\n",
    "from sniffbytes import get_lineterminator\n",
    "from sniffbytes import fix_na_reps\n",
    "\n",
    "def fix_dup_index(\n",
    "    inpt: Union[bytes, str, os.PathLike],\n",
    "    encoding: str = None,\n",
    "    hdr: bool = False,\n",
    "    delimiter: bytes = None,\n",
    "    nfields: int = None\n",
    ") -> bytes:\n",
    "    inpt = snif.get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    nfields = [nfields if nfields else snif.get_nfields(inpt, hdr)]\n",
    "    evdf, oddf = (df(line.split() for line in lines) for lines\n",
    "                  in snif.evenodd(inpt.splitlines()))\n",
    "    booltest = [itm[0] for itm in enumerate(\n",
    "                   tuple(zip([itm[1] for itm in\n",
    "                              evdf.iteritems()],\n",
    "                              [itm[1] for itm in\n",
    "                               oddf.iteritems()])))\n",
    "                if all(itm[1][0] == itm[1][1])]\n",
    "    datas = pd.concat((evdf[booltest], pd.Series((int(len(row[1].values)) == nfields[0] -1)\n",
    "                                         for row in evdf.iterrows()),\n",
    "                     oddf[booltest[-1]:]), axis = 1)\n",
    "    return b'\\n'.join(b'\\t'.join(itm.encode() for\n",
    "                               itm in row[1].values.tolist())\n",
    "                      for row in datas.iterrows())\n",
    "\n",
    "def fix_na_reps(inpt: bytes,\n",
    "                encoding: str = None,\n",
    "                delimiter: bytes = None,\n",
    "                lterm: bytes = None) -> bytes:\n",
    "    inpt = get_bytes(inpt)\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    delimiter = [delimiter if delimiter else get_delimiter(inpt, encoding)][0]\n",
    "    lterm = [lterm if lterm else get_lineterminator(inpt)][0]\n",
    "    return lterm.join(re.sub(delimiter+'{2,}'.encode(encoding),\n",
    "                      delimiter+str(np.nan).encode(encoding)+delimiter,\n",
    "                      line) for line in inpt.split(lterm))\n",
    "# newsheets = [fix_na_reps(row[1].bsheets, row[1].encoding, row[1].delimiter)\n",
    "#                              for row in testsheets.iterrows()]\n",
    "# testdfs = [df([line.split() for line in inpt.splitlines()]) for inpt in newsheets]\n",
    "# testdfs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fid = fix_dup_index(all_dup_indexes.bsheets[55],\n",
    "              all_dup_indexes.encoding[55],\n",
    "              all_dup_indexes.has_header[55],\n",
    "              all_dup_indexes.delimiter[55])\n",
    "\n",
    "fid\n",
    "#     '\\\\n'.encode(encoding).join(('\\t'.encode(encoding).join(\n",
    "#         row[1].values.tolist()[:booltest[-1]] + \\\n",
    "#         [str(np.nan).encode(encoding)] + \\\n",
    "#         row[1].values.tolist()[booltest[-1]:] + \\\n",
    "#         oddf.loc[row[0]].values.tolist()[booltest[-1]:])\n",
    "#                                  for row in evdf.iterrows()))\n",
    "#  for line in evdf.iterrows()fid[0][0].values.tolist()]\n",
    "# all(fid[0][0][3].values == fid[0][1][3].values)\n",
    "df((snif.evenodd((line.split() for line in\n",
    "                  all_dup_indexes.iloc[55].bsheets.splitlines()))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# fdi[0][3] == fdi[1][3]\n",
    "# def get_nfields(inpt: bytes, hdr: bool = None) -> bytes:\n",
    "#     inpt = [get_bytes(inpt).splitlines()[1:] if hdr else\n",
    "#             get_bytes(inpt).splitlines()][0]\n",
    "#     return pd.Series(len(line.split())\n",
    "#                       for line in inpt).max()\n",
    "#             if not hdr\n",
    "#             else pd.Series(len(line.split()) for line in\n",
    "#                            inpt.splitlines()[1:]).max()][0]\n",
    "# fix_dup_index(all_dup_indexes.bsheets[55])\n",
    "\n",
    "# [chr(itm) for itm in list(all_dup_indexes.delimiter[0])].__len__()\n",
    "# fxd = fix_dup_index()\n",
    "# fxd\n",
    "# all_dup_indexes['na_reps'] = [chr(list(row[1].bsheets)[-1])\n",
    "#                               for row in all_dup_indexes.iterrows()]\n",
    "\n",
    "\n",
    "# all_dup_indexes\n",
    "# all_dup_indexes['na_reps']\n",
    "# all_dup_indexes.bsheets['na_rep'] = [row[1].bsheets]\n",
    "#              encoding: str = None,\n",
    "#              hdr: bool = False,\n",
    "#              delimiter: bytes = None)\n",
    "# chardet.detect('�'.encode())\n",
    "# '�'.encode('utf32')\n",
    "# [line.split().__len__() for line in\n",
    "#  all_dup_indexes.bsheets[55].splitlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets'] = ['\\\\n'.encode(row[1].encoding).join(['\\t'.encode(row[1].encoding).join(\n",
    "                                   re.sub(b'\\\\s' + b'{2,}',\n",
    "                                          b'\\\\s',\n",
    "                                          re.sub(row[1].delimiter + b'{2,}',\n",
    "                                          row[1].delimiter + \\\n",
    "                                          str(np.nan).encode(row[1].encoding) + \\\n",
    "                                          row[1].delimiter,\n",
    "                                          line)) \\\n",
    "                                for line in row[1].bsheets.splitlines())])\n",
    "    for row in cimaq_infos.iterrows()]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['cleaned_sheets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dup_indexes['incr_delim'] = [Counter([snif.get_delimiter(line, 'ascii')\n",
    "                                          for line in row[1]['bsheets'].splitlines()]).most_common(1)[0][0]\n",
    "                                 for row in all_dup_indexes.iterrows()]\n",
    "all_dup_indexes['incr_delim'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['force_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "                             for row in cimaq_infos.iterrows()]\n",
    "# with open(pjoin(os.getcwd(), 'test.txt'), 'wb') as newfile:\n",
    "#     newfile.write(cimaq_infos.iloc[124]['bsheets'])\n",
    "#     newfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded'] = [row[1].force_utf8.decode()\n",
    "                          for row in cimaq_infos.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos['decoded']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_infos['newsheets'] = [snif.mkfrombytes(\n",
    "#                                 row[1].bsheets,\n",
    "#                                 encoding = row[1]['encoding'],\n",
    "#                                 delimiter = row[1]['delimiter'],\n",
    "#                                 hdr = row[1]['has_header'],\n",
    "#                                 dup_index = row[1]['dup_index']).decode(\n",
    "#                                     'utf8', 'replace').replace(\n",
    "#                                         '�', '').encode().decode().strip()\n",
    "                            \n",
    "                            \n",
    "#                              for row in tqdm(cimaq_infos.iterrows(),\n",
    "#                                              desc = 'repairing')]\n",
    "\n",
    "cimaq_infos['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n'\n",
    "                                            ))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace(\n",
    "                                        '�', '').encode().decode().strip().split('\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos.iloc[667]['as_df']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2 = pd.concat(val.T for val in cimaq_infos[0].values.flatten())\n",
    "cimaq_infos2 = cimaq_infos2.dropna()\n",
    "cimaq_infos2['newsheets'] = [snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode()\n",
    "                             for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                             desc = 'repairing')]\n",
    "\n",
    "cimaq_infos2['as_df'] = [df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n'))).T.set_index(0, drop = True).T\n",
    "                         if row[1].has_header else\n",
    "                         df((line.split('\\t') for line in\n",
    "                             snif.mkfrombytes(\n",
    "                                row[1].bsheets,\n",
    "                                encoding = row[1]['encoding'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                dup_index = row[1]['dup_index']).decode(\n",
    "                                    'utf8', 'replace').replace('�', '').encode().decode().strip().split(\n",
    "                                 '\\\\n')))\n",
    "                         for row in tqdm(cimaq_infos2.iterrows(),\n",
    "                                         desc = 'creating Pandas DataFrames')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['as_df'].sample(1).values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[99].as_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['to_csv'] = [pd.read_csv(StringIO(row[1].unidecoded),\n",
    "                                      sep = '\\t',\n",
    "                                      header = [0 if row[1].has_header\n",
    "                                                else None][0],\n",
    "                                      engine = 'c')\n",
    "#                                       quoting = 1)\n",
    "                              for row in cimaq_infos2.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2.iloc[667].to_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq_infos2['cleaned'] = ['\\n'.join(['\\t'.join(line.split()) for line in\n",
    "                                      snif.is_printable(row[1].newsheets.decode(\n",
    "                                          row[1].encoding).encode(\n",
    "                                          'utf8', 'replace').decode()).splitlines()]).encode().decode()\n",
    "                           for row in cimaq_infos2.iterrows()]\n",
    "cimaq_infos2['cleaned']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[pd.read_csv(StringIO(row[1].newsheets.decode(row[1].encoding)), sep = '\\t')\n",
    " for row in cimaq_infos2.iterrows()][666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(scanzip(loadimages(xpu(zeprimes))[0]).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chardet.detect(b'0xf8')#0xf\n",
    "# chr(int.from_bytes(b'0xf', sys.byteorder))\n",
    "# bytes(int.from_bytes(b'0xf8', sys.byteorder))\n",
    "# import codecs\n",
    "# help(codecs)\n",
    "[chr(item) for item in list(b'0xf')]\n",
    "[itm for itm in list]\n",
    "int.from_bytes(b'0xf', sys.byteorder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniffzip(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    scan_bytes(myzip.open(row[1].))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "        return pd.concat([df(snif.evenodd(repr(itm)[8:-1].strip().replace('=', ' ').split())).T[1]\n",
    "                          for itm in\n",
    "                (ZipFile(archv_path).__dict__['filelist'][1:],\n",
    "                ZipFile(archv_path).close())[0]\n",
    "                if '__MACOSX' not in repr(itm)], axis = 1).T\n",
    "\n",
    "# atest = list(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'][0].values())\n",
    "# btest = get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['filelist'][0]\n",
    "# atest# json.dumps(repr(get_zinfos(loadimages(xpu(zeprimes))[0])[0].loc['NameToInfo'].values.tolist()).split())\n",
    "# #.replace('=', '\":').replace('\\\\', '')\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])\n",
    "df(repr(itm[1])[8:-1].replace('=', ' ').split() for itm in\n",
    "      ZipFile(loadimages(xpu(zeprimes))[0]).__dict__['NameToInfo'].items()\n",
    "      if '__MACOSX' not in itm)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## GOOD ONE ######################333\n",
    "def get_zinfos(archv_path: Union[os.PathLike, str],\n",
    "               ntpl: Union[str, list, tuple] = [],\n",
    "               exclude: Union[str, list, tuple] = []) -> object:\n",
    "    return ((df(zip(tuple(itm.strip(\"'\") for itm in repr(\n",
    "               ZipFile(archv_path).getinfo(nm))[8:-1].strip().replace(\n",
    "                   b'='.decode(), ' ').split()#.replace(chr(34), '').split())\n",
    "                   for nm in snif.filter_lst_exc(\n",
    "                       exclude, [ntpl if ntpl\n",
    "                       else getnametuple(ZipFile(archv_path))][0])))),\n",
    "            ZipFile(archv_path).close())[0])\n",
    "\n",
    "get_zinfos(loadimages(xpu(zeprimes))[0])[0]\n",
    "#.replace('=', '\":').replace('\\\\', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ls_archv(apath: Union[str, os.PathLike], ntpl: Union[str, list, tuple] = []) -> object:\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zip_contents(\n",
    "    archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = [],\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    withbytes: bool = False,\n",
    "    to_sniff: bool = False,\n",
    "    to_close: bool = True,\n",
    ") -> object:\n",
    "    myzip = ZipFile(archv_path)\n",
    "    ntpl = snif.filter_list_exc(exclude,\n",
    "                                [ntpl if ntpl else snif.getnametuple(myzip)][0])\n",
    "\n",
    "    vals = (\n",
    "        df(\n",
    "            tuple(\n",
    "                dict(zip(evenodd(itm)[0], evenodd(itm)[1]))\n",
    "                for itm in tuple(\n",
    "                    tuple(\n",
    "                        force_ascii(repr(itm.lower()))\n",
    "                        .strip()\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"'\", \"\")\n",
    "                        .replace(\"=\", \" \")[:-2]\n",
    "                        .split()\n",
    "                    )[1:]\n",
    "                    for itm in set(\n",
    "                        repr(myzip.getinfo(itm))\n",
    "                        .strip(\" \")\n",
    "                        .replace(itm, itm.replace(\" \", \"_\"))\n",
    "                        if \" \" in itm\n",
    "                        else repr(myzip.getinfo(itm)).strip(\" \")\n",
    "                        for itm in ntpl\n",
    "                    )\n",
    "                )\n",
    "            ),\n",
    "            dtype=\"object\",\n",
    "        )\n",
    "        .sort_values(\"filename\")\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    vals[[\"src_name\", \"ext\"]] = [(nm, os.path.splitext(nm)[1]) for nm in ntpl]\n",
    "    vals[\"filename\"] = [\n",
    "        \"_\".join(\n",
    "            pd.Series(\n",
    "                row[1].filename.lower().replace(\"/\",\n",
    "                                                \"_\").replace(\"-\",\n",
    "                                                             \"_\").split(\"_\")\n",
    "            ).unique()\n",
    "            .__iter__()\n",
    "        )\n",
    "        for row in vals.iterrows()\n",
    "    ]\n",
    "    if exclude:\n",
    "        vals = vals.drop(\n",
    "            [\n",
    "                row[0]\n",
    "                for row in vals.iterrows()\n",
    "                if row[1].filename\n",
    "                not in filter_lst_exc(exclude, [itm.lower() for itm in vals.filename])\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    if withbytes:\n",
    "        vals[\"bsheets\"] = [\n",
    "#             snif.strip_null() \n",
    "            myzip.open(row[1].src_name).read().lower() for row in vals.iterrows()\n",
    "        ]\n",
    "        \n",
    "    if to_sniff:\n",
    "        vals[[\"encoding\", \"delimiter\", \"has_header\", \"width\", \"dup_index\", \"nrows\"]] = \\\n",
    "            [tuple(snif.scan_bytes(row[1].bsheets).values()) for row in vals.iterrows()]\n",
    "    if to_close:\n",
    "        myzip.close()\n",
    "        return vals\n",
    "    else:\n",
    "        return (myzip, vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# azpath = loadfiles(loadimages(xpu(zeprimes))).fpaths[0]\n",
    "# with zipfile.Zipfile(azpath) as myzip:\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =  dict((bname(apath.replace('-', '_').strip()),\n",
    "                    get_zip_contents(apath, withbytes = True,\n",
    "                         to_sniff = True, to_close = True))\n",
    "                    for apath in tqdm(loadimages(zeprimes),\n",
    "                          desc = 'scan sniff'))\n",
    "test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # from sniffbytes import no_ascii\n",
    "# # chardet.detect(no_ascii('bon matin tin�tin!').encode())\n",
    "# # Union[str, bytes]\n",
    "# ''.join([chr(int.from_bytes(int.from_bytes(itm.encode(), sys.byteorder)))#.encode(encoding)\n",
    "#  for itm in ])\n",
    "# # ==  dict(json.dumps(set(string.printable)))\n",
    "set([chr(int.from_bytes(itm, sys.byteorder)).encode('utf32')\n",
    " for itm in [itm.encode() for itm in list(string.printable)]])\n",
    "[int.from_bytes(itm.encode(), sys.byteorder) for itm in list(string.printable)[-2:]]\n",
    "# '\\x00'.encode('ISO-8859-1')\n",
    "encodings = ['Windows-1252', 'utf32', 'utf8', 'ISO-8859-1']\n",
    "[int.from_bytes(itm.encode(encoding), sys.byteorder) for itm in list(string.printable)]\n",
    "int.from_bytes(b'', sys.byteorder)\n",
    "chardet.detect(b'\\x00'), chardet.detect(b'None')\n",
    "is_printable = [itm.encode('utf32') for itm in list(string.printable)], '\\x00'.encode('ascii')\n",
    "\n",
    "def force_encoding(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    \"\"\"\n",
    "    Source: https://stackoverflow.com/questions/8689795/how-can-i-remove-non-ascii-characters-but-leave-periods-and-spaces-using-python\n",
    "    \"\"\"\n",
    "    notnull_printable = set(itm.encode(encoding) for itm in list(string.printable)\n",
    "                            if int.from_bytes(itm, sys.byteorder) != 0)\n",
    "#                             if chr(int.from_bytes(itm, sys.byteorder)) != \"\\x00\")\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "    return \"\".encode(encoding).join(filter(lambda x: x in notnull_printable, inpt))\n",
    "\n",
    "\n",
    "# is_printable = , '\\x00'.encode('ascii')\n",
    "# '0xff'.encode('utf32') in is_printable\n",
    "utf32p = ''.encode('utf32').join(set(itm.encode('utf32') for itm in list(string.printable)))\n",
    "windows1252p = ''.encode('Windows-1252').join(set(itm.encode('Windows-1252') for itm in list(string.printable)))\n",
    "utf8p = list(itm.encode('utf8') for itm in list(string.printable))\n",
    "utf32p = list(itm.encode('utf32') for itm in list(string.printable))\n",
    "test = set(itm.encode('utf32') for itm in list(string.printable)\n",
    "           if int.from_bytes(itm.encode('utf32'),\n",
    "                                 'little') != '\\x00'.encode('utf32'))\n",
    "# test == utf32set\n",
    "# force_encoding(utf8p, 'Windows-1252')\n",
    "# chr(0)\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in utf32set]\n",
    "\n",
    "astring = \"    Salut\\tbébé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\"\n",
    "chardet.detect((\"Salut bébé, mon nom c'est François!\").encode())\n",
    "# weird = list(chr(itm) for itm in tuple(''.join(utf32set).encode()))[-5]\n",
    "# astring.encode('ascii')\n",
    "\n",
    "# clean_astring = ''.join(itm for itm in list(astring) if\n",
    "#                         int.from_bytes(itm.encode('utf32'), sys.byteorder) != 0)\n",
    "\n",
    "def clean_bytes(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    return ''.join(itm for itm in list(inpt) if\n",
    "                        chr(int.from_bytes(itm)) != chr(0).encode(encoding) and itm in\n",
    "                       set(itm.encode(encoding) for itm in list(string.printable)))\n",
    "# chr(int.from_bytes(weird.encode('ascii'), sys.byteorder))\n",
    "# list(astring)\n",
    "# int.from_bytes('\\\\x00'.encode(), 'big')\n",
    "# int.from_bytes('\\\\x00'.encode(), sys.byteorder) == int.from_bytes('\\\\x00'.encode(), 'little')\n",
    "maybe_null == notnull_printable\n",
    "(astring, clean_astring)\n",
    "# '\\x00'.encode('utf32')\n",
    "nareps = [chr(0).encode('ascii'), chr(0).encode('utf32'), chr(0).encode('ISO-8859-1')]\n",
    "[int.from_bytes(itm, sys.byteorder) for itm in nareps]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astring = \"    \\\\\\tSalut\\tbé bé,\\tmon\\tnom\\tc'\\x00est\\sFrançois!\\S\"\n",
    "bstring = astring.encode('utf16')\n",
    "narep = chr(0).encode(snif.get_bencod(bstring))\n",
    "chr(0).encode('utf16') == chr(0).encode(snif.get_bencod(bstring))\n",
    "[chr(itm) for itm in list(bstring)]# chardet.detect('ç'.encode())\n",
    "# [chr(itm) for itm in list(bstring) if chr(itm) != ]\n",
    "narep, bstring, bstring.replace(narep, ''.encode(snif.get_bencod(bstring)))\n",
    "tuple(zip(list(astring), list(bstring)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chr(int.from_bytes(''.encode(), sys.byteorder)).encode('utf32'), chr(int.from_bytes('0'.encode(), sys.byteorder)).encode('utf32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# import bytes\n",
    "# # import format as fmt\n",
    "\n",
    "# def strip_null(inpt: bytes, nullrep: bytes = None, encoding: str = None):\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     nullrep = [nullrep if nullrep else snif.get_nullrep(inpt, encoding)][0]\n",
    "#     return {\n",
    "#         format(print(\"[%q]\",\n",
    "#                    bytes.Trim(byte(\" !!! Achtung! Achtung! !!! \"), \"! \")))}\n",
    "\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "#         ''.join(get_nullrep(inpt, encoding)), '').encode()\n",
    "    try:\n",
    "        return inpt.replace(chr(int.from_bytes(\n",
    "                   b\"\\x00\", sys.byteorder)).encode(encoding), ''.encode(encoding))\n",
    "    except UnidecodeError:\n",
    "        return inpt.replace(get_nullrep(inpt, encoding), ''.encode(encoding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbytes = b'bonjour\\xff\\xfe\\x00\\x00\\x00\\x00\\x00\\x00'\n",
    "chardet.detect(snif.strip_null(testbytes))#.encode('ISO-8859-1')\n",
    "chardet.detect(testbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_nullrep(inpt, encoding: str = None) -> bytes:\n",
    "#     return [itm for itm in list(inpt) if\n",
    "#             chr(int.from_bytes(itm, sys.byteorder)).encode(encoding) == \"\\x00\".encode(encoding)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "help(codecs.lookup('utf8').streamreader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest = [dict((subject[0], \n",
    "                df.from_dict(dict((itm for itm in\n",
    "                                   list(snif.scan_bytes(snif.force_utf8(\n",
    "                  row[1].bsheets, row[1].encoding)).items()) + \\\n",
    "                      [('filename', row[1].filename),\n",
    "                       (('pscid', 'dccid'), row[1].filename.split('_')[:2])])),\n",
    "                             orient = 'index'))\n",
    "               for row in tqdm(subject[1].iterrows(), desc = 'new sniff'))\n",
    "           for subject in list(test.items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newtest\n",
    "# # both = {**test, **newtest}\n",
    "# both = tuple({**itm[0], **itm[1]} for itm\n",
    "#              in )\n",
    "# both\n",
    "# both = tuple(zip([itm.values() for itm in test], [itm.values() for itm in newtest]))\n",
    "# both\n",
    "tuple(zip(test.values(), newtest.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nice_xtrct(archv_path: Union[os.PathLike, str],\n",
    "    ntpl: Union[str, list, tuple] = None,\n",
    "    to_xtrct: Union[str, list, tuple] = 'all',\n",
    "    exclude: Union[str, list, tuple] = [],\n",
    "    to_close: bool = True,\n",
    "    withbytes: bool = False,\n",
    "    dst_path: Union[os.PathLike, str] = None) -> object:\n",
    "    dst_path = [dst_path if dst_path\n",
    "                else pjoin(os.get_cwd(),\n",
    "                           os.path.splitext(bname(archv_path))[0])][0]\n",
    "    os.makedirs(dst_path, exist_ok=True)\n",
    "    myzip = zipfile.ZipFile(archv_path)\n",
    "    ntpl = filter_lst_exc([ntpl if ntpl else getnametuple(myzip)][0])\n",
    "    contents = get_zip_contents(archv_path, ntpl, excllude, to_xtrct,\n",
    "                                to_close = True, withbytes = True, to_sniff = True)\n",
    "    xtrct_lst = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename\n",
    "            in filter_lst_inc(to_xtrct, list(vals.filename), sort=True)\n",
    "        ]\n",
    "    ]\n",
    "    [\n",
    "        shutil.move(\n",
    "            myzip.extract(member=row[1].src_name, path=dst_path),\n",
    "            pjoin(\n",
    "                dst_path,\n",
    "                \"_\".join(\n",
    "                    pd.Series(\n",
    "                        row[1].filename.lower().replace(\"-\", \"_\").split(\"_\")\n",
    "                    ).unique()\n",
    "                ),\n",
    "            ),\n",
    "        )\n",
    "        for row in tqdm(xtrct_lst.iterrows(), desc=\"extracting\")\n",
    "    ]\n",
    "    vals = vals.loc[\n",
    "        [\n",
    "            row[0]\n",
    "            for row in vals.iterrows()\n",
    "            if row[1].filename not in xtrct_lst.values\n",
    "        ]\n",
    "    ]\n",
    "    removeEmptyFolders(dst_path, False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_printables = [chr(int.from_bytes(itm.encode(), sys.byteorder)).encode() for itm in string.printable]\n",
    "str(b_printables[-1]) == repr(b_printables[-1])\n",
    "\n",
    "def force_to(inpt, src_enc: str = None, dst_enc: str = 'utf8'):\n",
    "    inpt = get_bytes(inpt)\n",
    "    as_ints = [[line.split()]]\n",
    "    \n",
    "    \n",
    "chr(list(asheet)[0])\n",
    "list(asheet).__len__()\n",
    "# chr(int.from_bytes(b_printables[-1], sys.byteorder))\n",
    "\n",
    "# [chr(itm) for itm in list(asheet)]\n",
    "# help(chr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq_topdir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_03-19/derivatives/'\n",
    "# big = loadfiles([apath for apath in loadimages(xpu(cimaq_topdir))\n",
    "#                  if os.path.isfile(apath)])\n",
    "# big[['dccid', 'pscid']] = [([re.compile('\\d{6}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{6}').search(row[1].fname) != None\n",
    "#                              else None][0],\n",
    "#                             [re.compile('\\d{7}').search(row[1].fname).group()\n",
    "#                              if re.compile('\\d{7}').search(row[1].fname) != None\n",
    "#                              else None][0])\n",
    "#                            for row in big.iterrows()]\n",
    "'\\\\x0'.encode('utf16')\n",
    "chr(int.from_bytes(b\"\\0xff\", sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df(asheet)\n",
    "\n",
    "# test2 = pd.concat([itm[1] for itm in tqdm(sorted(list(test)), desc = 'concatenate')])\n",
    "\n",
    "# test2['forced_utf8'] = [snif.force_utf8(row[1].bsheets, row[1].encoding)\n",
    "#                         for row in test2.iterrows()]\n",
    "\n",
    "#.filename,\n",
    "#                    snif.scan_bytes(row[1].forced_utf8, encoding='utf'))\n",
    "#                         for row in itm[1].iterrows())\n",
    "#              for itm in tqdm(list(test.items()),  desc = 'scan & sniff utf8')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert newtest.loc['encoding'].all() == 'ascii' or 'UTF-8'\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder))\n",
    "\n",
    "b'n\\a' == 'n\\a'.encode()\n",
    "\n",
    "chr(int.from_bytes('n\\a'.encode(), sys.byteorder)).encode('utf8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_item(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Returns null byte representation as bytes in native file encoding'''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "#     rep = chr(list(inpt)[-1]).encode(encoding)\n",
    "    last1 = inpt.splitlines()[-1].split()[-1]\n",
    "    last2 = [chr(itm).encode(encoding) for itm in\n",
    "             list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]\n",
    "    return (last1, last2)\n",
    "#     return [chr(itm) for itm in list(chr(list(inpt.splitlines()[-1])[-1]).encode(encoding))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(chr(int.from_bytes(b'\\x00', sys.byteorder)))\n",
    "# snif.get_bencod(chr(0))\n",
    "int.from_bytes(b'\\x00', sys.byteorder)\n",
    "aguy = random.sample(list(test.values()), 1)\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aguy = list(test.values())[0]\n",
    "aguy['size_check'] = [int(row[1].file_size) == len(list(row[1].bsheets))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy['na_reps'] = [snif.get_nullrep(row[1].bsheets) for row in aguy.iterrows()]\n",
    "aguy['last_item'] = [last_item(row[1].bsheets, row[1].encoding) for row in aguy.iterrows()]\n",
    "\n",
    "aguy[['n_zbytes', 'n_scanbytes', 'chkup']] = [(int(len(list(row[1].bsheets))), int(row[1].file_size),\n",
    "                                      (int(len(list(row[1].bsheets))) == int(row[1].file_size)))\n",
    "                                     for row in aguy.iterrows()]\n",
    "aguy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "aguy['utf8len'] = [len(row[1].forced_utf8.splitlines()) for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_tophalf'] = [row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))] == \\\n",
    "                                 row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]\n",
    "                              for row in aguy.iterrows()]\n",
    "aguy['sheet_diff_bothalf'] = [[line.strip() for line in\n",
    "                               row[1].bsheets.splitlines()[:int(np.floor(len(\n",
    "                                 row[1].bsheets.splitlines())/2))]] == \\\n",
    "                                 [line.strip() for line in\n",
    "                                  row[1].forced_utf8.splitlines()[int(np.floor(len(\n",
    "                                     row[1].bsheets.splitlines())/2)):]]\n",
    "                              for row in aguy.iterrows()]\n",
    "\n",
    "aguy['nrows_test'] = [(len(row[1].bsheets.splitlines()) == len(row[1].forced_utf8.splitlines()))\n",
    "                      for row in aguy.iterrows()]\n",
    "aguy[['missing_line_index', 'missing_line_values']] = [(row[1].nrows_test)*2 if row[1].nrows_test else\n",
    "                        sorted([(line[0], line[1]) for line in enumerate(row[1].bsheets.splitlines()) if\n",
    "                                snif.force_utf8(line[1], 'utf8') not in row[1].forced_utf8.splitlines()])\n",
    "                       for row in aguy.iterrows()]\n",
    "# aguy['eq_lines'] = [len([line[0] for line in enumerate(tuple(zip(row[1].bsheets.splitlines(),\n",
    "#                       row[1].forced_utf8.splitlines()))) if line[1][0] == line[1][1]])\n",
    "#                     == len\n",
    "#                     for row in aguy.iterrows()]\n",
    "\n",
    "aguy.last_item.iloc[6][1][0].decode('utf16').encode('utf8').decode('utf8')\n",
    "aguy.forced_utf8, aguy.bsheets\n",
    "aguy['missing_line'].iloc[5]\n",
    "# aguy.iloc[5].bsheets.splitlines(), aguy.iloc[5].forced_utf8.splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asheet = aguy.iloc[4].bsheets[0]\n",
    "utfsheet = aguy.iloc[4].forced_utf8[0]\n",
    "asheet == utfsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asheet = aguy.sample(1).bsheets.values[0]\n",
    "# last_item(asheet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "enc = snif.get_bencod(abyte)\n",
    "nonull = strip_null(abyte, enc)\n",
    "nonull.splitllines()[:4], abyte.splitlines()[:4]\n",
    "# aguy['no_null'] = [strip_null(row[1].bsheets,\n",
    "#                               row[1].encoding)#.encode(row[1].encoding)\n",
    "#                    for row in aguy.iterrows()]\n",
    "# aguy['bprints'] = [snif.bytes_printable(row[1].bsheets).encode(snif.get_bencod(row[1].bsheets))\n",
    "#                  for row in aguy.iterrows()]\n",
    "# checkfx = []\n",
    "\n",
    "\n",
    "# list(abyte.splitlines()[-1])\n",
    "# [chr(itm).encode() for itm in\n",
    "#  [list(row[1].bsheets.splitlines()[-1])[-1]\n",
    "# for row in aguy.iterrows()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpt = list(test.values())[0].iloc[0].bsheets\n",
    "encoding = list(test.values())[0].iloc[0].encoding\n",
    "\n",
    "def strip_null(inpt: bytes, encoding: str = None) -> bytes:\n",
    "    ''' Remove null bytes from byte stream with proper representation\n",
    "        Adapted from:\n",
    "        https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "        All files end by a null byte, so the last byte in a file shows\n",
    "        how null bytes are represented within this file '''\n",
    "    encoding = [encoding if encoding else snif.get_bencod(inpt)][0]\n",
    "    return '\\n'.join(inpt.decode(encoding, 'replace').replace('�', '').replace(\n",
    "        ''.join(snif.get_nullrep(inpt, encoding)), '').splitlines())\n",
    "\n",
    "pd.read_csv(StringIO(snif.mkfrombytes(inpt.strip()).decode()), sep = '\\t')\n",
    "#     narep =''.encode(encoding).join([itm.encode(encoding) for itm in\n",
    "#                                            snif.get_nullrep(inpt, encoding)])\n",
    "#     return inpt.replace(narep, '|'.encode(encoding))\n",
    "#         inpt = inpt.replace(narep.encode(encoding),\n",
    "#                             '|'.encode(encoding)).replace('|'.encode(encoding),\n",
    "#                                                           ''.encode(encoding))\n",
    "#         return inpt\n",
    "#     return snif.bytes_printable(inpt.replace(,\n",
    "#                                              repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "# snif.mkfrombytes(strip_null(inpt, 'utf8').encode())\n",
    "# pd.read_csv(StringIO(snif.mkfrombytes(inpt).decode(encoding)),\n",
    "#             sep='\\t', header = [0 if snif.get_has_header(strip_null(inpt).encode(encoding)) else None][0])\n",
    "# inpt.decode(snif.get_bencod(strip_null(inpt)), 'ignore').replace('�', '')\n",
    "# pd.read_csv(StringIO(), sep='\\t')\n",
    "# [[list(itm for itm in line.split() if itm not in nareps)]\n",
    "#  for line in snif.bytes_printable(inpt)]\n",
    "# def get_na_reps(inpt: bytes, encoding: str = None) -> Union[list, int, bytes]:\n",
    "#     inpt = snif.bytes_printable(inpt)\n",
    "#     encoding = [encoding if encoding else snif.get_bencod(inpt)]\n",
    "#     return [chr(itm).encode(encoding) for itm\n",
    "#             in [chr(nulb) for nulb in\n",
    "#                 list(snif.get_nullrep(snif.bytes_printable(inpt)))][0]]\n",
    "\n",
    "# get_na_reps(inpt)\n",
    "# # [list(''.encode(encoding).join((line.split())))\n",
    "# #  for line in inpt.splitlines()]\n",
    "# chr(int.from_bytes('0xf8', sys.byteorder))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 3+5 ==9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiple_replace import multiple_replace\n",
    "\n",
    "def clear_nullbytes(inpt: bytes, encoding: str = None):\n",
    "    inpt = snif.bytes_printable(inpt)\n",
    "    encoding = snif.get_bencod(inpt)\n",
    "    toclear = dict((itm.encode(encoding), '|'.encode(encoding))\n",
    "                   for itm in\n",
    "                   )\n",
    "\n",
    "\n",
    "    for narep in nareps:\n",
    "        inpt = inpt.replace(narep, '|'.encode(encoding))\n",
    "    return inpt\n",
    "#     return inpt.replace('|'.encode(encoding), 'nan'.encode(encoding))\n",
    "# multiple_replace(toclear, snif.bytes_printable(abyte), encoding)\n",
    "# help(str.replace)\n",
    "# b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "# b_printable\n",
    "\n",
    "inpt = list(test.values())[0].iloc[8].bsheets\n",
    "encoding = list(test.values())[0].iloc[8].encoding\n",
    "# new = snif.bytes_printable(abyte).replace(tostrip, ''.encode(encoding))\n",
    "clear_nullbytes(inpt, encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def bytes_prntble(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Same as is_printable, but for bytes in native file encoding '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "\n",
    "#     b_printable = ''.encode(encoding).join([ch.encode(encoding)\n",
    "#                               for ch in list(string.printable)])\n",
    "#     return ''.encode(encoding).join([str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding) for ch in \n",
    "#                                     list(inpt) if str(chr(int.from_bytes(ch, sys.byteorder))).encode(encoding)\n",
    "#                                     in b_printable])\n",
    "    \n",
    "# def get_nullrep(inpt: bytes, encoding: str = None) -> bytes:\n",
    "#     ''' Returns null byte representation as bytes in native file encoding'''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     return bytes([inpt.splitlines(keepends = True)[-1][-1]]).decode(encoding).encode(encoding)\n",
    "\n",
    "# def strip_null(inpt: bytes, encoding: str = None, replace_val: str = None) -> bytes:\n",
    "#     ''' Remove null bytes from byte stream with proper representation\n",
    "#         Adapted from:\n",
    "#         https://stackoverflow.com/questions/21017698/converting-int-to-bytes-in-python-3\n",
    "#         All files end by a null byte, so the last byte in a file shows\n",
    "#         how null bytes are represented within this file '''\n",
    "#     encoding = [encoding if encoding else get_bencod(inpt)][0]\n",
    "#     repval = ['' if not replace_val else replace_val][0].encode(encoding)\n",
    "#     return bytes_prntble(inpt.replace(get_nullrep(inpt, encoding), repval).replace(chr(0).encode(encoding),\n",
    "#                                         repval).replace(chr(1).encode(encoding),\n",
    "#                                                         repval))\n",
    "\n",
    "# snif.get_nullrep(abyte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abyte = list(test.values())[0].iloc[1].bsheets\n",
    "enc = snif.get_bencod(abyte)\n",
    "# (abyte.decode(enc, 'replace').splitlines()[0], repr(abyte.decode(enc, 'replace').splitlines()[0]),\n",
    "#  repr(snif.force_utf8(abyte).splitlines()[0].decode())\n",
    "# )\n",
    "repr(abyte.decode(enc, 'replace'))\n",
    "def to_utf8(astring):\n",
    "    return unidecode(repr(astring.decode(enc, 'replace').replace('�', '').encode(\n",
    "              'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', ''))).encode()\n",
    "# len(to_utf8(abyte).splitlines()\n",
    "# )\n",
    "len(abyte.splitlines())\n",
    "new = b'\\n'.join([b'\\t'.join([to_utf8(itm) for itm in to_utf8(line).split()])\n",
    "            for line in abyte.splitlines()]).decode()\n",
    "\n",
    "df([' '.join(line.split(\"\\\\x00\")).split() for line in new.splitlines()])\n",
    "# clean_utf8 = '\\n'.join([['\\t'.join([to_utf8(itm) for itm in line.split()])]\n",
    "#               for line in abyte.splitlines()])\n",
    "# clean_utf8\n",
    "# newsheet = repr(abyte.replace(bytes(str(chr(0)), enc),\n",
    "#               str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x0'\", enc), str(np.nan).encode(enc)).replace(\n",
    "#                   bytes(\"'\\\\\\\\x'\", enc), ''.encode(enc)).replace(\n",
    "#                       bytes(str(np.nan), enc), str(\"\").encode(enc)).decode(\n",
    "#                           'ascii', 'replace').replace('�', '').replace('ÿ', ''))\n",
    "\n",
    "# cleaned = '\\n'.join(['\\t'.join([itm.replace('\\\\x0', ' ').strip().replace('\\\\x', ' ').strip() for itm in line.split()])\n",
    "#            for line in repr(newsheet).replace('\\\\x00', ' ').splitlines()]).strip()[1:].encode().decode()\n",
    "# pd.read_csv(StringIO(unidecode(cleaned)), sep='\\t')\n",
    "# [line.replace('\\\\x00', str(np.nan)).replace('\\\\x0', str(np.nan)) for line in newsheet.splitlines() if line != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abyte = list(test.values())[0].iloc[2].bsheets\n",
    "# encod = list(test.values())[0].iloc[2].encoding\n",
    "\n",
    "\n",
    "    \n",
    "enc = snif.get_bencod(snif.strip_null(abyte))\n",
    "# snif.strip_null(abyte)\n",
    "# [strip_null(line) for line in strip_null(bytes_prntble(abyte)).splitlines(keepends = True)]\n",
    "\n",
    "# get_nullrep(abyte)+b'1'\n",
    "# # int.from_bytes(b'\\\\x0', 'little'), int.from_bytes(b'\\\\x0', sys.byteorder), int.from_bytes(b'\\\\x0', 'big')\n",
    "\n",
    "# # newsheet = '\\n'.join(['\\t'.join([str(itm).replace(\"'\", \"\").replace(\"'\", '\"') for itm in\n",
    "# #                                  repr(line).replace('ÿ', '\\s').replace('\\\\x0', '\\s').replace(\n",
    "# #                                      '\\\\x', '\\s').replace('\\s', '').split()])\n",
    "# #                       for line in list(test.values())[0].iloc[5].bsheets.decode(\n",
    "# #                encod, 'replace').splitlines()]).splitlines()\n",
    "# # newsheet\n",
    "# # strtest = abyte.splitlines()[0]\n",
    "\n",
    "# repr(abyte.replace(repr(chr(0)).encode(encod), ''.encode(encod)).decode(encod, 'replace').replace('�', '')).encode(\n",
    "#     'ISO-8859-1', 'ignore').decode('ISO-8859-1', 'replace').replace('�', '').encode()\n",
    "# abyte.decode('ISO-8859-1').encode('ascii', 'ignore').decode('utf8', 'replace').replace('�', '')\n",
    "\n",
    "# unidecode(repr(abyte.decode(encod, 'replace').replace('�', '')))\n",
    "# is_printable(abyte.decode(encod, 'replace').replace('�', '').encode().replace(chr(0).encode(), ''.encode()).decode())\n",
    "\n",
    "# # new = [[unidecode(itm).encode('ascii', 'ignore').decode('ascii', 'replace').replace('�', '')\n",
    "# #         for itm in line.replace(repr(chr(0)),\n",
    "# #                                '').split()\n",
    "# #       if unidecode(is_printable(itm)) != '']\n",
    "# #      for line in abyte.decode(encod, 'replace').replace('�', '').splitlines()\n",
    "# #      if line != []]\n",
    "# # new[0], ''.join(new[0]).replace(repr(chr(0)).encode(encod), ' '.encode(encod))\n",
    "# new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'a' in string.printable #list(string.printable)\n",
    "\n",
    "chr(int.from_bytes(b'z', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(int.from_bytes(b'\\\\x00', 'big'))\n",
    "[chr(itm).encode() for itm in list(string.printable)]\n",
    "# chr(int.from_bytes('\\\\x0'.encode('Windows-1252'), sys.byteorder))\n",
    "# chr(int.from_bytes(b'\\\\x00', sys.byteorder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# abyte.decode('utf8', 'replace').replace('�', '').encode().decode()\n",
    "# unidecode(abyte.decode(snif.get_bencod(abyte), 'replace').replace('�', '')).encode()\n",
    "snif.force_utf8(abyte, enc).splitlines()\n",
    "clean1 = re.sub('\\\\x00', '', abyte.decode('ascii', 'replace').replace('�', '').encode('utf16').decode('utf8', 'replace'))\n",
    "# unidecode(clean1.encode().decode()).encode('utf16', 'replace')\n",
    "clean1\n",
    "[re.sub(bytes(str(np.nan), enc), bytes('', enc),\n",
    "        re.sub(bytes(\"\\x00\", enc),\n",
    "               bytes(str(np.nan), enc),\n",
    "               line)).decode('utf8', 'replace').replace('�', '').replace('\\\\', '')#unidecode(line.decode(enc, 'replace'))).encode('ascii', 'replace').decode('utf8', 'replace').replace('�', '')\n",
    "  for line in abyte.splitlines()][0].split()[0]\n",
    "# snif.force_utf8(abyte.decode(enc, 'replace'))\n",
    "#.decode(enc, 'replace')]\n",
    "# re.sub('\\x0', '', clean1)\n",
    "# unidecode(\n",
    "#           .replace('\\x00', '').replace(\n",
    "#     '\\\\x0', '').replace('\\\\x', ''))\n",
    "#.replace(b\"\\x\".decode(), '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chr(0)\n",
    "# str(\"\\xff\")\n",
    "chr(1).encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes('n\\a', 'Windows-1252')\n",
    "unidecode(b'\\x0b\\x0c'.decode(enc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_tsvs(drv_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_drv_dir_paths(\n",
    "#                     drv_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_drv_dir_paths(\n",
    "#                             drv_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths'])`.set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_drv_dir_paths(drv_dir)[0].zeprimes.fpaths,\n",
    "#                    get_drv_dir_paths(drv_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(drv_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                        [loadfiles(loadimages(row[1].fpaths)).dropna(axis = 0).T\n",
    "#                         for row in get_drv_dir_paths(drv_dir)[0].T.iloc[2: 4].iterrows()]).T\n",
    "#     test = dict((grp, dict((sgrp, cimaq.groupby('subids').get_group(grp).groupby(\n",
    "#                'fname').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "#                    'subids').get_group(grp).groupby('fname').groups))\n",
    "#                 for grp in allscans.groupby('subids').groups)\n",
    "# #     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "# #                       [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "# #                 columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "# #                                   for cimaqrow in get_drv_dir_paths(drv_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "# #                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "# #     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'stereonl', 4: 'behavioral',\n",
    "# #                                     5: 'confounds', 6: 'events', 7:'func'})\n",
    "#     return test\n",
    "# cimaq = load_tsvs(drv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq = fetch_cimaq(xpu(drv_dir))\n",
    "#cimaq.sort_values('dccid').set_index('dccid', drop = True).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fetch_cimaq(drv_dir):\n",
    "#     qc_ok = sorted([str(itm[0]) for itm in\n",
    "#                 pd.read_csv(get_drv_dir_paths(\n",
    "#                     drv_dir)[0].mean_qc.fpaths, sep='\\t').values])\n",
    "#     qc_ok\n",
    "#     to_exclude = df(sorted([(str(bname(itm).split('_')[0]),\n",
    "#                          str(bname(itm).split('_')[1]), itm) for itm in\n",
    "#                         loadimages(get_drv_dir_paths(\n",
    "#                             drv_dir)[0].zeprimes.fpaths)\n",
    "#                         if str(bname(itm).split('_')[1]) not in qc_ok]),\n",
    "#                    columns = ['pscid', 'dccid', 'fpaths']).set_index(\n",
    "#                        'dccid').sort_index().reset_index().fpaths.tolist()\n",
    "\n",
    "#     repair_dataset(get_drv_dir_paths(drv_dir)[0].zeprimes.fpaths,\n",
    "#                    get_drv_dir_paths(drv_dir)[0].temp_events_dir.fpaths,\n",
    "#                    exclude = ['pratique', 'practice', '.pdf', '.edat2'] + to_exclude)\n",
    "#     allids = repair_enc_task(drv_dir)\n",
    "#     pscids, dccids, subids = allids.pscid, allids.dccid, allids.subids\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + [loadfiles(loadimages(cimaqpath)).dropna(axis = 0)['fpaths']\n",
    "#                                   for cimaqpath in get_drv_dir_paths(drv_dir)[0].loc['fpaths'][2: 5]],\n",
    "#                       axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "#     cimaq = cimaq.rename(columns = {0: 'subid', 1: 'pscid', 2: 'dccid', 3: 'behavioral', 4: 'confounds',\n",
    "#                                     5: 'events'})\n",
    "#     return cimaq.set_index('dccid').sort_index()#.reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scans(drv_dir):\n",
    "    cimaq = fetch_cimaq(drv_dir)\n",
    "\n",
    "#     cimaq = pd.concat([subids, pscids, dccids] + \\\n",
    "#                   [loadfiles(loadimages(cimaqrow[1].fpaths)).dropna(axis = 0).rename(\n",
    "#             columns = {'fpaths': cimaqrow[1].suffixes})#['fpaths']\n",
    "#                               for cimaqrow in get_drv_dir_paths(drv_dir)[0].T.iloc[1: 6].iterrows()],\n",
    "#                   axis = 1).dropna(axis = 0).T.reset_index(drop = True).T\n",
    "    \n",
    "    allscans = pd.concat([loadfiles(loadimages(pjoin(dname(xpu(drv_dir)),\n",
    "                                                     sub))).dropna(axis = 0).T\n",
    "                          for sub in [itm for itm in ls(dname(xpu(drv_dir)))\n",
    "                        if itm.startswith('sub-')]], axis = 1).T\n",
    "    allscans[['dccid', 'modality', 'general']] = [(bname(row[1].fpaths).split('_')[0].split('-')[1],\n",
    "                                                   bname(row[1].fpaths).split('_')[-1],\n",
    "                                                   bname(dname(row[1].fpaths)))\n",
    "                         for row in allscans.iterrows()]\n",
    "\n",
    "    test = dict((grp, dict((sgrp, allscans.groupby('dccid').get_group(grp).groupby(\n",
    "               'general').get_group(sgrp)) for sgrp in allscans.groupby(\n",
    "                   'dccid').get_group(grp).groupby('general').groups))\n",
    "                for grp in allscans.groupby('dccid').groups)\n",
    "    allscans = df.from_dict(test, orient = 'index').sort_index()\n",
    "    indexes = set.intersection(set(cimaq.index), set(allscans.index)) \n",
    "    return pd.concat([allscans.loc[indexes], cimaq.loc[indexes]], axis = 0)\n",
    "test = load_scans(drv_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test2.iloc[0].fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = [pd.concat([row[1][['fname']], df.from_dict(json_read(row[1].fpaths, 'r'), orient = 'index')])\n",
    "#           for row in test[0].iterrows()\n",
    "#           if row[1].ext == '.json']\n",
    "# # allparams = pd.concat(((itm.groupby('fname').get_group(grp)\n",
    "# #                  for grp in itm.groupby('fname').groups)\n",
    "# #              for itm in allparams), axis = 1)\n",
    "# # pd.concat(params, axis =1).T.sort_values('fname')\n",
    "# pd.concat(params, axis = 1).T.set_index('fname')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "    \"\"\"\n",
    "    Context manager to set the timestamp of the path to plus or\n",
    "    minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "    if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "    \"\"\"\n",
    "    stats = os.stat(path)\n",
    "    if set_new:\n",
    "        new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "    else:\n",
    "        new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
