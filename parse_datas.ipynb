{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "373it [01:38,  3.78it/s]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "from cimaq_utils import *\n",
    "from cimaq_utils import loadimages\n",
    "from blind_rename import *\n",
    "from inspect_misc_text import *\n",
    "from zipctl import uzipfiles\n",
    "from os.path import expanduser as xpu\n",
    "from collections import OrderedDict\n",
    "\n",
    "def fix_cimaq(cimaq_dir):\n",
    "    # Paths to files and direcotries\n",
    "    cimaq_dir = xpu(cimaq_dir)\n",
    "    dlst = get_cimaq_dir_paths(cimaq_dir)\n",
    "\n",
    "    # Name and indexing patterns and prefixes\n",
    "    mean_sheets_patterns = (('dccid', '(?<!\\d)\\d{6}(?!\\d)'),\n",
    "                            ('pscid', '(?<!\\d)\\d{7}(?!\\d)'))\n",
    "    prefixes = pd.Series(('_'.join(val.split('_')[:2])\n",
    "                          for val in loadfiles(\n",
    "                              indir=dlst['uzeprimes'][0]).fname)).unique()\n",
    "    indiv_patterns = tuple(item for item\n",
    "                               in dict(zip(prefixes, prefixes)).items())\n",
    "    \n",
    "    # Structure file paths and participant identifiers in a DataFrame\n",
    "    events_sheets = loadfiles(indir=dlst['uzeprimes'][0])\n",
    "    confounds_sheets = loadfiles(dlst['confdir'][0])\n",
    "    events_sheets['pscid'] = [re.compile('(?<!\\d)\\d{7}(?!\\d)').search(fname).group()\n",
    "                       for fname in events_sheets.fname]\n",
    "    confounds_sheets['dccid'] = [re.compile('(?<!\\d)\\d{6}(?!\\d)').search(fname).group()\n",
    "                       for fname in loadfiles(dlst['confdir'][0]).fname]\n",
    "    \n",
    "    # Get parsing, dialect and encoding information with 'get_infos' function\n",
    "    events_infos = df((get_infos(fpath) for fpath in events_sheets.fpaths))\n",
    "    confounds_infos = df((get_infos(fpath) for fpath in confounds_sheets.fpaths))\n",
    "    events_infos = sortmap(pd.merge(events_infos, events_sheets,\n",
    "                                    how='outer'), indiv_patterns)\n",
    "    confounds_infos = pd.merge(confounds_infos, confounds_sheets, how='outer')\n",
    "    mean_parsing_infos = df((get_infos(fpath) for fpath in\n",
    "                             loadfiles(pathlist = dlst['mean_paths']).fpaths))\n",
    "    \n",
    "    # Group information about participants who passed QC assessment\n",
    "    new_mean_sheets = OrderedDict((row[1]['fpaths'],\n",
    "                                   prep_sheet(filename = row[1]['fpaths'],\n",
    "                                encoding=row[1]['encoding'],\n",
    "                                hdr = row[1]['has_header'],\n",
    "                                n_fields = row[1]['n_fields'],\n",
    "                                delimiter = row[1]['delimiter'],\n",
    "                                dupindex = row[1]['dup_index'],\n",
    "                                row_breaks = row[1]['row_breaks'],\n",
    "                                r_rowbreaks = row[1]['r_rowbreaks'],\n",
    "                                n_lines = row[1]['n_lines'],\n",
    "                                width = row[1]['width'],\n",
    "                                lineterminator = row[1]['lineterminator']))\n",
    "                     for row in mean_parsing_infos.iterrows())\n",
    "    new_mean_sheets = dict(zip(new_mean_sheets.keys(),\n",
    "                               tuple(sheet.convert_dtypes(str)\n",
    "                                     for sheet in rename_imposter_cols(\n",
    "                   [sheet for sheet in new_mean_sheets.values()],\n",
    "                   mean_sheets_patterns))))\n",
    "    min_ind = df([sheet.dccid for sheet in new_mean_sheets.values()\n",
    "               if sheet.shape[0] == pd.Series(\n",
    "                   [sheet.shape[0] for sheet in\n",
    "                    new_mean_sheets.values()]).min()]).T\n",
    "    inds = [sheet[1].dccid.tolist() for sheet in new_mean_sheets.items()]\n",
    "    new_mean_sheets = dict((item[0], item[1].set_index(\n",
    "                          'dccid').loc[min_ind.dccid].reset_index(drop=False))\n",
    "                           for item in new_mean_sheets.items())\n",
    "    renamer = [sheet[['dccid', 'pscid']] for sheet in new_mean_sheets.values()\n",
    "               if 'dccid' and 'pscid' in sheet.columns][0].astype(str).dropna()\n",
    "    events_infos = pd.merge(events_infos, renamer, on='pscid', how='outer').dropna(how='any')\n",
    "    confounds_infos = pd.merge(confounds_infos, renamer, on='dccid', how='outer').dropna(how='any')\n",
    "    all_infos = pd.concat([events_infos, confounds_infos]).fillna(False).reset_index(drop=True)\n",
    "    all_infos['confounds'] = ['confounds' in fname for fname in all_infos.fname]\n",
    "    all_infos['prefix'] = [find_key(row[1][['output_responses', 'onset_event',\n",
    "                               'output_retrieval', 'confounds']].T.to_dict(), True)\n",
    "                           for row in all_infos.iterrows()]\n",
    "    all_infos.to_csv(join(cimaq_dir, 'all_parsing_infos.csv'), index=False)\n",
    "\n",
    "    # Use 'prep_sheet' function to fix errors by reading data as a stream of bytes\n",
    "    new_events = tuple((''.join(['sub-', row[1]['pscid'], '-', row[1]['dccid'],\n",
    "                                '_', row[1]['prefix'], '.csv']),\n",
    "                        prep_sheet(filename = row[1]['fpaths'],\n",
    "                                   encoding=row[1]['encoding'],\n",
    "                                   hdr = row[1]['has_header'],\n",
    "                                   n_fields = row[1]['n_fields'],\n",
    "                                   delimiter = row[1]['delimiter'],\n",
    "                                   dupindex = row[1]['dup_index'],\n",
    "                                   row_breaks = row[1]['row_breaks'],\n",
    "                                   r_rowbreaks = row[1]['r_rowbreaks'],\n",
    "                                   n_lines = row[1]['n_lines'],\n",
    "                                   width = row[1]['width'],\n",
    "                                   lineterminator = row[1]['lineterminator']))\n",
    "                       for row in tqdm(all_infos.iterrows()))\n",
    "    newdir = join(cimaq_dir, 'cleaned_events_and_confounds')\n",
    "    os.makedirs(newdir, exist_ok=True)\n",
    "    next((itm[1].to_csv(join(newdir, itm[0]), index=False)\n",
    "          for itm in new_events))\n",
    "    \n",
    "def main():\n",
    "    fix_cimaq(cimaq_dir = '~/../../media/francois/seagate_1tb/cimaq_03-19/cimaq_derivatives')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__reversed__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'clear',\n",
       " 'copy',\n",
       " 'fromkeys',\n",
       " 'get',\n",
       " 'items',\n",
       " 'keys',\n",
       " 'move_to_end',\n",
       " 'pop',\n",
       " 'popitem',\n",
       " 'setdefault',\n",
       " 'update',\n",
       " 'values']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
