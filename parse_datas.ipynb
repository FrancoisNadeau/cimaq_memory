{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fnadeau/.local/lib/python3.6/site-packages/nilearn-0.7.0-py3.6.egg/nilearn/glm/__init__.py:56: FutureWarning: The nilearn.glm module is experimental. It may change in any future release of Nilearn.\n",
      "  'It may change in any future release of Nilearn.', FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import chardet\n",
    "import csv\n",
    "import json\n",
    "import nibabel as nib\n",
    "import nilearn\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import regex as re\n",
    "import shutil\n",
    "import string\n",
    "import struct\n",
    "import sys\n",
    "\n",
    "from chardet import UniversalDetector as udet\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "from io import StringIO\n",
    "from io import BytesIO\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "from os.path import join as pjoin\n",
    "from pandas import DataFrame as df\n",
    "from string import printable\n",
    "from typing import Union\n",
    "from zipfile import ZipFile\n",
    "from tqdm import tqdm\n",
    "from typing import Generator\n",
    "# from typing import list_iterator\n",
    "from unidecode import unidecode\n",
    "from operator import itemgetter\n",
    "\n",
    "from nilearn.glm.first_level import make_first_level_design_matrix as mfldm\n",
    "from nilearn.glm.first_level import first_level_from_bids\n",
    "from nilearn.plotting import plot_design_matrix as plot1dm\n",
    "\n",
    "from sniffbytes import flatten\n",
    "from sniffbytes import loadfiles\n",
    "from sniffbytes import loadimages\n",
    "\n",
    "from bidsify_utils import bidsify_load_scans\n",
    "from bidsify_utils import bidsify_names\n",
    "\n",
    "# from json_read import json_read\n",
    "\n",
    "import sniffbytes as snif\n",
    "\n",
    "from scanzip import scanzip\n",
    "# from fetch_cimaq import fetch_cimaq\n",
    "\n",
    "\n",
    "from removeEmptyFolders import removeEmptyFolders\n",
    "# from multiple_replace import multiple_replace\n",
    "\n",
    "from cimaq import cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning archive: 100%|██████████| 94/94 [00:01<00:00, 47.18it/s]\n",
      "indexing participants: 282it [00:00, 594.56it/s]\n",
      "cleaning: 282it [00:05, 50.83it/s]\n",
      "grouping participants: 100%|██████████| 94/94 [00:00<00:00, 431.61it/s]\n",
      "fetching CIMAQ: 100%|██████████| 94/94 [00:01<00:00, 49.13it/s]\n",
      "loading confounds: 100%|██████████| 94/94 [00:00<00:00, 111.81it/s]\n",
      "loading subjects: 100%|██████████| 94/94 [00:00<00:00, 413.90it/s]\n"
     ]
    }
   ],
   "source": [
    "cimaq_dir = xpu('~/../../data/simexp/datasets/cimaq_03-19/')\n",
    "# from fetch_cimaq import cimaq\n",
    "cimaq = cimaq.fetch(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subid</th>\n",
       "      <th>events</th>\n",
       "      <th>behav</th>\n",
       "      <th>subid</th>\n",
       "      <th>subid</th>\n",
       "      <th>events</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3025432_658178</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                          stim old...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>108391</td>\n",
       "      <td>filena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3123186_920577</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                       stim oldnum...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>120839</td>\n",
       "      <td>filena...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3163875_199801</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                       stim oldnum...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>122922</td>\n",
       "      <td>filen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3291977_748676</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                            stim o...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>127228</td>\n",
       "      <td>filen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3400972_956130</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                             stim ...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>139593</td>\n",
       "      <td>filen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>9003010_785245</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                           stim ol...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>979001</td>\n",
       "      <td>filename...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>9249304_778749</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                          stim old...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>983291</td>\n",
       "      <td>filename...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>9296157_955548</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                             stim ...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>988602</td>\n",
       "      <td>file...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>9494076_630120</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                            stim o...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>996599</td>\n",
       "      <td>filename...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>9929164_197192</td>\n",
       "      <td>trialnumber trial_type trialcode oldnumber...</td>\n",
       "      <td>category                            stim o...</td>\n",
       "      <td>motion_tx  motion_ty  motion_tz  motion_r...</td>\n",
       "      <td>998166</td>\n",
       "      <td>filen...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>94 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             subid                                             events  \\\n",
       "0   3025432_658178      trialnumber trial_type trialcode oldnumber...   \n",
       "1   3123186_920577      trialnumber trial_type trialcode oldnumber...   \n",
       "2   3163875_199801      trialnumber trial_type trialcode oldnumber...   \n",
       "3   3291977_748676      trialnumber trial_type trialcode oldnumber...   \n",
       "4   3400972_956130      trialnumber trial_type trialcode oldnumber...   \n",
       "..             ...                                                ...   \n",
       "89  9003010_785245      trialnumber trial_type trialcode oldnumber...   \n",
       "90  9249304_778749      trialnumber trial_type trialcode oldnumber...   \n",
       "91  9296157_955548      trialnumber trial_type trialcode oldnumber...   \n",
       "92  9494076_630120      trialnumber trial_type trialcode oldnumber...   \n",
       "93  9929164_197192      trialnumber trial_type trialcode oldnumber...   \n",
       "\n",
       "                                                behav  \\\n",
       "0       category                          stim old...   \n",
       "1       category                       stim oldnum...   \n",
       "2       category                       stim oldnum...   \n",
       "3       category                            stim o...   \n",
       "4       category                             stim ...   \n",
       "..                                                ...   \n",
       "89      category                           stim ol...   \n",
       "90      category                          stim old...   \n",
       "91      category                             stim ...   \n",
       "92      category                            stim o...   \n",
       "93      category                            stim o...   \n",
       "\n",
       "                                                subid   subid  \\\n",
       "0        motion_tx  motion_ty  motion_tz  motion_r...  108391   \n",
       "1        motion_tx  motion_ty  motion_tz  motion_r...  120839   \n",
       "2        motion_tx  motion_ty  motion_tz  motion_r...  122922   \n",
       "3        motion_tx  motion_ty  motion_tz  motion_r...  127228   \n",
       "4        motion_tx  motion_ty  motion_tz  motion_r...  139593   \n",
       "..                                                ...     ...   \n",
       "89       motion_tx  motion_ty  motion_tz  motion_r...  979001   \n",
       "90       motion_tx  motion_ty  motion_tz  motion_r...  983291   \n",
       "91       motion_tx  motion_ty  motion_tz  motion_r...  988602   \n",
       "92       motion_tx  motion_ty  motion_tz  motion_r...  996599   \n",
       "93       motion_tx  motion_ty  motion_tz  motion_r...  998166   \n",
       "\n",
       "                                               events  \n",
       "0                                           filena...  \n",
       "1                                           filena...  \n",
       "2                                            filen...  \n",
       "3                                            filen...  \n",
       "4                                            filen...  \n",
       "..                                                ...  \n",
       "89                                        filename...  \n",
       "90                                        filename...  \n",
       "91                                            file...  \n",
       "92                                        filename...  \n",
       "93                                           filen...  \n",
       "\n",
       "[94 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "scanning archive: 100%|██████████| 94/94 [00:01<00:00, 50.67it/s]\n",
      "indexing participants: 282it [00:00, 598.46it/s]\n",
      "cleaning: 282it [00:05, 51.55it/s]\n",
      "grouping participants: 100%|██████████| 94/94 [00:00<00:00, 429.70it/s]\n",
      "fetching CIMAQ: 100%|██████████| 94/94 [00:01<00:00, 47.99it/s]\n",
      "loading confounds: 100%|██████████| 94/94 [00:00<00:00, 122.62it/s]\n",
      "loading subjects: 100%|██████████| 94/94 [00:00<00:00, 405.41it/s]\n"
     ]
    }
   ],
   "source": [
    "cimaq_dir = xpu('~/../../data/simexp/datasets/cimaq_03-19/')\n",
    "\n",
    "\n",
    "class CIMAQ:\n",
    "    @ classmethod\n",
    "    def fetch(self, cimaq_dir: Union[str, os.PathLike]):\n",
    "        return fetch_cimaq(cimaq_dir)\n",
    "\n",
    "cimaq = CIMAQ.fetch(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nilearn First Level Matrix parameters explained:\n",
    "- https://nilearn.github.io/modules/generated/nilearn.glm.first_level.make_first_level_design_matrix.html#nilearn.glm.first_level.make_first_level_design_matrix\n",
    "\n",
    "\n",
    "#### The pd.DataFrame instance might have these keys:\n",
    "##### \"onset\" and \"duration\" are mandatory\n",
    "\n",
    "- ‘onset’: column to specify the start time of each events in\n",
    "   seconds. An error is raised if this key is missing.\n",
    "\n",
    "- ‘trial_type’: column to specify per-event experimental conditions\n",
    "   identifier. If missing each event are labelled ‘dummy’ and considered to form a unique condition.\n",
    "\n",
    "- ‘duration’: column to specify the duration of each events in\n",
    "   seconds. If missing the duration of each events is set to zero.\n",
    "\n",
    "- ‘modulation’: column to specify the amplitude of each\n",
    "   events. If missing the default is set to ones(n_events).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Encoding Type Matters\n",
    "\n",
    "- See how encoding type formats the same \"null-byte value\" differently \\\n",
    "  when printed to be human readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\"\\x00\".encode('utf8'), \"\\x00\".encode('utf16'), \"\\x00\".encode('utf32'),\n",
    "        \"\".encode('utf8'), \"\".encode('utf16'), \"\".encode('utf32'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "help(namedtuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from os.path import basename as bname\n",
    "from os.path import dirname as dname\n",
    "from os.path import expanduser as xpu\n",
    "from os import listdir as ls\n",
    "\n",
    "from typing import Union\n",
    "from pandas import DataFrame as df\n",
    "import sniffbytes as sniff\n",
    "from bidsify_utils import bidsify_load_scans\n",
    "from bidsify_utils import bidsify_names\n",
    "\n",
    "def xtrct_cimaq(cimaq_dir: Union[str, os.PathLike]) -> np.ndarray:\n",
    "    return df(tuple(scanzip(apath,\n",
    "                            exclude = ['Practice', 'Pratique',\n",
    "                                       'PRATIQUE', 'PRACTICE', 'READ',\n",
    "                                       'Encoding-scan', 'Retrieval-'],\n",
    "                            to_xtrct = ['.pdf', '.edat2'],\n",
    "                            dst_path = pjoin(os.getcwd(), 'newdevs',\n",
    "                                             'cimaq_uzeprimes'))\n",
    "                    for apath in\n",
    "                    tqdm(snif.filter_lst_inc(snif.clean_bytes(\n",
    "                        xpu(pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                                  'participants/sub_list_TaskQC.tsv'))).decode(\n",
    "                        ).split()[1:], snif.loadimages(xpu(pjoin(\n",
    "                        cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                        'task_files/zipped_eprime')))))))[0].values.flatten()\n",
    "\n",
    "def fix_cimaq(cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newtest', 'events'), exist_ok = True)\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newtest', 'behavioural'), exist_ok = True)\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newdevs', 'cimaq_uzeprimes'), exist_ok = True)\n",
    "    for val in tqdm(xtrct_cimaq(cimaq_dir), 'fixing cimaq'):\n",
    "        # creating events files\n",
    "        (pd.concat([snif.bytes2df(val['bsheets'].values[1],\n",
    "                                  has_header = None),\n",
    "                    snif.bytes2df(val['bsheets'].values[0]).loc[:snif.bytes2df(\n",
    "                        val['bsheets'].values[1],\n",
    "                        has_header = None).shape[0] -1, :].drop(\n",
    "                        columns = [0,1,2,3, 4, 6],\n",
    "                        index = 0).rename(columns = {5: 'onset',\n",
    "                                                     7: 'fix_onset',\n",
    "                                                     8: 'fix_duration'})],\n",
    "                   axis = 1).to_csv(pjoin(os.getcwd(), 'newtest', 'events', 'sub-_' + \\\n",
    "                                          '_'.join(val['filename'].values[0].replace('-', '_').split(\n",
    "            '_')[:2])+'_run-01_task-encoding_events.tsv'), sep = '\\t', index = None),\n",
    "         # creating behavioural files\n",
    "         snif.bytes2df(val['bsheets'].values[2], has_header = True).to_csv(pjoin(\n",
    "             os.getcwd(), 'newtest', 'behavioural', 'sub-_' + '_'.join(\n",
    "                 val['filename'].values[0].replace('-', '_').split('_')[:2]) + \\\n",
    "                 '_run-01_task-encoding_behavioural.tsv'), sep = '\\t', index = None))\n",
    "\n",
    "def cimaq2nilearn(cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "    fix_cimaq(cimaq_dir)\n",
    "    events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "                           os.getcwd(), 'newtest', 'events')))\n",
    "    os.makedirs(pjoin(os.getcwd(), 'newtest', 'nilearn_events'), exist_ok = True)\n",
    "    for row in tqdm(events.iterrows(), desc = 'conforming events.tsv files'):\n",
    "        sheet = pd.read_csv(row[1].fpaths, sep = '\\t')\n",
    "        \n",
    "        sheet['duration'] = sheet['onset'] + \\\n",
    "                                    sheet['fix_duration'] + \\\n",
    "                                    sheet['onset'].sub(\n",
    "                                    sheet['fix_onset']).round(0)\n",
    "        sheet.rename(columns = {'category': 'trial_type'}).set_index(\n",
    "            'trialnumber').to_csv(pjoin(os.getcwd(), 'newtest',\n",
    "                                        'nilearn_events', row[1].filename + '.tsv'),\n",
    "                                  sep = '\\t', index = None)\n",
    "\n",
    "def fetch_cimaq(cimaq_dir: Union[str, os.PathLike]) -> dict:\n",
    "    cimaq2nilearn(cimaq_dir)    \n",
    "    scan_infos = bidsify_load_scans(cimaq_dir, snif.clean_bytes(xpu(pjoin(\n",
    "            cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "            'participants/sub_list_TaskQC.tsv'))).decode().split()[1:])\n",
    "\n",
    "    scan_infos['dccid'] = sorted([(filename, filename.split('-')[1].split('_')[0])[1]\n",
    "                                  for filename in scan_infos.filename])\n",
    "    behav = snif.loadfiles(snif.loadimages(pjoin(\n",
    "                           os.getcwd(), 'newtest', 'behavioural')))\n",
    "    behav[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "                                  for filename in behav.filename]\n",
    "    events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "                           os.getcwd(), 'newtest', 'nilearn_events')))\n",
    "    events[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "                                  for filename in events.filename]\n",
    "\n",
    "    confounds = snif.loadfiles(snif.filter_lst_inc(\n",
    "        snif.clean_bytes(xpu(pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                                   'participants/sub_list_TaskQC.tsv'))).decode().split()[1:],\n",
    "        snif.loadimages( pjoin(cimaq_dir, 'derivatives/CIMAQ_fmri_memory/data',\n",
    "                                  'confounds', 'resample'))))\n",
    "    confounds['bids_names'] = [bidsify_names(filename) for\n",
    "                               filename in confounds.filename]\n",
    "    \n",
    "    subs = df(((grp, scan_infos.groupby('dccid').get_group(grp))\n",
    "               for grp in tqdm(scan_infos.groupby('dccid').groups,\n",
    "                               desc = 'loading subjects')),\n",
    "              columns = ['subject', 'scans']).set_index(\n",
    "                            'subject').sort_index().reset_index(\n",
    "                         drop = False)\n",
    "    return dict(zip(['scans', 'behavior', 'confounds', 'events'],\n",
    "                    [subs, behav, confounds, events]))\n",
    "\n",
    "class CIMAQ:\n",
    "    def __init__(self, cimaq_dir: Union[str, os.PathLike]) -> None:\n",
    "        self.cimaq_dir = cimaq_dir\n",
    "    @ classmethod\n",
    "    def fetch(self, cimaq_dir: Union[str, os.PathLike]):\n",
    "        return fetch_cimaq(cimaq_dir)\n",
    "\n",
    "cimaq = CIMAQ.fetch(cimaq_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimaq.fetch(cimaq_dir)\n",
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cimaq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scan_infos = bidsify_load_scans(cimaq_dir, snif.clean_bytes(xpu(pjoin(\n",
    "#             drv_dir, 'participants/sub_list_TaskQC.tsv'))).decode().split()[1:])\n",
    "\n",
    "# scan_infos['dccid'] = sorted([(filename, filename.split('-')[1].split('_')[0])[1]\n",
    "#                             for filename in scan_infos.filename])\n",
    "# behav = snif.loadfiles(snif.loadimages(pjoin(\n",
    "#                        os.getcwd(), 'newtest', 'behavioural')))\n",
    "# behav[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "#                               for filename in behav.filename]\n",
    "# events = snif.loadfiles(snif.loadimages(pjoin(\n",
    "#                        os.getcwd(), 'newtest', 'events')))\n",
    "# events[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "#                               for filename in events.filename]\n",
    "\n",
    "# confounds = snif.loadfiles(snif.loadimages(pjoin(drv_dir, 'confounds', 'resample')))\n",
    "# confounds['bids_names'] = [bidsify_names(filename) for\n",
    "#                            filename in confounds.filename]\n",
    "\n",
    "# # confounds[['pscid', 'dccid']] = [filename.split('_')[1:3]\n",
    "# #                                  for filename in confounds.filename]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "subs = df(((grp, scan_infos.groupby('dccid').get_group(grp))\n",
    "           for grp in scan_infos.groupby('dccid').groups),\n",
    "          columns = ['subject', 'scans']).set_index(\n",
    "                        'subject').sort_index().reset_index(\n",
    "                     drop = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(events, behav, subs)\n",
    "# subs = df(((grp, scan_infos.groupby('filename').get_group(grp))\n",
    "#            for grp in scan_infos.groupby('filename').groups),\n",
    "#           columns = ['subject', 'scans']).set_index(\n",
    "#                         'subject').sort_index().reset_index(\n",
    "#                      drop = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs.iloc[44].scans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "byfilename = df((df(((grp, row[1].scans.groupby('filename').get_group(grp))\n",
    "                 for grp in row[1].scans.groupby('filename').groups),\n",
    "                columns = ['modality', 'scans']) for row in subs.iterrows())).values.flat\n",
    "# json.dumps([itm.values.tolist() for itm in byfilename.values.tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_infos.modality.unique()\n",
    "\n",
    "list(next((itm))['scans'].values.tolist() for itm in byfilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scan_infos\n",
    "\n",
    "# By subject\n",
    "# subs = df(((grp, scan_infos.groupby('dccid').get_group(grp))\n",
    "#            for grp in scan_infos.groupby('dccid').groups),\n",
    "#           columns = ['subject', 'scans']).set_index(\n",
    "#                         'subject').sort_index().reset_index(\n",
    "#                      drop = False)\n",
    "\n",
    "files_per_sub = pd.Series(row[1].scans.shape[0] for row\n",
    "                          in subs.iterrows()).describe()\n",
    "files_per_sub\n",
    "# By scan type (parent column)\n",
    "# mods = df(((grp, scan_infos.groupby('parent').get_group(grp))\n",
    "#            for grp in scan_infos.groupby('parent').groups),\n",
    "#           columns = ['scan_type', 'scans']).set_index('scan_type')\n",
    "\n",
    "\n",
    "# display(mods.loc['dwi']['scans'],\n",
    "#         mods.loc['anat']['scans'])\n",
    "# dwi_scans = [mods['dwi']] + [dwi_only.groupby('ext').get_group(grp)\n",
    "#                              for grp in dwi_only.groupby('ext').groups]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonfiles\n",
    "from io import StringIO\n",
    "\n",
    "def json_bytes(apath: Union[str, os.PathLike]):\n",
    "    with open(apath, 'rb', buffering = 0) as jfile:\n",
    "        jbytes3 = json.load(jfile)\n",
    "    jfile.close()\n",
    "    return jbytes3\n",
    "\n",
    "# jsons = list((row[1].filename, json_bytes(row[1].fpaths))\n",
    "#              for row in tqdm(jsonfiles.iterrows()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(pd.read_csv(apath, sep = '\\t') for apath in events.fpaths)\n",
    "# display(behav, events, scan_infos, jsonfiles, nifti_only, dwi_only)\n",
    "\n",
    "# nifti_only['jsoninfo'] = jsonfiles.fpaths.tolist()\n",
    "\n",
    "# mods = dict((grp, nifti_only.groupby('parent').get_group(grp))\n",
    "#         for grp in nifti_only.groupby('parent').groups)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "# full_infos = pd.concat([df.from_dict({**dict(nib.load(row[1].fpaths).header),\n",
    "#                                       **json_bytes(row[1].jsoninfo)},\n",
    "#                                      orient = 'index').T\n",
    "#                         for row in tqdm(nifti_only.iterrows())], join = 'inner')\n",
    "# full_infos['filename'] = nifti_only['filename'].tolist()\n",
    "# full_infos = full_infos.set_index('filename')\n",
    "# full_infos.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(dwi_scans[0], dwi_scans[1], dwi_scans[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.from_dict(mods['dwi'], orient ='index').T#, dwi_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_infos.to_csv(pjoin(os.getcwd(), 'newdevs',\n",
    "                        'inner_join_nifti_header_json.tsv'),\n",
    "                  sep = '\\t', index = None)\n",
    "\n",
    "test = [row[1].values for row in full_infos.T.iterrows()]\n",
    "\n",
    "test\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ungrouped\n",
    "\n",
    "from nilearn import image\n",
    "from dict2csv import dict2csv\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [itm[1].unique()[0] for itm in\n",
    " df((itm[1].columns for itm\n",
    "     in jsons)).dropna().iteritems()\n",
    " if len(itm[1].unique()) == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dumps(unidecode(jsons[0][1][1].decode())).encode().split(\n",
    "    snif.get_lineterminator(json.dumps(str(\n",
    "        jsons[0][1][1].decode())).encode()))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(pjoin(os.getcwd(), 'cimaq_json_headers'), exist_ok = True)\n",
    "\n",
    "cimaq_json_parameters = list((df.from_dict(dict(row[1].parameters),\n",
    "                                          orient = 'index').to_csv(\n",
    "                            pjoin(os.getcwd(), 'cimaq_nifti_headers', row[1].filename + \\\n",
    "                                  '_nifti_headers.tsv'), sep = '\\t', index = None))\n",
    "                             for row in nifti_only.iterrows())\n",
    "cimaq_scan_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(pjoin(os.getcwd(), 'cimaq_nifti_headers'), exist_ok = True)\n",
    "\n",
    "cimaq_scan_parameters = list((df.from_dict(dict(row[1].parameters),\n",
    "                                          orient = 'index').to_csv(\n",
    "                            pjoin(os.getcwd(), 'cimaq_nifti_headers', row[1].filename + \\\n",
    "                                  '_nifti_headers.tsv'), sep = '\\t'))\n",
    "                             for row in nifti_only.iterrows())\n",
    "cimaq_scan_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def dict2json(adict, dst_path: Union[str, os.PathLike]):\n",
    "\n",
    "#     adict = {\n",
    "#       'bill': 'tech',\n",
    "#       'federer': 'tennis',\n",
    "#       'ronaldo': 'football',\n",
    "#       'woods': 'golf',\n",
    "#       'ali': 'boxing'\n",
    "#     }\n",
    "\n",
    "    with open(dst_path, 'w') as json_file:\n",
    "        json.dump(adict, json_file)\n",
    "        json_file.close()\n",
    "\n",
    "dict2json(cimaq_scan_parameters, pjoin(os.getcwd(), 'cimaq_scan_parameters.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pjoin(os.getcwd(), 'cimaq_scan_parameters.csv')\n",
    "cimaq_scans_json = json.load(pjoin(os.getcwd(), 'cimaq_scan_parameters.json'))\n",
    "cimaq_scans_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dict2csv(cimaq_scan_parameters, pjoin(os.getcwd(), 'cimaq_scan_parameters.txt'))\n",
    "\n",
    "list(cimaq_scan_parameters.keys())\n",
    "# with open(pjoin(os.getcwd(), 'cimaq_scan_parameters.json'), 'w') as jfile:\n",
    "#     json.dump(cimaq_scan_parameters, pjoin(os.getcwd(), 'cimaq_scan_parameters.json'))jfile.write(cimaq_scan_parameters)\n",
    "# #     jfile.close()\n",
    "\n",
    "# cimaq_scan_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = pd.concat([df.from_dict(dict(row[1].parameters), orient = 'index')\n",
    "                   for row in nifti_only.iterrows()], axis = 1).T\n",
    "# params['filename'] = nifti_only.filename\n",
    "# infos = df(df(((pd.Series(list(itm[1].values for itm in nifti_only.iteritems()) + \\\n",
    "#         list(itm[1].values for itm in params.iteritems()))))).T.values)\n",
    "display(infos)\n",
    "# nifti_only[params.columns] = [params.iteritems()]\n",
    "# nifti_only\n",
    "\n",
    "# pd.merge(params, nifti_only, on = 'filename', how = 'outer', axis =1)\n",
    "# pd.concat((nifti_only, params), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting anatomical images\n",
    "\n",
    "from nilearn import plotting\n",
    "\n",
    "nif = nifti_only.iloc[:5]\n",
    "display([(row[1].filename,\n",
    "          plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "         for row in tqdm(nif.iterrows(), 'Plotting anatomical images')\n",
    " if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])\n",
    "# # test = df(zip(nifti_only.iterrows(), jsonfiles.iterrows()))\n",
    "\n",
    "# # test = [behav, events, scan_infos, jsonfiles, nifti_only, dwi_only]\n",
    "# # test = [list(((itm.groupby('dccid').get_group(grp).groupby('modality').get_group(mod)\n",
    "# #               for grp in itm.groupby('dccid').groups) for mod in\n",
    "# #              itm.groupby('dccid').get_group(grp).groupby('modality').groups))\n",
    "# #         for itm in test]\n",
    "\n",
    "\n",
    "# # jsonfiles, scan_infos= (scan_infos.drop([[row[0]\n",
    "# #                                 for row in scan_infos.iterrows()\n",
    "# #                                  if row[1].ext != '.json']][0], axis=0),\n",
    "# #             scan_infos.drop([[row[0]\n",
    "# #                                 for row in scan_infos.iterrows()\n",
    "# #                                  if row[1].ext == '.json']][0], axis=0))\n",
    "# # display(behav, events, scan_infos, jsonfiles, nifti_only, dwi_only, test)\n",
    "# image.load_img(nifti_only.iloc[0]['scans'])\n",
    "# # display(test.iloc[0][0].scans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # All ungrouped\n",
    "# from nilearn import image\n",
    "\n",
    "# jsonfiles = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                               if row[1].ext == '.json']]\n",
    "# scan_infos = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if row[0] not in jsonfiles.index]]\n",
    "# nifti_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                              if '.nii' in row[1].ext]]\n",
    "\n",
    "\n",
    "# dwi_only = scan_infos.loc[[row[0] for row in scan_infos.iterrows()\n",
    "#                            if '.nii' not in row[1].ext\n",
    "#                            and '.json' not in row[1].ext]]\n",
    "\n",
    "# nifti_only = nifti_only.loc[[row[0] for row in nifti_only.iterrows()\n",
    "#                              if row[1].fpaths not in dwi_only.fpaths]]\n",
    "\n",
    "\n",
    "# nifti_only[['scans', 'parameters']] = tuple(((nib.load(fpath),\n",
    "#                                               nib.load(fpath).header)\n",
    "#                                              for fpath in nifti_only.fpaths))\n",
    "\n",
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nif = nifti_only.iloc[:5]\n",
    "# from nilearn import plotting\n",
    "# display([(row[1].filename,\n",
    "#           plotting.plot_img(image.load_img(row[1]['scans'])))\n",
    "#          for row in nif.iterrows()\n",
    "#  if 'anat' in row[1]['fpaths']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epis = nifti_only.iloc[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ############ TODO: Group by modality ################3333\n",
    "\n",
    "# subs_json = [jsonfiles.groupby('dccid').get_group(grp)\n",
    "#              for grp in jsonfiles.groupby('dccid').groups]\n",
    "# modgrps = [[sjson.groupby('modality').get_group(grp) for grp in sjson.groupby('modality').groups]\n",
    "#            for sjson in subs_json]\n",
    "# modgrps[4][0]\n",
    "# # loaded_json = [pd.read_json(row[1].fpaths) for row in itm.iterrows()]\n",
    "# #                for itm in subs_json]\n",
    "\n",
    "# # loaded_json2 = [row[1].unique() for row in loaded_json.iterrows()]\n",
    "# # loaded_json2 = [Counter(row[1]).most_common(1) for row in loaded_json.iterrows()]\n",
    "# # loaded_json2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _update_timestamp(path: os.PathLike, set_new: bool) -> None:\n",
    "#     \"\"\"\n",
    "#     Context manager to set the timestamp of the path to plus or\n",
    "#     minus a fixed delta, regardless of modifications within the context.\n",
    "\n",
    "#     if set_new is True, the delta is added. Otherwise, the delta is subtracted.\n",
    "#     \"\"\"\n",
    "#     stats = os.stat(path)\n",
    "#     if set_new:\n",
    "#         new_timestamp = (stats.st_atime_ns + _TIMESTAMP_DELTA, stats.st_mtime_ns + _TIMESTAMP_DELTA)\n",
    "#     else:\n",
    "#         new_timestamp = (stats.st_atime_ns - _TIMESTAMP_DELTA, stats.st_mtime_ns - _TIMESTAMP_DELTA)\n",
    "#     try:\n",
    "#         yield\n",
    "#     finally:\n",
    "#         os.utime(path, ns=new_timestamp)\n",
    "\n",
    "\n",
    "# # Public Methods "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
